{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenACC Loop Optimizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version of the lab is intended for C/C++ programmers. The Fortran version of this lab is available [here](../Fortran/README.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will receive a warning five minutes before the lab instance shuts down. Remember to save your work! If you are about to run out of time, please see the [Post-Lab](#Post-Lab-Summary) section for saving this lab to view offline later.\n",
    "\n",
    "Don't forget to check out additional [OpenACC Resources](https://www.openacc.org/resources) and join our [OpenACC Slack Channel](https://www.openacc.org/community#slack) to share your experience and get more help from the community."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Let's execute the cell below to display information about the GPUs running on the server. To do this, execute the cell block below by giving it focus (clicking on it with your mouse), and hitting Ctrl-Enter, or pressing the play button in the toolbar above.  If all goes well, you should see some output returned below the grey cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Apr 25 13:43:37 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GH200 120GB             On  |   00000009:01:00.0 Off |                    0 |\n",
      "| N/A   44C    P0             93W /  900W |       7MiB /  97871MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA GH200 120GB             On  |   00000019:01:00.0 Off |                    0 |\n",
      "| N/A   44C    P0            110W /  900W |       2MiB /  97871MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Our goal for this lab is to use the OpenACC Loop clauses to opimize our Parallel Loops.\n",
    "  \n",
    "  \n",
    "  \n",
    "![development_cycle.png](../images/development_cycle.png)\n",
    "\n",
    "This is the OpenACC 3-Step development cycle.\n",
    "\n",
    "**Analyze** your code, and predict where potential parallelism can be uncovered. Use profiler to help understand what is happening in the code, and where parallelism may exist.\n",
    "\n",
    "**Parallelize** your code, starting with the most time consuming parts. Focus on maintaining correct results from your program.\n",
    "\n",
    "**Optimize** your code, focusing on maximizing performance. Performance may not increase all-at-once during early parallelization.\n",
    "\n",
    "We are currently tackling the **optimize** step. We will include the OpenACC loop clauses to optimize the execution of our parallel loop nests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Run the Code\n",
    "\n",
    "In the previous labs, we have built up a working parallel code that can run on both a multicore CPU and a GPU. Let's run the code and note the performance, so that we can compare the runtime to any future optimizations we make."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Ma version**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jacobi.c:\n",
      "laplace2d.c:\n",
      "Compilation Successful!\n",
      "Jacobi relaxation Calculation: 4096 x 4096 mesh\n",
      "    0, 0.250000\n",
      "  100, 0.002397\n",
      "  200, 0.001204\n",
      "  300, 0.000804\n",
      "  400, 0.000603\n",
      "  500, 0.000483\n",
      "  600, 0.000403\n",
      "  700, 0.000345\n",
      "  800, 0.000302\n",
      "  900, 0.000269\n",
      " total: 0.217505 s\n"
     ]
    }
   ],
   "source": [
    "!nvc  -acc -fast -o laplace2d \\\n",
    "    -Mprof=ccff \\\n",
    "    -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/include \\\n",
    "    jacobi.c laplace2d.c \\\n",
    "  && echo \"Compilation Successful!\" \\\n",
    "  && ./laplace2d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Analyze the Code\n",
    "\n",
    "If you would like a refresher on the code files that we are working on, you may view both of them using the two links below.\n",
    "\n",
    "[jacobi.c](jacobi.c)  \n",
    "[laplace2d.c](laplace2d.c)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Profile the Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: CUDA tracing has been automatically enabled since it is a prerequisite for tracing OpenACC.\n",
      "Collecting data...\n",
      "Jacobi relaxation Calculation: 4096 x 4096 mesh\n",
      "    0, 0.250000\n",
      "  100, 0.002397\n",
      "  200, 0.001204\n",
      "  300, 0.000804\n",
      "  400, 0.000603\n",
      "  500, 0.000483\n",
      "  600, 0.000403\n",
      "  700, 0.000345\n",
      "  800, 0.000302\n",
      "  900, 0.000269\n",
      " total: 0.254337 s\n",
      "Generating '/tmp/nsys-report-9deb.qdstrm'\n",
      "[1/7] [========================100%] laplace2d.nsys-rep\n",
      "[2/7] [========================100%] laplace2d.sqlite\n",
      "[3/7] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)          Name        \n",
      " --------  ---------------  ---------  --------  --------  --------  --------  -----------  --------------------\n",
      "     89.2        212420192       6001   35397.5    4224.0      1152   1425024      56440.9  cuStreamSynchronize \n",
      "      4.9         11663200       1000   11663.2    8800.0      7968   1125952      37317.2  cuMemcpyDtoHAsync_v2\n",
      "      3.1          7488000       3000    2496.0    1984.0      1344    366880       6805.6  cuLaunchKernel      \n",
      "      2.0          4736960       1000    4737.0    1344.0       896   3135776      99116.7  cuMemsetD32Async    \n",
      "      0.3           758240          5  151648.0  120800.0      2912    321408     155553.2  cuMemAlloc_v2       \n",
      "      0.3           694144          2  347072.0  347072.0    341536    352608       7829.1  cuMemcpyHtoDAsync_v2\n",
      "      0.1           176352          1  176352.0  176352.0    176352    176352          0.0  cuMemAllocHost_v2   \n",
      "      0.1           167616          1  167616.0  167616.0    167616    167616          0.0  cuModuleLoadDataEx  \n",
      "      0.0             1280          3     426.7     288.0       160       832        356.8  cuCtxSetCurrent     \n",
      "\n",
      "[4/7] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)                 Name                \n",
      " --------  ---------------  ---------  --------  --------  --------  --------  -----------  -----------------------------------\n",
      "     48.9         89432287       1000   89432.3   89440.0     85728     90944        448.5  _11laplace2d_c_calcNext_39_gpu     \n",
      "     47.0         85928991       1000   85929.0   85888.0     84032     89024        651.2  _11laplace2d_c_swap_55_gpu         \n",
      "      4.1          7492416       1000    7492.4    7456.0      6976      8896        255.0  _11laplace2d_c_calcNext_39_gpu__red\n",
      "\n",
      "[5/7] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)           Operation          \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ----------------------------\n",
      "     48.3          1379936   1000    1379.9    1376.0      1344      3712         85.2  [CUDA memcpy Device-to-Host]\n",
      "     28.5           815296   1000     815.3     800.0       768      1312         47.0  [CUDA memset]               \n",
      "     23.2           661728      2  330864.0  330864.0    328960    332768       2692.7  [CUDA memcpy Host-to-Device]\n",
      "\n",
      "[6/7] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)           Operation          \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ----------------------------\n",
      "    268.435      2   134.218   134.218   134.218   134.218        0.000  [CUDA memcpy Host-to-Device]\n",
      "      0.008   1000     0.000     0.000     0.000     0.000        0.000  [CUDA memcpy Device-to-Host]\n",
      "      0.008   1000     0.000     0.000     0.000     0.000        0.000  [CUDA memset]               \n",
      "\n",
      "[7/7] Executing 'openacc_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls  Avg (ns)   Med (ns)   Min (ns)  Max (ns)  StdDev (ns)                Name              \n",
      " --------  ---------------  ---------  ---------  ---------  --------  --------  -----------  --------------------------------\n",
      "     22.9        113571360       3000    37857.1     5664.0      1440   1005536      53845.7  Wait@laplace2d.c:39             \n",
      "     22.8        112992352       1000   112992.4   105776.0    103776   1011584      44159.8  Compute Construct@laplace2d.c:39\n",
      "     20.5        101753760       1000   101753.8    93792.0     91520   1430176      65985.9  Compute Construct@laplace2d.c:55\n",
      "     20.0         98965888       2000    49482.9    63728.0      1408   1426528      66117.1  Wait@laplace2d.c:55             \n",
      "      3.5         17220512       2000     8610.3    10896.0       416   1135904      30710.4  Exit Data@laplace2d.c:39        \n",
      "      3.1         15489888       2000     7744.9     7936.0      1984   3149632      70632.4  Enter Data@laplace2d.c:39       \n",
      "      2.5         12297280       1000    12297.3     9184.0      8288   1127328      37727.3  Enqueue Download@laplace2d.c:50 \n",
      "      1.1          5428160       2000     2714.1     2368.0      1760     25664       1449.2  Enqueue Launch@laplace2d.c:39   \n",
      "      1.1          5248512       1000     5248.5     1760.0      1184   3137472      99155.7  Enqueue Upload@laplace2d.c:39   \n",
      "      0.7          3339584       1000     3339.6     2496.0      1792    368032      11651.0  Enqueue Launch@laplace2d.c:55   \n",
      "      0.5          2374528       1000     2374.5     2144.0      2016     46400       2372.6  Enter Data@laplace2d.c:55       \n",
      "      0.5          2352320       1000     2352.3     1536.0      1440    345088      15039.3  Wait@laplace2d.c:50             \n",
      "      0.5          2234368       1000     2234.4      512.0       448   1310080      41783.4  Exit Data@laplace2d.c:55        \n",
      "      0.3          1433984          1  1433984.0  1433984.0   1433984   1433984          0.0  Enter Data@laplace2d.c:35       \n",
      "      0.1           699840          2   349920.0   349920.0    342976    356864       9820.3  Enqueue Upload@laplace2d.c:35   \n",
      "      0.0           210464          1   210464.0   210464.0    210464    210464          0.0  Device Init@laplace2d.c:35      \n",
      "      0.0             7296          1     7296.0     7296.0      7296      7296          0.0  Exit Data@laplace2d.c:71        \n",
      "      0.0             3264          1     3264.0     3264.0      3264      3264          0.0  Wait@laplace2d.c:35             \n",
      "      0.0                0          2        0.0        0.0         0         0          0.0  Alloc@laplace2d.c:35            \n",
      "      0.0                0          1        0.0        0.0         0         0          0.0  Alloc@laplace2d.c:39            \n",
      "      0.0                0          2        0.0        0.0         0         0          0.0  Create@laplace2d.c:35           \n",
      "      0.0                0       1000        0.0        0.0         0         0          0.0  Create@laplace2d.c:39           \n",
      "      0.0                0       1000        0.0        0.0         0         0          0.0  Delete@laplace2d.c:50           \n",
      "      0.0                0          2        0.0        0.0         0         0          0.0  Delete@laplace2d.c:71           \n",
      "\n",
      "Generated:\n",
      "    /gpfs/home/scortinhal/CHPS0904/OpenACC/module6/English/C/laplace2d.nsys-rep\n",
      "    /gpfs/home/scortinhal/CHPS0904/OpenACC/module6/English/C/laplace2d.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile -t openacc --stats=true --force-overwrite true -o laplace2d ./laplace2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dLet's check out the profiler's report. Once the profiling run has completed, download and save the report file by holding down <mark>Shift</mark> and <mark>Right-Clicking</mark> [Here](laplace_baseline.qdrep) (choose *save link as*), and open it via the GUI. To view the profiler report locally, please see the section on [How to view the report](../../../module2/English/C/README.ipynb#viewreport)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## OpenACC Loop Directive\n",
    "\n",
    "The *loop directive* allows us to mark specific loops for parallelization. The *loop directive* also allows us to map specific optimizations/alterations to our loops using **loop clauses**. Not all loop clauses are used to optimize our code; some are also used to verify code correctness. A few examples of the *loop directive* are as follows:\n",
    "\n",
    "```cpp\n",
    "#pragma acc parallel loop < loop clauses >\n",
    "for(int i = 0; i < N; i++)\n",
    "{\n",
    "    < loop code >\n",
    "}\n",
    "```\n",
    "\n",
    "```cpp\n",
    "#pragma acc kernels loop < loop clauses >\n",
    "for(int i = 0; i < N; i++)\n",
    "{\n",
    "    < loop code >\n",
    "}\n",
    "```\n",
    "\n",
    "```cpp\n",
    "#pragma acc parallel loop < loop clauses >\n",
    "for(int i = 0; i < N; i++)\n",
    "{\n",
    "    #pragma acc loop < loop clauses >\n",
    "    for(int j = 0; j < M; j++)\n",
    "    {\n",
    "        < loop code >\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "Also, including loop optimizations does not always optimize the code. It is up to the programmer to determine via analysis and profiling, which loop optimizations will work best for their loops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Independent Clause\n",
    "\n",
    "When using the `kernels` directive, the compiler will decide which loops are, and are not, parallelizable. However, the programmer can override this compiler decision by using the `independent` clause. The `independent` clause is a way for the programmer to guarantee to the compiler that a loop is **parallelizable**.\n",
    "\n",
    "```cpp\n",
    "#pragma acc kernels loop independent\n",
    "for(int i = 0; i < N; i++)\n",
    "{\n",
    "    < Parallel Loop Code >\n",
    "}\n",
    "\n",
    "#pragma acc kernels\n",
    "{\n",
    "    for(int i = 0; i < N; i++)\n",
    "    {\n",
    "        < Parallel Loop Code >\n",
    "    }\n",
    "    \n",
    "    #pragma acc loop independent\n",
    "    for(int i = 0; i < N; i++)\n",
    "    {\n",
    "        < Parallel Loop Code >\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "In the second example, we have two loops. The compiler will make a decision whether or not the first loop is parallelizable. In the second loop however, we have included the independent clause. This means that the compiler will trust the programmer, and assume that the second loop is parallelizable.\n",
    "\n",
    "When using the `parallel` directive, the `independent` clause is automatically implied on all `loop` directives contained within. This means that you do not need to use the `independent` clause when you are using the `parallel` directive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto Clause\n",
    "\n",
    "The `auto` clause is more-or-less the complete opposite of the `independent` clause. When you are using the `parallel` directive, the compiler will trust anything that the programmer decides. This means that if the programmer believes that a loop is parallelizable, the compiler will trust the programmer. However, if you include the auto clause with your loops, the compiler will double check the loops, and decide whether or not to parallelize them.\n",
    "\n",
    "```cpp\n",
    "#pragma acc parallel loop auto\n",
    "for(int i = 0; i < N; i++)\n",
    "{\n",
    "    < Parallel Loop Code >\n",
    "}\n",
    "```\n",
    "\n",
    "The `independent` clause is a way for the programmer to assert to the compiler that a loop is parallelizable. The `auto` clause is a way for the programmer to tell the compiler to analyze the loop, and to determine whether or not it is parallelizable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq Clause\n",
    "\n",
    "The `seq` clause (short for *\"sequential\"*) is used to define a loop that should run sequentially on the parallel hardware. This loop clause is usually automatically applied to large, multidimensional loop nests, since the compiler may only be able to describe parallelism for the outer-most loops. For example:\n",
    "\n",
    "```cpp\n",
    "for(int i = 0; i < N; i++)\n",
    "{\n",
    "    for(int j = 0; j < M; j++)\n",
    "    {\n",
    "        for(int k = 0; k < Q; k++)\n",
    "        {\n",
    "            < Loop Code >\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "The compiler may only be able to parallelize the `i` and `j` loops, and will choose to run the `k` loop **sequentially**. The `seq` clause is also useful for running very small, nested loops sequentially. For example:\n",
    "\n",
    "```cpp\n",
    "for(int i = 0; i < 1000000; i++)\n",
    "{\n",
    "    for(int j = 0; j < 4; j++)\n",
    "    {\n",
    "        for(int k = 0; k < 1000000; k++)\n",
    "        {\n",
    "            < Loop Code >\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "The middle loop is very small, and will most likely not benefit from parallelization. To fix this, we may apply the `seq` clause as follows:\n",
    "\n",
    "```cpp\n",
    "#pragma acc parallel loop\n",
    "for(int i = 0; i < 1000000; i++)\n",
    "{\n",
    "    #pragma acc loop seq\n",
    "    for(int j = 0; j < 4; j++)\n",
    "    {\n",
    "        #pragma acc loop\n",
    "        for(int k = 0; k < 1000000; k++)\n",
    "        {\n",
    "            < Loop Code >\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "In this code snippet, the middle loop will be run sequentially, while the outer-most loop and inner-most loop will be run in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduction Clause\n",
    "\n",
    "Up to this point, we have technically been using the `reduction` clause in our laplace code. We were not explicitly defining the reduction, instead the compiler has been automatically applying the reduction clause to our code. Let's look at one of the loops from within our `laplace2d.c` code file.\n",
    "\n",
    "```cpp\n",
    "#pragma acc parallel loop present(A,Anew)\n",
    "for( int j = 1; j < n-1; j++\n",
    "{\n",
    "    #pragma acc loop\n",
    "    for( int i = 1; i < m-1; i++ )\n",
    "    {\n",
    "        Anew[OFFSET(j, i, m)] = 0.25 * ( A[OFFSET(j, i+1, m)] + A[OFFSET(j, i-1, m)] \n",
    "                                       + A[OFFSET(j-1, i, m)] + A[OFFSET(j+1, i, m)] );\n",
    "        error = fmax( error, fabs(Anew[OFFSET(j, i, m)] - A[OFFSET(j, i , m)]));\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "More specifically, let's focus on this single line of code:\n",
    "\n",
    "```cpp\n",
    "error = fmax( error, fabs(Anew[OFFSET(j, i, m)] - A[OFFSET(j, i , m)]));\n",
    "```\n",
    "\n",
    "Each iteration of our inner-loop will write to the value `error`. When we are running thousands of these loop iterations **simultaneously**, it can become very dangerous to let all of them write directly to `error`. To fix this, we must use the OpenACC `reduction` clause. Let's look at the syntax.\n",
    "\n",
    "```cpp\n",
    "#pragma acc parallel loop reduction(operator:value)\n",
    "```\n",
    "\n",
    "And let's look at a quick example of the use.\n",
    "\n",
    "```cpp\n",
    "#pragma acc parallel loop reduction(+:sum)\n",
    "for(int i = 0; i < N; i++)\n",
    "{\n",
    "    sum += A[i];\n",
    "}\n",
    "```\n",
    "\n",
    "This is a list of all of the available operators in OpenACC.\n",
    "\n",
    "|Operator    |Example                     |Description           |\n",
    "|:----------:|:---------------------------|:---------------------|\n",
    "|+           |reduction(+:sum)            |Mathematical summation|\n",
    "|*           |reduction(*:product)        |Mathematical product  |\n",
    "|max         |reduction(max:maximum)      |Maximum value         |\n",
    "|min         |reduction(min:minimum)      |Minimum value         |\n",
    "|&           |reduction(&:val)            |Bitwise AND           |\n",
    "|&#124;      |reduction(&#124;:val)       |Bitwise OR            |\n",
    "|&&          |reduction(&&:bool)          |Logical AND           |\n",
    "|&#124;&#124;|reduction(&#124;&#124;:bool)|Logical OR            |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional: Implementing the Reduction Clause\n",
    "\n",
    "We are compiling our code with the nvc compiler, which is automatically able to include the reduction clause. However, in other compilers, we may not be as fortunate. Use the following link to add the `reduction` clause with the `max` operator to our code.\n",
    "\n",
    "[laplace2d.c](laplace2d.c)  \n",
    "(make sure to save your code with ctrl+s)\n",
    "\n",
    "You may then run the following script to verify that the compiler is properly recognizing your reduction clause."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Ma version**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Reduction/jacobi.c:\n",
      "./Reduction/laplace2d.c:\n",
      "Compilation Successful!\n",
      "Jacobi relaxation Calculation: 4096 x 4096 mesh\n",
      "    0, 0.250000\n",
      "  100, 0.002397\n",
      "  200, 0.001204\n",
      "  300, 0.000804\n",
      "  400, 0.000603\n",
      "  500, 0.000483\n",
      "  600, 0.000403\n",
      "  700, 0.000345\n",
      "  800, 0.000302\n",
      "  900, 0.000269\n",
      " total: 0.217121 s\n"
     ]
    }
   ],
   "source": [
    "!nvc  -acc -fast -o laplace2d_reduction \\\n",
    "    -Mprof=ccff \\\n",
    "    -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/include \\\n",
    "    ./Reduction/jacobi.c ./Reduction/laplace2d.c \\\n",
    "  && echo \"Compilation Successful!\" \\\n",
    "  && ./laplace2d_reduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may also check your answer by selecting the following link.\n",
    "\n",
    "[laplace2d.c](solutions/reduction/laplace2d.c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Private Clause\n",
    "\n",
    "The private clause allows us to mark certain variables (and even arrays) as *\"private\"*. The best way to visualize it is with an example:\n",
    "\n",
    "```cpp\n",
    "int tmp;\n",
    "\n",
    "#pragma acc parallel loop private(tmp)\n",
    "for(int i = 0; i < N/2; i++)\n",
    "{\n",
    "    tmp = A[i];\n",
    "    A[i] = A[N-i-1];\n",
    "    A[N-i-1] = tmp;\n",
    "}\n",
    "```\n",
    "\n",
    "In this code, each thread will have its own *private copy of tmp*. You may also declare static arrays as private, like this:\n",
    "\n",
    "```cpp\n",
    "int tmp[10];\n",
    "\n",
    "#pragma acc parallel loop private(tmp[0:10])\n",
    "for(int i = 0; i < N; i++)\n",
    "{\n",
    "    < Loop code that uses the tmp array >\n",
    "}\n",
    "```\n",
    "\n",
    "When using *private variables*, the variable only exists within the private scope. This generally means that the private variable only exists for a single loop iteration, and the values you store in the private variable cannot extend out of the loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collapse Clause\n",
    "\n",
    "This is our first true *loop optimization*. The `collapse` clause allows us to transform multi-dimensional loop nests into a single-dimensional loop. This process is helpful for increasing the overall length (which usually increases parallelism) of our loops, and will often help with memory locality. Let's look at the syntax.\n",
    "\n",
    "```cpp\n",
    "#pragma acc parallel loop collapse( N )\n",
    "```\n",
    "\n",
    "Where N is the number of loops to collapse.\n",
    "\n",
    "```cpp\n",
    "#pragma acc parallel loop collapse( 3 )\n",
    "for(int i = 0; i < N; i++)\n",
    "{\n",
    "    for(int j = 0; j < M; j++)\n",
    "    {\n",
    "        for(int k = 0; k < Q; k++)\n",
    "        {\n",
    "            < loop code >\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "This code will combine the 3-dimensional loop nest into a single 1-dimensional loop. It is important to note that when using the `collapse` clause, the inner loops should not have their own `loop` directive. What this means is that the following code snippet is **incorrect** and will give a warning when compiling.\n",
    "\n",
    "```cpp\n",
    "#pragma acc parallel loop collapse( 3 )\n",
    "for(int i = 0; i < N; i++)\n",
    "{\n",
    "    #pragma acc loop\n",
    "    for(int j = 0; j < M; j++)\n",
    "    {\n",
    "        #pragma acc loop\n",
    "        for(int k = 0; k < Q; k++)\n",
    "        {\n",
    "            < loop code >\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing the Collapse Clause\n",
    "\n",
    "Use the following link to edit our code. Use the `collapse` clause to collapse our multi-dimensional loops into a single dimensional loop.\n",
    "\n",
    "[laplace2d.c](laplace2d.c)  \n",
    "(make sure to save your code with ctrl+s)\n",
    "\n",
    "Then run the following script to see how the code runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Ma version**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Collapse/jacobi.c:\n",
      "./Collapse/laplace2d.c:\n",
      "Compilation Successful!\n",
      "Jacobi relaxation Calculation: 4096 x 4096 mesh\n",
      "    0, 0.250000\n",
      "  100, 0.002397\n",
      "  200, 0.001204\n",
      "  300, 0.000804\n",
      "  400, 0.000603\n",
      "  500, 0.000483\n",
      "  600, 0.000403\n",
      "  700, 0.000345\n",
      "  800, 0.000302\n",
      "  900, 0.000269\n",
      " total: 0.358741 s\n"
     ]
    }
   ],
   "source": [
    "!nvc  -acc -fast -o laplace2d_reduction \\\n",
    "    -Mprof=ccff \\\n",
    "    -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/include \\\n",
    "    ./Collapse/jacobi.c ./Collapse/laplace2d.c \\\n",
    "  && echo \"Compilation Successful!\" \\\n",
    "  && ./laplace2d_reduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tile Clause\n",
    "\n",
    "The `tile` clause allows us to break up a multi-dimensional loop into *tiles*, or *blocks*. This is often useful for increasing memory locality in some codes. Let's look at the syntax.\n",
    "\n",
    "```cpp\n",
    "#pragma acc parallel loop tile( x, y, z, ... )\n",
    "```\n",
    "\n",
    "Our tiles can have as many dimensions as we want, though we must be careful to not create a tile that is too large. Let's look at an example:\n",
    "\n",
    "```cpp\n",
    "#pragma acc parallel loop tile( 32, 32 )\n",
    "for(int i = 0; i < N; i++)\n",
    "{\n",
    "    for(int j = 0; j < M; j++)\n",
    "    {\n",
    "        < loop code >\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "The above code will break our loop iterations up into 32x32 tiles (or blocks), and then execute those blocks in parallel. Let's look at a slightly more specific code.\n",
    "\n",
    "```cpp\n",
    "#pragma acc parallel loop tile( 32, 32 )\n",
    "for(int i = 0; i < 128; i++)\n",
    "{\n",
    "    for(int j = 0; j < 128; j++)\n",
    "    {\n",
    "        < loop code >\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "In this code, we have 128x128 loop iterations, which are being broken up into 32x32 tiles. This means that we will have 16 tiles, each tile being size 32x32. Similar to the `collapse` clause, the inner loops should not have the `loop` directive. This means that the following code is **incorrect** and will give a warning when compiling.\n",
    "\n",
    "```cpp\n",
    "#pragma acc parallel loop tile( 32, 32 )\n",
    "for(int i = 0; i < N; i++)\n",
    "{\n",
    "    #pragma acc loop\n",
    "    for(int j = 0; j < M; j++)\n",
    "    {\n",
    "        < loop code >\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing the Tile Clause\n",
    "\n",
    "Use the following link to edit our code. Replace the `collapse` clause with the `tile` clause to break our multi-dimensional loops into smaller tiles. Try using a variety of different tile sizes, but always keep one of the dimensions as a **multiple of 32**. We will talk later about why this is important.\n",
    "\n",
    "[laplace2d.c](laplace2d.c)  \n",
    "(make sure to save your code with ctrl+s)\n",
    "\n",
    "Then run the following script to see how the code runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Ma version**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Tile/jacobi.c:\n",
      "./Tile/laplace2d.c:\n",
      "Compilation Successful!\n",
      "Jacobi relaxation Calculation: 4096 x 4096 mesh\n",
      "    0, 0.250000\n",
      "  100, 0.002397\n",
      "  200, 0.001204\n",
      "  300, 0.000804\n",
      "  400, 0.000603\n",
      "  500, 0.000483\n",
      "  600, 0.000403\n",
      "  700, 0.000345\n",
      "  800, 0.000302\n",
      "  900, 0.000269\n",
      " total: 0.503192 s\n"
     ]
    }
   ],
   "source": [
    "!nvc  -acc -fast -o laplace2d_reduction \\\n",
    "    -Mprof=ccff \\\n",
    "    -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/include \\\n",
    "    ./Tile/jacobi.c ./Tile/laplace2d.c \\\n",
    "  && echo \"Compilation Successful!\" \\\n",
    "  && ./laplace2d_reduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gang/Worker/Vector\n",
    "\n",
    "This is our last optimization, and arguably the most important one. In OpenACC, the concepts of *Gang*, *Worker*, and *Vector* are used to define additional levels of parallelism. Specifically for NVIDIA GPUs, *Gang*, *Worker*, and *Vector* will define the *organization* of our GPU threads (or sometimes referred to as the *decomposition* of for loop iterations). Each loop will have an optimal Gang Worker Vector implementation, and finding that correct implementation will often take a bit of thinking, and possibly some trial and error. So let's explain how Gang Worker Vector actually works.\n",
    "\n",
    "![gang_worker_vector.png](../images/gang_worker_vector.png)\n",
    "\n",
    "This image represents a single **gang**. When parallelizing our **for loops**, the **loop iterations** will be **broken up evenly** among a number of gangs. Each gang will contain a number of **threads**. These threads are organized into **blocks**. A **worker** is a row of threads. In the above graphic, there are 3 **workers**, which means that there are 3 rows of threads. The **vector** refers to how long each row is. So in the above graphic, the vector is 8, because each row is 8 threads long.\n",
    "\n",
    "By default, when programming for a GPU, **gang** and **vector** paralleism is automatically applied. Let's see a simple GPU sample code where we explicitly show how the gang and vector works.\n",
    "\n",
    "```cpp\n",
    "#pragma acc parallel loop gang\n",
    "for(int i = 0; i < N; i++)\n",
    "{\n",
    "    #pragma acc loop vector\n",
    "    for(int j = 0; j < M; j++)\n",
    "    {\n",
    "        < loop code >\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "The outer loop will be evenly spread across a number of **gangs**. Then, within those gangs, the inner-loop will be executed in parallel across the **vector**. This is a process that usually happens automatically, however, we can usually achieve better performance by optimzing the *Gang*, *Worker*, and *Vector* ourselves.\n",
    "\n",
    "Lets look at an example where using *Gang*, *Worker*, and *Vector* can greatly increase a loops parallelism.\n",
    "\n",
    "```cpp\n",
    "#pragma acc parallel loop gang\n",
    "for(int i = 0; i < N; i++)\n",
    "{\n",
    "    #pragma acc loop vector\n",
    "    for(int j = 0; j < M; k++)\n",
    "    {\n",
    "        for(int k = 0; k < Q; k++)\n",
    "        {\n",
    "            < loop code >\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "In this loop, we have **gang-level** parallelism on the outer-loop, and **vector-level** parallelism on the middle-loop. However, the inner-loop does not have any parallelism. This means that each thread will be running the inner-loop, however, GPU threads aren't really made to run entire loops. To fix this, we could use **worker-level** parallelism to add another layer.\n",
    "\n",
    "```cpp\n",
    "#pragma acc parallel loop gang\n",
    "for(int i = 0; i < N; i++)\n",
    "{\n",
    "    #pragma acc loop worker\n",
    "    for(int j = 0; j < M; k++)\n",
    "    {\n",
    "        #pragma acc loop vector\n",
    "        for(int k = 0; k < Q; k++)\n",
    "        {\n",
    "            < loop code >\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "Now, the outer-loop will be split across the gangs, the middle-loop will be split across the workers, and the inner loop will be executed by the threads within the vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gang, Worker, Vector Syntax\n",
    "\n",
    "We have been showing really general examples of *Gang*, *Worker*, and *Vector* so far. One of the largest benefits of *Gang*, *Worker*, and *Vector* is the ability to explicitly define how many gangs and workers you need, and how many threads should be in the vector. Let's look at the syntax for the `parallel` directive:\n",
    "\n",
    "```cpp\n",
    "#pragma acc parallel num_gangs( 2 ) num_workers( 4 ) vector_length( 32 )\n",
    "{\n",
    "    #pragma acc loop gang worker\n",
    "    for(int i = 0; i < N; i++)\n",
    "    {\n",
    "        #pragma acc loop vector\n",
    "        for(int j = 0; j < M; j++)\n",
    "        {\n",
    "            < loop code >\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "And now the syntax for the `kernels` directive:\n",
    "\n",
    "```cpp\n",
    "#pragma acc kernels loop gang( 2 ) worker( 4 )\n",
    "for(int i = 0; i < N; i++)\n",
    "{\n",
    "    #pragma acc loop vector( 32 )\n",
    "    for(int j = 0; j < M; j++)\n",
    "    {\n",
    "        < loop code >\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avoid Wasting Threads\n",
    "\n",
    "When parallelizing small arrays, you have to be careful that the number of threads within your vector is not larger than the number of loop iterations. Let's look at a simple example:\n",
    "\n",
    "```cpp\n",
    "#pragma acc kernels loop gang\n",
    "for(int i = 0; i < 1000000000; i++)\n",
    "{\n",
    "    #pragma acc loop vector(256)\n",
    "    for(int j = 0; j < 32; j++)\n",
    "    {\n",
    "        < loop code >\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "In this code, we are parallelizing an inner-loop that has 32 iterations. However, our vector is 256 threads long. This means that when we run this code, we will have a lot more threads than loop iterations, and a lot of the threads will be sitting idly. We could fix this in a few different ways, but let's use **worker-level parallelism** to fix it.\n",
    "\n",
    "```cpp\n",
    "#pragma acc kernels loop gang worker(8)\n",
    "for(int i = 0; i < 1000000000; i++)\n",
    "{\n",
    "    #pragma acc loop vector(32)\n",
    "    for(int j = 0; j < 32; j++)\n",
    "    {\n",
    "        < loop code >\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "Originally we had 1 (implied) worker, that contained 256 threads. Now, we have 8 workers that each have only 32 threads. We have eliminated all of our wasted threads by reducing the length of the **vector** and increasing the number of **workers**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Rule of 32 (Warps)\n",
    "\n",
    "The general rule of thumb for programming for NVIDIA GPUs is to always ensure that your vector length is a multiple of 32 (which means 32, 64, 96, 128, ... 512, ... 1024... etc.). This is because NVIDIA GPUs are optimized to use *warps*. Warps are groups of 32 threads that are executing the same computer instruction. So as a reference:\n",
    "\n",
    "```cpp\n",
    "#pragma acc kernels loop gang\n",
    "for(int i = 0; i < N; i++)\n",
    "{\n",
    "    #pragma acc loop vector(32)\n",
    "    for(int j = 0; j < M; j++)\n",
    "    {\n",
    "        < loop code >\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "will perform much better than:\n",
    "\n",
    "```cpp\n",
    "#pragma acc kernels loop gang\n",
    "for(int i = 0; i < N; i++)\n",
    "{\n",
    "    #pragma acc loop vector(31)\n",
    "    for(int j = 0; j < M; j++)\n",
    "    {\n",
    "        < loop code >\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing the Gang, Worker, and Vector Clauses\n",
    "\n",
    "Use the following link to edit our code. Replace our ealier clauses with **gang, worker, and vector** To reorganize our thread blocks. Try it using a few different numbers, but always keep the vector length as a **multiple of 32** to fully utilize **warps**.\n",
    "\n",
    "[laplace2d.c](laplace2d.c)  \n",
    "(make sure to save your code with ctrl+s)\n",
    "\n",
    "Then run the following script to see how the code runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Ma version**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Gang/jacobi.c:\n",
      "./Gang/laplace2d.c:\n",
      "Compilation Successful!\n",
      "Jacobi relaxation Calculation: 4096 x 4096 mesh\n",
      "    0, 0.250000\n",
      "  100, 0.002397\n",
      "  200, 0.001204\n",
      "  300, 0.000804\n",
      "  400, 0.000603\n",
      "  500, 0.000483\n",
      "  600, 0.000403\n",
      "  700, 0.000345\n",
      "  800, 0.000302\n",
      "  900, 0.000269\n",
      " total: 0.278765 s\n"
     ]
    }
   ],
   "source": [
    "!nvc  -acc -fast -o laplace2d_reduction \\\n",
    "    -Mprof=ccff \\\n",
    "    -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/include \\\n",
    "    ./Gang/jacobi.c ./Gang/laplace2d.c \\\n",
    "  && echo \"Compilation Successful!\" \\\n",
    "  && ./laplace2d_reduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Everything we Learned\n",
    "\n",
    "Now that we have covered the various ways to edit our loops, apply this knowledge to our laplace code. Try mixing some of the loop clauses, and see how the loop optimizations will differ between the parallel and the kernels directive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may run the following script to reset your code with the `kernels` directive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp ./solutions/kernels/laplace2d.c ./laplace2d.c && echo \"Reset Finished\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may run the following script to reset your code with the `parallel` directive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp ./solutions/parallel/laplace2d.c ./laplace2d.c && echo \"Reset Finished\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then use the following link to edit our laplace code.\n",
    "\n",
    "[laplace2d.c](laplace2d.c)  \n",
    "(make sure to save your code with ctrl+s)\n",
    "\n",
    "Then run the following script to see how the code runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvc -ta=tesla -Minfo=accel -o laplace jacobi.c laplace2d.c && ./laplace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Our primary goal when using OpenACC is to parallelize our large for loops. To accomplish this, we must use the OpenACC `loop` directive and loop clauses. There are many ways to alter and optimize our loops, though it is up to the programmer to determine via analysis and profiling, which route is the best to take. At this point in the lab series, you should be able to begin parallelizing your own personal code, and to be able to achieve a relatively high performance using OpenACC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bonus Task\n",
    "\n",
    "If you would like some additional lessons on using OpenACC, there is an Introduction to OpenACC video series available from the OpenACC YouTube page. If you haven't already, I recommend watching this 6 part series. Each video is under 10 minutes, and will give a visual, and hands-on look at a lot of the material we have covered in these labs. The following link will bring you to Part 1 of the series.\n",
    "\n",
    "[Introduction to Parallel Programming with OpenACC - Part 1](https://youtu.be/PxmvTsrCTZg)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Lab Summary\n",
    "\n",
    "If you would like to download this lab for later viewing, it is recommended you go to your browsers File menu (not the Jupyter notebook file menu) and save the complete web page.  This will ensure the images are copied down as well.\n",
    "\n",
    "You can also execute the following cell block to create a zip-file of the files you've been working on, and download it with the link below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -f openacc_files.zip\n",
    "zip -r openacc_files.zip *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After** executing the above zip command, you should be able to download and save the zip file by holding down <mark>Shift</mark> and <mark>Right-Clicking</mark> [Here](openacc_files.zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Licensing\n",
    "This material is released by NVIDIA Corporation under the Creative Commons Attribution 4.0 International (CC BY 4.0)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
