{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aed246f7-aafd-4528-8b3a-b6e10fb70307",
   "metadata": {},
   "source": [
    "# **Multi gpu - CHPS0904 - Multi GPU Programming Models for HPC and AI**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635c214e-6069-4c50-b8bf-24d454430b37",
   "metadata": {},
   "source": [
    "# Partie 1 : Version CPU \n",
    "\n",
    "- Jacobi séquentiel sur CPU (pas de parallélisme, pas de MPI, pas de GPU)\n",
    "- Lecture des paramètres depuis la ligne de commande : nx, ny, max_iter, tol, affichage\n",
    "- Allocation dynamique des deux tableaux (a, a_new) pour stocker la grille\n",
    "- Initialisation des tableaux avec conditions aux bords : gauche/droite à 1, intérieur à 0\n",
    "- Boucle d'itération Jacobi utilisant une fonction dédiée (jacobi_step)\n",
    "- Application d'une condition périodique sur les bords haut/bas (apply_periodic_bc)\n",
    "- Calcul du maximum des différences (norme d’erreur) entre a et a_new pour tester la convergence\n",
    "- Échange des pointeurs a et a_new pour alterner à chaque itération sans recopie de données\n",
    "- Affichage optionnel de la grille à la fin, plus affichage des valeurs aux coins et au centre\n",
    "- Impression du nombre d’itérations, de l’erreur finale, du temps d’exécution total (avec omp_get_wtime, mais sans parallélisme)\n",
    "- Découpage en fonctions indépendantes pour chaque étape (initialisation, étape Jacobi, conditions bords, affichage, etc.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb254c3f-4144-4ebe-abd0-53571fc4649a",
   "metadata": {},
   "source": [
    "## Compilation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8d73b78a-a6a4-4a7f-a0d6-72b6c0c3f833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f jacobi_seq\n",
      "gcc -O3 -Wall -march=native -funroll-loops -ffast-math -std=c99 -fopenmp -o jacobi_seq jacobi.c laplace2d.c -lm\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd CPU\n",
    "make clean \n",
    "make\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd62542-88ff-412e-a777-1a3b3eb61d7f",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7411ab7c-f1d9-451f-a5ec-24e3b69d2b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacobi solver stopped after 1000 iterations\n",
      "Reached max_iter = 1000 (error = 2.419263e-04 > tol = 1.000000e-06)\n",
      "Elapsed time   : 15.544706 seconds\n",
      "Values at corners: [0,0]=1.00  [0,4095]=1.00\n",
      "Value at center [2048,2048] = 0.000000\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "./CPU/jacobi_seq 4096 4096 1000 1e-6 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412d80f5-0b3b-4ad8-8786-3e9275540c79",
   "metadata": {},
   "source": [
    "# Partie 2 : Version CPU + GPU "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672acc20-64eb-4817-beb2-695c4d586616",
   "metadata": {},
   "source": [
    "- Ajout du GPU avec CUDA : calculs réalisés sur le device, et non plus sur le CPU.\n",
    "- Utilisation de kernels CUDA pour :\n",
    "    - Effectuer Jacobi sur le tableau (jacobi_step).\n",
    "    - Appliquer les conditions aux limites périodiques en Y (apply_periodic_bc).\n",
    "- Allocation mémoire séparée sur le CPU (host) et le GPU (device).\n",
    "- Copie initiale des données du host vers le device avant le calcul.\n",
    "- Boucle principale :\n",
    "    - Lancement du kernel Jacobi sur la grille (d_a_new ← d_a).\n",
    "    - Lancement du kernel d’application des bords périodiques en Y.\n",
    "    - Calcul de l’erreur de convergence directement sur le GPU avec **Thrust** :  \n",
    "      Thrust est une bibliothèque C++ intégrée à CUDA qui fournit des primitives algorithmiques (comme les réductions, tris, scans) optimisées pour le GPU. Ici, j'utilise `thrust::transform_reduce` pour parcourir les tableaux sur le GPU et calculer en parallèle l’erreur maximale entre deux itérations, sans repasser les données sur le CPU.\n",
    "    - Inversion des pointeurs device (std::swap) pour éviter une recopie à chaque itération.\n",
    "- Mesure du temps CUDA (calcul pur) via cudaEvent_t, en plus du temps global CPU.\n",
    "- Récupération des résultats finaux du device vers le host pour affichage.\n",
    "- Affichage du temps CUDA, du temps global, du nombre d’itérations, de l’erreur finale, et de l’état final de la grille si demandé.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b4be28-3f6d-49e0-ba53-34bc0deec2a0",
   "metadata": {},
   "source": [
    "## Compilation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6434f52-319d-4e3d-a257-5dfb4358e844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc -O3 -arch=sm_60 -std=c++11        -o jacobi_cuda jacobi.cu -lm                             \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd CPU-GPU\n",
    "make"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f571c66-da19-4ecb-b4be-f79f9775e3ab",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5248b517-2afd-45e4-ab7c-bd146d5c7a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solveur Jacobi CUDA convergé en 1000 itérations (erreur = 2.419e-04)\n",
      "Temps passé (calcul CUDA) : 0.238161 s\n",
      "Temps total du programme (tout inclus) : 2.551201 s\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "./CPU-GPU/jacobi_cuda 4096 4096 1000 1e-6 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4559e5f-3c1c-4c2b-bfc7-09eae2c6aedb",
   "metadata": {},
   "source": [
    "# Partie 3 : MPI + GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af60e48c-8e5a-420d-8ee3-8d8d6102ba7c",
   "metadata": {},
   "source": [
    "- Ajout de la **parallelisation distribuée** avec **MPI** : le domaine 2D est découpé entre plusieurs processus MPI, chacun pouvant s’exécuter sur un GPU distinct.\n",
    "- Allocation mémoire device par sous-domaine local (chaque rang alloue uniquement sa bande locale + halos).\n",
    "- Chaque processus traite une bande de la grille ; gestion fine des indices pour découpage non uniforme si nécessaire.\n",
    "- Ajout d’une communication explicite entre processus MPI à chaque itération pour échanger les **halos** (les halos sont les bandes de points supplémentaires ajoutées au bord de chaque sous-domaine, servant à stocker temporairement les valeurs des points voisins d’un autre sous-domaine ; ils permettent à chaque processus de calculer ses points de frontière sans accès direct à toute la grille globale) via `MPI_Sendrecv`.\n",
    "- Les échanges de halos utilisent les pointeurs device CUDA : le code est compatible MPI CUDA-aware, c’est-à-dire que les transferts MPI sont réalisés directement depuis/vers la mémoire GPU.\n",
    "- Synchronisation inter-rangs avant/après le calcul (MPI_Barrier) pour bien délimiter les phases de calcul scientifique dans les mesures de temps.\n",
    "- Réduction MPI (`MPI_Allreduce`) pour calculer l’erreur globale de convergence sur l’ensemble des sous-domaines (chaque GPU calcule l’erreur locale avec Thrust, puis réduction MPI).\n",
    "- Regroupement et affichage des résultats sur le rang 0 via des communications MPI pour rassembler toute la grille.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28376d3-241c-43b1-ac81-64c993bb6281",
   "metadata": {},
   "source": [
    "## Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34e00f77-d58b-493d-8904-fe87c7ccec7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir -p nsight\n",
      "# Adapte la ligne ci-dessous selon les arguments à donner à ton programme\n",
      "nsys profile -o nsight/jacobi_cuda_profile \\\n",
      "\tmpirun -np 4 ./jacobi_cuda 4096 4096 10000 1e-6 0\n",
      "Solveur Jacobi MPI+multiGPU convergé en 10000 itérations (erreur = 2.420e-05)\n",
      "Temps calcul MPI+multiGPU (hors init): 10.474947 s\n",
      "Temps total du programme (allocs, MPI, init, calcul, etc.): 12.065351 s\n",
      "Collecting data...\n",
      "Generating '/tmp/nsys-report-35d9.qdstrm'\n",
      "[1/1] [========================100%] jacobi_cuda_profile.nsys-rep\n",
      "Generated:\n",
      "    /gpfs/home/scortinhal/CHPS0904/MultiGPU/MPI-GPU/nsight/jacobi_cuda_profile.nsys-rep\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd MPI-GPU\n",
    "make nsight\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "857c76fc-5659-4962-a0fa-1b8f4c3e17a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solveur Jacobi MPI+multiGPU convergé en 1000 itérations (erreur = 2.422e-04)\n",
      "Temps calcul MPI+multiGPU (hors init): 1.312076 s\n",
      "Temps total du programme (allocs, MPI, init, calcul, etc.): 2.197987 s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd MPI-GPU\n",
    "\n",
    "mpirun -np 4 ./jacobi_cuda 4096 4096 1000 1e-6 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3760f8-4fa9-4239-9a0e-db4ce6f389c1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93b5e5d9-e106-444f-9e9d-b792fd9d445c",
   "metadata": {},
   "source": [
    "# Partie 4 : MPI + Overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772c3301-4f6c-4226-bf97-0727543532bc",
   "metadata": {},
   "source": [
    "- Modif pour  **asynchrone et overlap** entre les communications MPI (transferts des halos) et les calculs sur le GPU.\n",
    "- Utilisation de **plusieurs streams CUDA** :\n",
    "  - Un stream pour chaque bande (top, bottom, intérieur) afin de lancer simultanément calculs et transferts.\n",
    "  - Un stream dédié aux copies mémoire host/device (cudaMemcpyAsync) pour les halos.\n",
    "- Implémentation du **schéma overlap** :\n",
    "  - Les halos (zones tampon en haut et en bas du sous-domaine, servant à stocker les valeurs échangées avec les voisins MPI et nécessaires pour calculer les points en bordure) sont d’abord transférés du GPU vers le CPU de façon asynchrone.\n",
    "  - Pendant que les échanges MPI asynchrones (MPI_Isend/Irecv) des halos sont en cours, le calcul sur la bande intérieure (qui ne dépend pas des halos) est lancé.\n",
    "  - Dès que les halos sont reçus, ils sont recopiés du CPU vers le GPU (toujours en asynchrone) ; on attend la fin de ce transfert avant de calculer les bandes du haut et du bas, dépendantes des halos.\n",
    "- Synchronisation sur les streams CUDA pour garantir la cohérence des calculs (on synchronise seulement ce qui est nécessaire, pas tout le GPU à chaque étape).\n",
    "- Attente asynchrone des communications MPI pour maximiser l’overlap (on lance la réduction Thrust/Allreduce seulement quand tout est fini).\n",
    "- La convergence reste mesurée avec **Thrust** (comme avant : `transform_reduce` calcule directement sur le GPU le maximum des différences entre deux itérations, sans repasser par le CPU).\n",
    "- Ce schéma permet d’**overlap** le calcul et la communication\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df5e3100-2c30-4051-a06e-22f1a7c1d9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir -p nsight\n",
      "# Adapte la ligne ci-dessous selon les arguments à donner à ton programme\n",
      "nsys profile -o nsight/jacobi_cuda_profile \\\n",
      "\tmpirun -np 4 ./jacobi_cuda 4096 4096 10000 1e-6 0\n",
      "Solveur Jacobi MPI+multiGPU (full overlap memcpyasync) convergé en 10000 itérations (error = 2.420e-05)\n",
      "Temps calcul MPI+multiGPU (hors alloc/init): 10.920290 s\n",
      "Temps total du programme (tout inclus): 12.466443 s\n",
      "Collecting data...\n",
      "Generating '/tmp/nsys-report-b766.qdstrm'\n",
      "[1/1] [========================100%] jacobi_cuda_profile.nsys-rep\n",
      "Generated:\n",
      "    /gpfs/home/scortinhal/CHPS0904/MultiGPU/MPI-GPU-overlap/nsight/jacobi_cuda_profile.nsys-rep\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd MPI-GPU-overlap\n",
    "make nsight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9438358-adbd-4f00-a3d1-bffbaeb305d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solveur Jacobi MPI+multiGPU (full overlap memcpyasync) convergé en 1000 itérations (error = 2.422e-04)\n",
      "Temps calcul MPI+multiGPU (hors alloc/init): 1.110611 s\n",
      "Temps total du programme (tout inclus): 1.979046 s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd MPI-GPU-overlap\n",
    "mpirun -np 4 ./jacobi_cuda 4096 4096 1000 1e-6 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ecc13c-f926-4dbe-9b3e-b79a668fc751",
   "metadata": {},
   "source": [
    "# Partie 5 : NCCL\n",
    "\n",
    "\n",
    "- Utilisation de **NCCL** (NVIDIA Collective Communication Library) pour les communications entre GPUs, à la place de MPI pur ou CUDA-aware MPI.\n",
    "    - Les échanges de halos (zones tampon haut/bas entre sous-domaines voisins) se font ici directement entre GPUs via `ncclSend` et `ncclRecv`, de façon totalement device-to-device, sans passer par la RAM du CPU.\n",
    "    - Le schéma d’échange est `ncclGroupStart/ncclGroupEnd` pour lancer plusieurs envois/réceptions en une seule opération collective, ce qui améliore le débit.\n",
    "- Initialisation de NCCL : un communicateur NCCL est créé pour permettre les échanges collectifs entre tous les GPUs.\n",
    "- Synchronisation sur un **stream CUDA** après les échanges de halos NCCL, pour garantir que les données sont prêtes avant le calcul.\n",
    "- Le calcul local du Jacobi reste inchangé : il est fait sur le GPU via un kernel CUDA.\n",
    "- La mesure de la convergence utilise toujours **Thrust** pour faire le calcul de la norme maximale sur GPU.\n",
    "- La réduction finale de l’erreur (convergence globale) se fait par **MPI_Allreduce** : NCCL n’est utilisé que pour les échanges de halos, la convergence reste synchronisée au niveau MPI.\n",
    "- Les chronos sont placés pour mesurer :\n",
    "    - Le temps global (du début à la fin du programme).\n",
    "    - Le temps après initialisation de NCCL.\n",
    "    - Le temps pur de la boucle Jacobi.\n",
    "- L'affichage/réassemblage final des résultats reste identique aux versions précédentes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddff1e20-d7f2-46eb-ad60-8437da34ecda",
   "metadata": {},
   "source": [
    "## Compilation et execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4388d1e5-0243-428b-832f-cd06a8b61460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make: Nothing to be done for 'all'.\n",
      "Solveur Jacobi NCCL convergé en 1000 itérations (error = 2.422e-04)\n",
      "1. Temps total du programme (MPI_Init → fin)                : 6.495399 s\n",
      "2. Temps après init NCCL (juste avant alloc/init CUDA)      : 1.747198 s\n",
      "3. Temps NCCL (calcul pur boucle Jacobi)                    : 1.744446 s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd NCCL\n",
    "\n",
    "eval $(spack load --sh nccl)\n",
    "export LD_LIBRARY_PATH=$(spack location -i nccl)/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "make\n",
    "mpirun -np 4 ./jacobi_nccl 4096 4096 1000 1e-6 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5bd7b3-2412-4543-af79-8133db1e5c65",
   "metadata": {},
   "source": [
    "# Partie 6 : NCCL - Overlap\n",
    "\n",
    "- Utilisation de **NCCL** pour l’échange des halos entre GPUs, comme dans la version précédente, mais :\n",
    "    - On applique un **overlap** entre communications et calcul :\n",
    "        - Les échanges de halos (lignes du haut et du bas de chaque sous-domaine, nécessaires pour la continuité de la solution) se font via `ncclSend`/`ncclRecv` sur deux streams CUDA distincts (`stream_halo_top`, `stream_halo_bot`).\n",
    "        - Pendant que les transferts NCCL s’exécutent, on lance immédiatement le calcul Jacobi :\n",
    "            - Sur les bandes extrêmes du sous-domaine (`iy = 1` pour le haut, `iy = local_ny` pour le bas) sur les mêmes streams que les communications associées.\n",
    "            - Sur l’intérieur du domaine (lignes qui ne dépendent pas des halos) sur un troisième stream CUDA (`stream_interior`).\n",
    "    - On effectue une synchronisation (`cudaStreamSynchronize`) sur chaque stream avant de poursuivre, pour garantir que calculs et transferts sont bien terminés.\n",
    "- Ce **recouvrement comm/calcul** (overlap) permet de gagner du temps si le coût des transferts et du calcul sont similaires ou si l’un des deux prend plus de temps que l’autre.\n",
    "- Le découpage en bandes (slice) pour les kernels Jacobi permet ce parallélisme : on peut commencer à travailler sur ce qui est prêt sans attendre toute la communication.\n",
    "- Calcul de la convergence toujours via Thrust sur GPU, puis synchronisation globale via `MPI_Allreduce`.\n",
    "- Affichage et réassemblage final identiques aux versions précédentes.\n",
    "- Multiples chronomètres pour séparer le temps total, le temps d’allocation/init CUDA/NCCL, et le temps pur de la boucle Jacobi avec overlap.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5c553686-9caa-4982-bcf9-ac4a1e2182d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make: Nothing to be done for 'all'.\n",
      "NCCL Comm Init: 1.053 s\n",
      "Solveur Jacobi NCCL + overlap convergé en 1000 itérations (error = 2.422e-04)\n",
      "1. Temps total du programme (MPI_Init → fin)                 : 2.094690 s\n",
      "2. Temps après init NCCL (juste avant alloc/init CUDA)       : 0.380852 s\n",
      "3. Temps calcul NCCL + overlap (boucle Jacobi uniquement)    : 0.379018 s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd NCCL-overlap\n",
    "\n",
    "eval $(spack load --sh nccl)\n",
    "export LD_LIBRARY_PATH=$(spack location -i nccl)/lib:$LD_LIBRARY_PATH\n",
    "export NCCL_IB_HCA=mlx5_0         # si tu as Infiniband\n",
    "export NCCL_IB_DISABLE=0\n",
    "export NCCL_P2P_DISABLE=1\n",
    "make\n",
    "mpirun -np 1 ./jacobi_nccl 4096 4096 1000 1e-6 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51849b41-74e4-401a-a2d8-64867697436b",
   "metadata": {},
   "source": [
    "# Partie 7 : NCCL - graph\n",
    "\n",
    "- Ajout de l'utilisation des **CUDA Graphs** pour la capture et l'exécution de la boucle Jacobi :\n",
    "    - On capture la séquence \"comm/halo + calcul Jacobi (bande intérieures et extrêmes) + swap\" en graph CUDA, avec deux variantes :\n",
    "        - un graph sans calcul d’erreur,\n",
    "        - un graph qui inclut le calcul d’erreur locale toutes les 10 itérations (ou à une fréquence réglable).\n",
    "    - Cela permet d’**accélérer les itérations** (moins d’overhead de lancement de kernels, moins de passages CPU/GPU, ordonnancement optimisé par CUDA).\n",
    "- L'attribution du GPU à chaque processus MPI se fait proprement via le \"local_rank\" obtenu à partir du communicator partagé MPI (pour que deux processus MPI du même nœud ne prennent pas le même GPU).\n",
    "- Toujours un découpage en sous-domaines horizontaux équilibré même si ny-2 n'est pas multiple du nombre de processus.\n",
    "- Toujours overlap entre communications (NCCL) et calcul (multi-streams CUDA), mais cette logique est incluse à l’intérieur du CUDA Graph.\n",
    "- Nettoyage mémoire : destruction des graphs et exécuteurs (`cudaGraphExecDestroy`) à la fin.\n",
    "- Pour le reste : gestion des halos (lignes fantômes haut/bas pour l’échange entre voisins), calcul local Jacobi en slice, calcul de l’erreur via Thrust puis réduction MPI pour la convergence globale, impression et réassemblage final.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35845d0f-801b-4e8f-a94b-8d330fe19d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make: Nothing to be done for 'all'.\n",
      "Solveur Jacobi NCCL+Graphs convergé en 1000 itérations (error = 2.442e-04) en 0.148811s\n",
      "Error while terminating subprocess (pid=3719123): \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd NCCL-graph\n",
    "\n",
    "eval $(spack load --sh nccl)\n",
    "export LD_LIBRARY_PATH=$(spack location -i nccl)/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "make\n",
    "mpirun -np 4 ./jacobi_nccl 4096 4096 1000 1e-6 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554f7281-d79c-4257-84ce-3567d78a22a1",
   "metadata": {},
   "source": [
    "# Partie 8 : NVSHMEM\n",
    "\n",
    "- Introduction de **NVSHMEM** comme méthode de communication pour les échanges de halos entre GPUs, en remplacement de NCCL ou MPI direct :\n",
    "    - NVSHMEM permet le \"Remote Memory Access\" (RMA) : chaque GPU peut écrire directement dans la mémoire d’un voisin (put), sans impliquer le CPU, ce qui réduit la latence et le coût des échanges de halos.\n",
    "    - Utilisation de `nvshmem_double_put` pour écrire directement la première/dernière ligne réelle dans le halo du voisin.\n",
    "    - Utilisation de `nvshmem_fence()` pour garantir la visibilité des échanges mémoire, et `nvshmem_barrier_all()` pour synchroniser tous les processus avant de lancer le calcul.\n",
    "- Initialisation avancée :\n",
    "    - Calcul et mise à disposition de la taille réelle des buffers (ghost_ny) sur chaque PE pour gérer les halos même si la décomposition est déséquilibrée (i.e., tous les processus n'ont pas le même nombre de lignes).\n",
    "    - Allocation des tableaux via `nvshmem_malloc` (heap symétrique, identique sur tous les PE).\n",
    "    - Calcul et configuration dynamique de la taille du heap symétrique avec la variable d’environnement `NVSHMEM_SYMMETRIC_SIZE`.\n",
    "    - Initialisation de NVSHMEM avec le communicateur MPI pour garantir le mapping entre processus MPI et PE NVSHMEM.\n",
    "- Toujours le calcul Jacobi en kernel CUDA sur le sous-domaine local, synchronisation CUDA stream comme dans les autres versions.\n",
    "- Utilisation de **Thrust** pour la réduction GPU de l’erreur max entre a_new et a (calcul de convergence), puis MPI_Allreduce pour la convergence globale.\n",
    "- Affichage des chronos \n",
    "- Rassemblement final et affichage de la grille comme dans les autres versions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c8509a4-88ea-43bb-ac9e-ae0c81f3bf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "export NVSHMEM_HOME=/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/nvshmem\n",
    "export CPATH=$NVSHMEM_HOME/include:$CPATH\n",
    "export LD_LIBRARY_PATH=$NVSHMEM_HOME/lib:$LD_LIBRARY_PATH\n",
    "export PATH=$NVSHMEM_HOME/bin:$PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ec800b66-9455-4cea-8cbd-2cb99d6c2501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f jacobi_nvshmem *.o *.qdrep *.sqlite\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd NVSHMEM\n",
    "make clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78e010da-4b63-4d81-9782-7f7856db762e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make: Nothing to be done for 'all'.\n",
      "[NVSHMEM] Temps d'init avant NVSHMEM : 2.531944s\n",
      "[NVSHMEM] Temps d'init NVSHMEM : 4.115990s\n",
      "[NVSHMEM] Converged in 1000 iterations | Error: 2.42e-04\n",
      "Temps de calcul Jacobi (seulement boucle)         : 0.127134s\n",
      "Temps setup+calcul+affichage (après init SHMEM)   : 0.127302s\n",
      "Temps total du programme (tout compris)           : 7.639148s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd NVSHMEM\n",
    "\n",
    "# Charge NVSHMEM et NCCL\n",
    "export NVSHMEM_HOME=/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/nvshmem\n",
    "export LD_LIBRARY_PATH=$NVSHMEM_HOME/lib:$LD_LIBRARY_PATH\n",
    "export CPATH=$NVSHMEM_HOME/include:$CPATH\n",
    "export PATH=$NVSHMEM_HOME/bin:$PATH\n",
    "\n",
    "eval $(spack load --sh nccl)\n",
    "export LD_LIBRARY_PATH=$(spack location -i nccl)/lib:$LD_LIBRARY_PATH\n",
    "export NVSHMEM_ENABLE_ALL_DEVICE_INLINING=1\n",
    "\n",
    "\n",
    "make \n",
    "mpirun -np 4 ./jacobi_nvshmem 4096 4096 1000 1e-6 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa7c996-4c15-4a89-9241-d7555763ce83",
   "metadata": {},
   "source": [
    "# Partie 9 : NVSHMEM-LTO\n",
    "\n",
    "- Ajout du support de la compilation **LTO CUDA** (Link-Time Optimization) dans le Makefile :\n",
    "    - Ajout du flag `-gencode=arch=compute_90,code=lto_90` à la variable NVCCFLAGS pour activer la génération du code intermédiaire LTO pour l’architecture sm_90.\n",
    "    - Pas de modification du code source, uniquement une optimisation de la génération du binaire final via la chaîne de compilation NVCC pour de meilleures performances potentielles à l’exécution.\n",
    "    - Optimiser les appels de fonctions entre différents fichiers sources CUDA (inlining, suppression de code mort, etc.)\n",
    "    - Réorganiser ou fusionner des kernels s’il le juge pertinent.\n",
    "    - Générer un code plus performant, parfois plus rapide à l’exécution que le même code compilé sans LTO.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f68c86fb-c880-411f-bdf0-81a5bd37c927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f jacobi_nvshmem *.o *.qdrep *.sqlite\n",
      "nvcc -O3 -arch=sm_90  -std=c++17 --expt-relaxed-constexpr -rdc=true -gencode=arch=compute_90,code=sm_90 -gencode=arch=compute_90,code=lto_90 -Xcompiler \"-Wall -Wextra\" -ccbin=mpicxx -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/nvshmem/include -I/apps/2025/spack_install/linux-rhel9-neoverse_v2/linux-rhel9-neoverse_v2/gcc-11.4.1/cuda-12.6.2-3mzltpzgs4dekx5xvbqzz2no3j3tkcxq/include -o jacobi_nvshmem jacobi.cu -L/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/nvshmem/lib -lnvshmem -lnvshmem_host -L/apps/2025/spack_install/linux-rhel9-neoverse_v2/linux-rhel9-neoverse_v2/gcc-11.4.1/cuda-12.6.2-3mzltpzgs4dekx5xvbqzz2no3j3tkcxq/lib64 -lcudart -lmpi\n",
      "[NVSHMEM] Temps d'init avant NVSHMEM : 2.511211s\n",
      "[NVSHMEM] Temps d'init NVSHMEM : 4.076924s\n",
      "[NVSHMEM] Converged in 1000 iterations | Error: 2.42e-04\n",
      "Temps de calcul Jacobi (seulement boucle)         : 0.129117s\n",
      "Temps setup+calcul+affichage (après init SHMEM)   : 0.129289s\n",
      "Temps total du programme (tout compris)           : 6.973905s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd NVSHMEM-LTO\n",
    "\n",
    "\n",
    "export NVSHMEM_HOME=/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/nvshmem\n",
    "export CUDA_HOME=/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.3\n",
    "export LD_LIBRARY_PATH=$NVSHMEM_HOME/lib:$CUDA_HOME/lib64:$LD_LIBRARY_PATH\n",
    "export CPATH=$NVSHMEM_HOME/include:$CUDA_HOME/include:$CPATH\n",
    "export PATH=$NVSHMEM_HOME/bin:$CUDA_HOME/bin:$PATH\n",
    "\n",
    "# Charge NCCL via Spack si tu utilises aussi NCCL ailleurs\n",
    "eval $(spack load --sh nccl)\n",
    "export LD_LIBRARY_PATH=$(spack location -i nccl)/lib:$LD_LIBRARY_PATH\n",
    "export NVSHMEM_ENABLE_ALL_DEVICE_INLINING=1\n",
    "\n",
    "make clean\n",
    "make\n",
    "#  lance avec mpirun (remplace nvshmrun si dispo) :\n",
    "mpirun -np 4 ./jacobi_nvshmem 4096 4096 1000 1e-6 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361ada1e-aea3-4cb6-ac4f-2b930c159cad",
   "metadata": {},
   "source": [
    "# Partie 10 : NVSHMEM-neighborhood_sync-lto\n",
    "\n",
    "\n",
    "- Utilisation de **NVSHMEM** pour les échanges de halos entre GPUs (Remote Memory Access/RMA) :\n",
    "    - Chaque processus écrit directement dans la mémoire du halo (bord) de ses voisins grâce à `nvshmem_double_put`.\n",
    "    - Synchronisation mémoire assurée avec `nvshmem_fence()` puis barrière NVSHMEM pour garantir la cohérence avant le calcul.\n",
    "- Gestion avancée de la **synchronisation neighborhood** sur device (entre GPU voisins) :\n",
    "    - Ajout d’un kernel `syncneighborhood_kernel` pour notifier les voisins haut et bas que les halos sont prêts (utilisation de `nvshmemx_signal_op` et `nvshmem_uint64_wait_until_all` pour attendre les notifications des deux côtés).\n",
    "- Initialisation :\n",
    "    - Attribution automatique des GPU locaux par rang MPI intra-nœud.\n",
    "    - Calcul dynamique des tailles de sous-domaines (ghost_ny) pour chaque processus, prise en compte des décompositions déséquilibrées.\n",
    "    - Allocation de la mémoire sur le heap symétrique NVSHMEM (`nvshmem_malloc`) pour les tableaux et les variables de synchronisation, avec configuration dynamique de la variable d'environnement `NVSHMEM_SYMMETRIC_SIZE` pour garantir un espace mémoire suffisant.\n",
    "    - Initialisation de NVSHMEM avec un communicateur MPI personnalisé via `nvshmemx_init_attr`.\n",
    "- Calcul Jacobi toujours exécuté sur GPU via kernel CUDA, synchronisation des streams CUDA.\n",
    "- Calcul de l’erreur max avec **Thrust** sur GPU (utilisation de la bibliothèque Thrust pour la réduction GPU des différences a_new/a_old), puis MPI_Allreduce pour la convergence globale.\n",
    "- Mesure détaillée des temps : temps avant et après initialisation NVSHMEM, temps de calcul pur, temps total global, etc.\n",
    "- Rassemblement final et affichage de la grille sur le rang 0 avec gestion correcte des tailles locales, comme dans les versions précédentes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7093a9d-f001-490b-bf43-06a84951ae4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f jacobi_nvshmem *.o *.qdrep *.sqlite\n",
      "nvcc -O3 -arch=sm_90  -std=c++17 --expt-relaxed-constexpr -rdc=true -gencode=arch=compute_90,code=sm_90 -gencode=arch=compute_90,code=lto_90 -Xcompiler \"-Wall -Wextra\" -ccbin=mpicxx -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/nvshmem/include -I/apps/2025/spack_install/linux-rhel9-neoverse_v2/linux-rhel9-neoverse_v2/gcc-11.4.1/cuda-12.6.2-3mzltpzgs4dekx5xvbqzz2no3j3tkcxq/include -o jacobi_nvshmem jacobi.cu -L/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/nvshmem/lib -lnvshmem -lnvshmem_host -L/apps/2025/spack_install/linux-rhel9-neoverse_v2/linux-rhel9-neoverse_v2/gcc-11.4.1/cuda-12.6.2-3mzltpzgs4dekx5xvbqzz2no3j3tkcxq/lib64 -lcudart -lmpi\n",
      "[NVSHMEM-neighSync] Converged in 1000 iterations | Error: 2.42e-04\n",
      "Temps de calcul Jacobi (seulement boucle)         : 0.122526s\n",
      "Temps setup+calcul+affichage (après init SHMEM)   : 0.122720s\n",
      "Temps d'init avant NVSHMEM                        : 2.492420s\n",
      "Temps d'init NVSHMEM                              : 4.072520s\n",
      "Temps total du programme (tout compris)           : 6.850103s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd NVSHMEM-neighborhood_sync-lto\n",
    "\n",
    "# Charge les bonnes librairies NVSHMEM et CUDA\n",
    "export NVSHMEM_HOME=/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/nvshmem\n",
    "export CUDA_HOME=/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.3\n",
    "export LD_LIBRARY_PATH=$NVSHMEM_HOME/lib:$CUDA_HOME/lib64:$LD_LIBRARY_PATH\n",
    "export CPATH=$NVSHMEM_HOME/include:$CUDA_HOME/include:$CPATH\n",
    "export PATH=$NVSHMEM_HOME/bin:$CUDA_HOME/bin:$PATH\n",
    "\n",
    "# Charge NCCL via Spack si nécessaire\n",
    "eval $(spack load --sh nccl)\n",
    "export LD_LIBRARY_PATH=$(spack location -i nccl)/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "# Active l'inlining maximal NVSHMEM\n",
    "export NVSHMEM_ENABLE_ALL_DEVICE_INLINING=1\n",
    "\n",
    "make clean && make\n",
    "\n",
    "mpirun -np 4 ./jacobi_nvshmem 4096 4096 1000 1e-6 0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91144fa-f2d8-4398-a778-f3dfbe61a129",
   "metadata": {},
   "source": [
    "# Partie 11 : nvshmem-norm_overlap-neighborhood_sync+lto \n",
    "\n",
    "- le **calcul sur la grille locale est vraiment overlap avec l'échange des halos grâce à un découpage fin des kernels et à la synchronisation asynchrone device-side.\n",
    "\n",
    "- On découpe la mise à jour Jacobi en :\n",
    "    - `jacobi_inner_kernel` (zone intérieure, ne dépend pas du halo) qui est lancé pendant que les halos sont envoyés aux voisins.\n",
    "    - Une fois l'échange de halos terminé (vérifié via le kernel `syncneighborhood_kernel` qui utilise des signaux device NVSHMEMX),  \n",
    "      on lance `jacobi_border_kernel` sur les deux bandes frontalières haut/bas qui ont besoin du halo à jour.\n",
    "  Ce découpage permet de **masquer le coût de communication** et d'accélérer la convergence pour des grosses grilles.\n",
    "\n",
    "- **Synchronisation neighborhood**: au lieu d'une barrière globale (`nvshmem_barrier_all`), chaque processus attend juste un signal de ses deux voisins directs (haut/bas) pour la cohérence des halos : cela limite l'attente et améliore l'overlap comm/calcul.\n",
    "\n",
    "- Le reste de la structure générale (allocation heap symétrique, gestion des tailles fantômes variables, calcul erreur avec thrust, réduction MPI_Allreduce, réassemblage final) reste identique à la version précédente.\n",
    "\n",
    "- L'objectif est de **maximiser le recouvrement entre calcul local et communication** des halos, ce qui est particulièrement efficace sur des architectures GPU modernes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8280652-6caf-4dbb-9593-70015d37395f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f jacobi_nvshmem *.o *.qdrep *.sqlite\n",
      "nvcc -O3 -arch=sm_90  -std=c++17 --expt-relaxed-constexpr -rdc=true -gencode=arch=compute_90,code=sm_90 -gencode=arch=compute_90,code=lto_90 -Xcompiler \"-Wall -Wextra\" -ccbin=mpicxx -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/nvshmem/include -I/apps/2025/spack_install/linux-rhel9-neoverse_v2/linux-rhel9-neoverse_v2/gcc-11.4.1/cuda-12.6.2-3mzltpzgs4dekx5xvbqzz2no3j3tkcxq/include -o jacobi_nvshmem jacobi.cu -L/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/nvshmem/lib -lnvshmem -lnvshmem_host -L/apps/2025/spack_install/linux-rhel9-neoverse_v2/linux-rhel9-neoverse_v2/gcc-11.4.1/cuda-12.6.2-3mzltpzgs4dekx5xvbqzz2no3j3tkcxq/lib64 -lcudart -lmpi\n",
      "[NVSHMEM-norm_overlap-neighSync+LTO] Converged in 1000 iterations | Error: 2.42e-04\n",
      "Temps de calcul Jacobi (seulement boucle)         : 0.110126s\n",
      "Temps setup+calcul+affichage (après init SHMEM)   : 0.110322s\n",
      "Temps d'init avant NVSHMEM                        : 2.480157s\n",
      "Temps d'init NVSHMEM                              : 4.116901s\n",
      "Temps total du programme (tout compris)           : 7.095077s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd NVSHMEM-norm_overlap-neighborhood_sync+lto\n",
    "\n",
    "# Charge les bonnes librairies NVSHMEM et CUDA\n",
    "export NVSHMEM_HOME=/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/nvshmem\n",
    "export CUDA_HOME=/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.3\n",
    "export LD_LIBRARY_PATH=$NVSHMEM_HOME/lib:$CUDA_HOME/lib64:$LD_LIBRARY_PATH\n",
    "export CPATH=$NVSHMEM_HOME/include:$CUDA_HOME/include:$CPATH\n",
    "export PATH=$NVSHMEM_HOME/bin:$CUDA_HOME/bin:$PATH\n",
    "\n",
    "# Charge NCCL via Spack si nécessair\n",
    "\n",
    "eval $(spack load --sh nccl)\n",
    "export LD_LIBRARY_PATH=$(spack location -i nccl)/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "# Active l'inlining maximal NVSHMEM\n",
    "export NVSHMEM_ENABLE_ALL_DEVICE_INLINING=1\n",
    "\n",
    "make clean && make\n",
    "\n",
    "mpirun -np 4 ./jacobi_nvshmem 4096 4096 1000 1e-6 0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bef36e4-97e0-49c0-b629-5bce390fbeb2",
   "metadata": {},
   "source": [
    "# Partie 12 : nvshmem-norm_overlap-neighborhood_sync+lto[1]  \n",
    "\n",
    "\n",
    "- **Ici, c'est une version \"baseline\" (de base, séquentielle côté device)** :\n",
    "    - On utilise **un seul kernel Jacobi** (`jacobi_kernel_full`) qui fait tout le calcul d'un coup sur tout le sous-domaine local, sans découpage ni overlap calcul/communication.\n",
    "    - **Pas de découpage** en \"zone intérieure\" et \"zone de bord\" : tout est fait d’un bloc, donc la communication des halos ne peut pas être masquée par le calcul local.\n",
    "    - **Pas de synchronisation neighborhood device** (pas de kernel de synchronisation avec les voisins via NVSHMEMX), ni de stratégie fine pour attendre juste les voisins.\n",
    "    - On se contente d’un schéma classique : \n",
    "        - On fait tout le calcul local,\n",
    "        - Puis l’erreur est calculée (avec thrust),\n",
    "        - Puis un `MPI_Allreduce` pour la convergence globale,\n",
    "        - Et on passe à l’itération suivante.\n",
    "\n",
    "- **En résumé :**  \n",
    "  - Version \"simple\" qui sert de référence pour les perfs : pas d’overlap calcul/comm, pas d’optimisation, pas de synchronisation fine, **juste un Jacobi standard sur tout le domaine local**.\n",
    "  - Cela permet de comparer l’apport réel des optimisations dans les autres versions (découpage kernels + overlap, synchronisation neighborhood, etc).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f28e42c0-e360-4fa8-a2af-65a4b823fa2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f jacobi_nvshmem *.o *.qdrep *.sqlite\n",
      "nvcc -O3 -arch=sm_90  -std=c++17 --expt-relaxed-constexpr -rdc=true -gencode=arch=compute_90,code=sm_90 -gencode=arch=compute_90,code=lto_90 -Xcompiler \"-Wall -Wextra\" -ccbin=mpicxx -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/nvshmem/include -I/apps/2025/spack_install/linux-rhel9-neoverse_v2/linux-rhel9-neoverse_v2/gcc-11.4.1/cuda-12.6.2-3mzltpzgs4dekx5xvbqzz2no3j3tkcxq/include -o jacobi_nvshmem jacobi.cu -L/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/nvshmem/lib -lnvshmem -lnvshmem_host -L/apps/2025/spack_install/linux-rhel9-neoverse_v2/linux-rhel9-neoverse_v2/gcc-11.4.1/cuda-12.6.2-3mzltpzgs4dekx5xvbqzz2no3j3tkcxq/lib64 -lcudart -lmpi\n",
      "[NVSHMEM-baseline-single-rank] Converged in 1000 iterations | Error: 2.42e-04\n",
      "Temps de calcul Jacobi (seulement boucle)         : 0.087986s\n",
      "Temps setup+calcul+affichage (après init SHMEM)   : 0.088175s\n",
      "Temps d'init avant NVSHMEM                        : 2.522717s\n",
      "Temps d'init NVSHMEM                              : 4.209264s\n",
      "Temps total du programme (tout compris)           : 7.769264s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd NVSHMEM-norm_overlap-neighborhood_sync+lto[1]\n",
    "\n",
    "# Charge les bonnes librairies NVSHMEM et CUDA\n",
    "export NVSHMEM_HOME=/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/nvshmem\n",
    "export CUDA_HOME=/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.3\n",
    "export LD_LIBRARY_PATH=$NVSHMEM_HOME/lib:$CUDA_HOME/lib64:$LD_LIBRARY_PATH\n",
    "export CPATH=$NVSHMEM_HOME/include:$CUDA_HOME/include:$CPATH\n",
    "export PATH=$NVSHMEM_HOME/bin:$CUDA_HOME/bin:$PATH\n",
    "\n",
    "# Charge NCCL via Spack si nécessair\n",
    "\n",
    "eval $(spack load --sh nccl)\n",
    "export LD_LIBRARY_PATH=$(spack location -i nccl)/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "# Active l'inlining maximal NVSHMEM\n",
    "export NVSHMEM_ENABLE_ALL_DEVICE_INLINING=1\n",
    "\n",
    "make clean && make\n",
    "\n",
    "mpirun -np 4 ./jacobi_nvshmem 4096 4096 1000 1e-6 0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ab51b5-354a-4dbe-a2a3-052dbb4e5c62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
