{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#65AE11;\">Memory Copies in Non-Default Streams</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section you will perform host-to-device and device-to-host memory transfers in non-default streams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#65AE11;\">Objectives</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the time you complete this section you will:\n",
    "\n",
    "* Know how to create pinned memory, which can be asynchronously transfered in non-default streams\n",
    "* Be able to perform host-to-device memory transfers in non-default streams\n",
    "* Be able to perform device-to-host memory transfers in non-default streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#65AE11;\">Allocating Pinned Memory</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to asynchronously copy data, CUDA needs to make assumptions about its location. Typical host memory uses [paging](https://en.wikipedia.org/wiki/Paging) so that in addition to RAM, data can be stored on some backup storage device like a physical disk.\n",
    "\n",
    "Pinning, or page-locking memory bypasses host OS paging, storing allocated memory in RAM. Page-locking, or pinning memory is required to transfer memory asynchronously in a non-default stream.\n",
    "\n",
    "Because it prevents storage of data on some backup storage, pinned memory is a limited resource, and care must be taken not to over use it.\n",
    "\n",
    "Pinned host memory is allocated with `cudaMallocHost`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```c\n",
    "const uint64_t num_entries = 1UL << 26;\n",
    "uint64_t *data_cpu;\n",
    "cudaMallocHost(&data_cpu, sizeof(uint64_t)*num_entries);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```c\n",
    "const uint64_t num_entries = 1UL << 26;\n",
    "uint64_t *data_cpu;\n",
    "cudaMallocHost(&data_cpu, sizeof(uint64_t)*num_entries);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#65AE11;\">Host-to-Device Memory Transfers in a Non-Default Stream</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pinned host memory can be transfered to GPU memory in a non-default stream using `cudaMemcpyAsync` which is similar to `cudaMemcpy` but expects a 5th stream identifier argument:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```c\n",
    "cudaStream_t stream;\n",
    "cudaStreamCreate(&stream);\n",
    "\n",
    "const uint64_t num_entries = 1UL << 26;\n",
    "\n",
    "uint64_t *data_cpu, *data_gpu;\n",
    "\n",
    "cudaMallocHost(&data_cpu, sizeof(uint64_t)*num_entries);\n",
    "cudaMalloc(&data_cpu, sizeof(uint64_t)*num_entries);\n",
    "\n",
    "cudaMemcpyAsync(data_gpu, \n",
    "                data_cpu, \n",
    "                sizeof(uint64_t)*num_entries, \n",
    "                cudaMemcpyHostToDevice, \n",
    "                stream);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#65AE11;\">Device-to-Host Memory Transfers in a Non-Default Stream</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU memory can be also be transfered to pinned host memory in a non-default stream using `cudaMemcpyAsync`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```c\n",
    "// Assume data is already present on the GPU, and that `data_cpu` is pinned.\n",
    "\n",
    "cudaMemcpyAsync(data_cpu, \n",
    "                data_gpu, \n",
    "                sizeof(uint64_t)*num_entries, \n",
    "                cudaMemcpyDeviceToHost, \n",
    "                stream);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is the case with all modern GPUs, GPU devices with 2 or more copy engines can perform host-to-device and device-to-host memory transfers in different non-default streams at the same time. You will do this yourself later in the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#65AE11;\">Stream Synchronization</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `cudaStreamSynchronize` will cause host code to block until a given stream has completed its operations. Stream synchronization should be used when guarantees are needed about the completion of a stream's work, for example, when host code needs to wait for asynchronous memory transfers in a non-default stream to complete:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```c\n",
    "// Assume data is already present on the GPU, and that `data_cpu` is pinned.\n",
    "\n",
    "cudaMemcpyAsync(data_cpu, \n",
    "                data_gpu, \n",
    "                sizeof(uint64_t)*num_entries, \n",
    "                cudaMemcpyDeviceToHost, \n",
    "                stream);\n",
    "\n",
    "// Block until work (in this case memory transfer to host) in `stream` is complete.\n",
    "cudaStreamSynchronize(stream);\n",
    "\n",
    "// `data_cpu` transfer to host via `stream` is now guaranteed to be complete.\n",
    "checkResultCpu(data_cpu);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#65AE11;\">Exercise: Perform Memory Transfers in Non-Default Stream</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open and refactor [*07_Memcpy_in_Streams/baseline_cipher/baseline.cu*](baseline_cipher/baseline.cu) to perform both host-to-device and device-to-host memory transfers in a non-default stream.\n",
    "\n",
    "Generate a report file for the refactored application by using a JupyterLab terminal and running `make profile` from within the *07_Memcpy_in_Streams/baseline_cipher* directory. (See the [*Makefile*](baseline_cipher/Makefile) there for details).\n",
    "\n",
    "Open the report file in Nsight Systems. If you've closed the Nsight Systems tab, you can reopen it by following the instructions in [*Nsight Systems Setup*](../04_Nsight_Systems_Setup/Nsight_Systems_Setup.ipynb). As a reminder the password is `nvidia`.\n",
    "\n",
    "If you were successful, you should notice in the Nsight Systems visual timeline that memory transfers are now occuring in non-default streams, as is shown in the screenshot below.\n",
    "\n",
    "If you get stuck, please refer to [07_Memcpy_in_Streams/baseline_cipher/baseline_solution.cu](../07_Memcpy_in_Streams/baseline_cipher/baseline_solution.cu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gpfs/home/scortinhal/CHPS0904/task/07_Memcpy_in_Streams\n",
      "nsys profile --stats=true --force-overwrite=true -o baseline-report ./baseline\n",
      "TIMING: 24.0244 ms (allocate memory)\n",
      "TIMING: 15736.8 ms (encrypt data on CPU)\n",
      "TIMING: 1.92035 ms (copy data from CPU to GPU)\n",
      "TIMING: 33.8242 ms (decrypt data on GPU)\n",
      "TIMING: 3.18275 ms (copy data from GPU to CPU)\n",
      "TIMING: 39.0159 ms (total time on GPU)\n",
      "STATUS: test passed\n",
      "TIMING: 4.78464 ms (checking result on CPU)\n",
      "TIMING: 11.6836 ms (free memory)\n",
      "TIMING: 15816.6 ms (Temps total)\n",
      "Collecting data...\n",
      "Generating '/tmp/nsys-report-2710.qdstrm'\n",
      "[1/8] [========================100%] baseline-report.nsys-rep\n",
      "[2/8] [========================100%] baseline-report.sqlite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SKIPPED: /gpfs/home/scortinhal/CHPS0904/task/07_Memcpy_in_Streams/kernel_baseline_cipher/baseline-report.sqlite does not contain NV Tools Extension (NVTX) data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)     Med (ns)    Min (ns)  Max (ns)   StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  -----------  -----------  --------  ---------  -----------  ----------------------\n",
      "     52.0      16081260032        171   94042456.3  100113984.0      1568  154654784   24151459.0  poll                  \n",
      "     47.0      14524869280         63  230553480.6    4045184.0     32096  982468064  394648271.7  futex                 \n",
      "      1.0        301383936        851     354152.7      13568.0      1088   80187232    3073977.6  ioctl                 \n",
      "      0.0          6017472         38     158354.5       9184.0      2848    5612800     908761.4  mmap                  \n",
      "      0.0          5737280         17     337487.1     319296.0    305568     475840      49178.4  pthread_create        \n",
      "      0.0           716160         81       8841.5       5568.0      1920     132512      15422.7  fopen                 \n",
      "      0.0           539616         48      11242.0      10272.0      5568      22784       3245.8  mmap64                \n",
      "      0.0           421312         76       5543.6       5488.0      1216      14656       1979.8  open64                \n",
      "      0.0           365696         10      36569.6      30320.0     21120     104544      24591.5  sem_timedwait         \n",
      "      0.0           210208         37       5681.3       1600.0      1024     127040      20630.8  fclose                \n",
      "      0.0            83392          5      16678.4      10080.0      1376      47552      17937.4  fread                 \n",
      "      0.0            61440         12       5120.0       4608.0      1856      12256       3324.4  fflush                \n",
      "      0.0            61312          5      12262.4       4352.0      2688      41120      16338.0  fgets                 \n",
      "      0.0            53408          1      53408.0      53408.0     53408      53408          0.0  pthread_cond_wait     \n",
      "      0.0            53088          3      17696.0       5056.0      2592      45440      24058.6  fcntl                 \n",
      "      0.0            52864          9       5873.8       5920.0      1664      13152       3442.4  open                  \n",
      "      0.0            32640          6       5440.0       4928.0      1440      11872       3512.5  munmap                \n",
      "      0.0            31680          4       7920.0       4768.0      3840      18304       6947.5  fopen64               \n",
      "      0.0            31680         11       2880.0       3232.0      1248       4352       1142.7  write                 \n",
      "      0.0            25120          9       2791.1       1664.0      1024       6880       2195.6  read                  \n",
      "      0.0            24160          3       8053.3       9440.0      2400      12320       5103.3  pipe2                 \n",
      "      0.0            18720          2       9360.0       9360.0      7200      11520       3054.7  socket                \n",
      "      0.0            13568          1      13568.0      13568.0     13568      13568          0.0  connect               \n",
      "      0.0            10912          4       2728.0       2400.0      1184       4928       1578.2  fwrite                \n",
      "      0.0             2784          1       2784.0       2784.0      2784       2784          0.0  bind                  \n",
      "      0.0             2144          1       2144.0       2144.0      2144       2144          0.0  pthread_cond_broadcast\n",
      "      0.0             1248          1       1248.0       1248.0      1248       1248          0.0  listen                \n",
      "      0.0             1152          1       1152.0       1152.0      1152       1152          0.0  putc                  \n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)  StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  --------  -----------  ----------------------\n",
      "     44.2         33113824          9   3679313.8      6752.0      5952  33044864   11012082.5  cudaEventSynchronize  \n",
      "     30.4         22763680          1  22763680.0  22763680.0  22763680  22763680          0.0  cudaHostAlloc         \n",
      "     14.3         10704352          1  10704352.0  10704352.0  10704352  10704352          0.0  cudaFreeHost          \n",
      "      6.6          4942912          2   2471456.0   2471456.0   1780768   3162144     976780.3  cudaStreamSynchronize \n",
      "      1.6          1230976          1   1230976.0   1230976.0   1230976   1230976          0.0  cudaMalloc            \n",
      "      1.3           966592          1    966592.0    966592.0    966592    966592          0.0  cudaFree              \n",
      "      1.0           776608          1    776608.0    776608.0    776608    776608          0.0  cudaLaunchKernel      \n",
      "      0.2           143200          2     71600.0     71600.0     15200    128000      79761.6  cudaMemcpyAsync       \n",
      "      0.2           139968         18      7776.0      4032.0      1600     45856      10634.2  cudaEventRecord       \n",
      "      0.1            47264          1     47264.0     47264.0     47264     47264          0.0  cudaStreamCreate      \n",
      "      0.0            15904          1     15904.0     15904.0     15904     15904          0.0  cudaStreamDestroy     \n",
      "      0.0            12096          6      2016.0       416.0       224     10208       4014.2  cudaEventCreate       \n",
      "      0.0             3136          6       522.7       272.0       192      1472        496.0  cudaEventDestroy      \n",
      "      0.0             2240          1      2240.0      2240.0      2240      2240          0.0  cuModuleGetLoadingMode\n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances   Avg (ns)    Med (ns)   Min (ns)  Max (ns)  StdDev (ns)                             Name                           \n",
      " --------  ---------------  ---------  ----------  ----------  --------  --------  -----------  ----------------------------------------------------------\n",
      "    100.0         33047712          1  33047712.0  33047712.0  33047712  33047712          0.0  decrypt_gpu(unsigned long *, unsigned long, unsigned long)\n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)   Med (ns)   Min (ns)  Max (ns)  StdDev (ns)           Operation          \n",
      " --------  ---------------  -----  ---------  ---------  --------  --------  -----------  ----------------------------\n",
      "     64.0          3153408      1  3153408.0  3153408.0   3153408   3153408          0.0  [CUDA memcpy Device-to-Host]\n",
      "     36.0          1773920      1  1773920.0  1773920.0   1773920   1773920          0.0  [CUDA memcpy Host-to-Device]\n",
      "\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)           Operation          \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ----------------------------\n",
      "    536.871      1   536.871   536.871   536.871   536.871        0.000  [CUDA memcpy Device-to-Host]\n",
      "    536.871      1   536.871   536.871   536.871   536.871        0.000  [CUDA memcpy Host-to-Device]\n",
      "\n",
      "Generated:\n",
      "    /gpfs/home/scortinhal/CHPS0904/task/07_Memcpy_in_Streams/kernel_baseline_cipher/baseline-report.nsys-rep\n",
      "    /gpfs/home/scortinhal/CHPS0904/task/07_Memcpy_in_Streams/kernel_baseline_cipher/baseline-report.sqlite\n",
      "TIMING: 25.033 ms (allocate memory)\n",
      "TIMING: 14770.3 ms (encrypt data on CPU)\n",
      "TIMING: 2.02992 ms (copy data from CPU to GPU)\n",
      "TIMING: 33.626 ms (decrypt data on GPU)\n",
      "TIMING: 3.1841 ms (copy data from GPU to CPU)\n",
      "TIMING: 38.9294 ms (total time on GPU)\n",
      "STATUS: test passed\n",
      "TIMING: 7.55194 ms (checking result on CPU)\n",
      "TIMING: 7.29062 ms (free memory)\n",
      "TIMING: 14849.8 ms (Temps total)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Affiche le répertoire courant pour vérifier le point de départ\n",
    "pwd\n",
    "\n",
    "cd kernel_baseline_cipher\n",
    "\n",
    "module load cuda/12.6\n",
    "\n",
    "make profile\n",
    "\n",
    "./baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![memcpy_in_stream](images/memcpy_in_stream.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#65AE11;\">Check for Understanding</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please answer the following to confirm you've learned the main objectives of this section. You can display the answers for each question by clicking on the \"...\" cells below the questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Knowing what you do about default stream behavior, explain why in the exercise above (and as shown in the screenshot above) we did not see any overlap between memory transfers and kernel execution, or, between host-to-device and device-to-host memory transfers, even though memory transfers were performed in non-default streams.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "Commands issued into the default stream will wait until all other non-default stream commands have completed, and, will block any other non-default streams from doing work until it completes.\n",
    "\n",
    "In the above exercise, because we did not specify a non-default stream, the kernel launch `decrypt_gpu` occured in the default stream. Therefore, it waited for the host-to-device memory transfers in the non-default stream to complete before beginning, and then, blocked the device-to-host memory transfers from beginning until it completed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Memory allocated on the host with `malloc` can be transfered asynchronously with `cudaMemcpyAsync`?**\n",
    "\n",
    "1. True\n",
    "2. False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer: 2.**\n",
    "\n",
    "Host OS memory paging, cannot guarantee the immediate location of any memory in RAM, but rather, might use paging so that the memory can be stored outside of RAM.\n",
    "\n",
    "In order to transfer memory to or from the host asynchronously in a non-default stream, memory must be page-locked or pinned. To do this, we use `cudaMallocHost` and not `malloc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Host code that uses data being transfered in a non-default stream will wait for memory transfers to complete before beginning work.**\n",
    "\n",
    "1. True\n",
    "2. False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer: 2.**\n",
    "\n",
    "`cudaStreamSynchronize` must be used to block host code from proceeding until work in a given stream is complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#65AE11;\">Next</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While you have learned how to perform kernel launches and memory transfers in non-default streams, the last 2 sections have not actually resulted in performance gains for the cipher application.\n",
    "\n",
    "In the next sections, you will learn how to perform copy/compute overlap, and will begin to see actual performance gains from using concurrent streams.\n",
    "\n",
    "Please continue to the next section: [*Copy Compute Considerations*](../08_Copy_Compute_Considerations/Copy_Compute_Considerations.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#65AE11;\">Optional Further Study</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are for students with time and interest to do additional study on topics related to this workshop.\n",
    "\n",
    "* The `async` suffix for some memcpy operations \"is a misnomer as each function may exhibit synchronous or asynchronous behavior depending on the arguments passed to the function.\" See [the CUDA Runtime Docs](https://docs.nvidia.com/cuda/cuda-runtime-api/api-sync-behavior.html#api-sync-behavior) for more details.\n",
    "* For those of you working with applications utilizing unified memory, see the answer to [this Stack Overflow answer](https://stackoverflow.com/questions/23518299/unified-memory-and-streams-in-c) (including its references to [the docs](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-coherency-hd) for details."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
