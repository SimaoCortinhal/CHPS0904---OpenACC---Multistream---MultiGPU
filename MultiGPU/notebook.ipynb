{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aed246f7-aafd-4528-8b3a-b6e10fb70307",
   "metadata": {},
   "source": [
    "# **Multi gpu - CHPS0904 - Multi GPU Programming Models for HPC and AI**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635c214e-6069-4c50-b8bf-24d454430b37",
   "metadata": {},
   "source": [
    "# Partie 1 : Version CPU \n",
    "\n",
    "## Fonctionnement global\n",
    "\n",
    "- **Initialisation** : la grille 2D est initialisée avec des conditions aux limites (bords gauche et droit à 1, le reste à 0).\n",
    "- **Itérations de Jacobi** : à chaque itération, chaque point intérieur de la grille est mis à jour en prenant la moyenne de ses 4 voisins.\n",
    "- **Application des conditions périodiques** : les lignes du haut et du bas sont reliées (périodicité en Y).\n",
    "- **Critère d’arrêt** : l’algorithme s’arrête si l’erreur maximale entre deux itérations consécutives est inférieure à la tolérance demandée, ou si le nombre maximal d’itérations est atteint.\n",
    "- **Affichage et analyse** : le programme affiche l’état final de la grille, les valeurs aux coins, au centre, et le temps de calcul.\n",
    "\n",
    "---\n",
    "\n",
    "## Explications code\n",
    "\n",
    "### 1. Initialisation de la grille\n",
    "\n",
    "```c\n",
    "void initialize(double *a, int nx, int ny, double left, double right) {\n",
    "    for (int iy = 0; iy < ny; iy++) {\n",
    "        int base = iy * nx;\n",
    "        a[base] = left; // bord gauche\n",
    "        a[base + nx - 1] = right; // bord droit\n",
    "        for (int ix = 1; ix < nx - 1; ix++) {\n",
    "            a[base + ix] = 0.0;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "Cette fonction initialise chaque ligne de la grille :\n",
    "- La première et la dernière colonne (bords gauche et droit) sont mises à la valeur choisie (ici 1.0).\n",
    "- Toutes les autres cases (l’intérieur de la grille) sont initialisées à 0.0.\n",
    "- Cette étape est appelée pour les deux tableaux `a` et `a_new` avant d’entrer dans la boucle principale.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Étape de Jacobi : mise à jour des points intérieurs\n",
    "\n",
    "```c\n",
    "void jacobi_step(double *a_new, double *a, int nx, int ny) {\n",
    "    for (int iy = 1; iy < ny - 1; iy++) {\n",
    "        int row = iy * nx;\n",
    "        int row_up = (iy - 1) * nx;\n",
    "        int row_down = (iy + 1) * nx;\n",
    "        for (int ix = 1; ix < nx - 1; ix++) {\n",
    "            a_new[row + ix] = 0.25 * (\n",
    "                a[row_up + ix] +     // voisin du haut\n",
    "                a[row_down + ix] +   // voisin du bas\n",
    "                a[row + (ix - 1)] +  // voisin de gauche\n",
    "                a[row + (ix + 1)]    // voisin de droite\n",
    "            );\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "- Cette fonction met à jour chaque point intérieur de la grille (on ne touche pas aux bords).\n",
    "- Pour chaque point, on prend la moyenne de ses quatre voisins immédiats (haut, bas, gauche, droite) de l’ancienne itération et on stocke le résultat dans `a_new`.\n",
    "- L’utilisation de variables `row`, `row_up`, `row_down` permet de rendre le code plus lisible et efficace.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Application des conditions périodiques en Y\n",
    "\n",
    "```c\n",
    "void apply_periodic_bc(double *a, int nx, int ny) {\n",
    "    int base_last = (ny - 2) * nx;\n",
    "    int base_first = 1 * nx;\n",
    "    // Ligne du haut\n",
    "    for (int ix = 0; ix < nx; ix++) {\n",
    "        a[ix] = a[base_last + ix];\n",
    "    }\n",
    "    // Ligne du bas\n",
    "    int base_bottom = (ny - 1) * nx;\n",
    "    for (int ix = 0; ix < nx; ix++) {\n",
    "        a[base_bottom + ix] = a[base_first + ix];\n",
    "    }\n",
    "}\n",
    "```\n",
    "- Cette fonction copie la première ligne intérieure sur la ligne du bas et la dernière ligne intérieure sur la ligne du haut, imposant ainsi une périodicité en Y.\n",
    "- Cela signifie que le haut et le bas de la grille sont connectés.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Boucle principale et calcul de l’erreur\n",
    "\n",
    "Dans la fonction `main`, la boucle principale s’occupe de :\n",
    "- Lancer une étape de Jacobi,\n",
    "- Appliquer les conditions périodiques,\n",
    "- Calculer l’erreur maximale sur tous les points intérieurs :\n",
    "\n",
    "```c\n",
    "error = 0.0;\n",
    "for (iy = 1; iy < ny - 1; iy++) {\n",
    "    for (ix = 1; ix < nx - 1; ix++) {\n",
    "        int tot = iy*nx;\n",
    "        double diff = fabs(a_new[tot + ix] - a[tot + ix]);\n",
    "        if (diff > error) error = diff;\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "- Échanger les pointeurs des tableaux (pour ne pas recopier les valeurs) :\n",
    "\n",
    "```c\n",
    "double *tmp = a;\n",
    "a = a_new;\n",
    "a_new = tmp;\n",
    "```\n",
    "\n",
    "- Incrémenter le compteur d’itération.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Critère d’arrêt et affichage\n",
    "\n",
    "- La boucle s’arrête si `error <= tol` ou si `iter >= max_iter`.\n",
    "- À la fin, on affiche :\n",
    "    - Le nombre d’itérations,\n",
    "    - L’erreur finale et l’état de la convergence,\n",
    "    - Le temps total écoulé,\n",
    "    - Les valeurs aux coins et au centre de la grille.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb254c3f-4144-4ebe-abd0-53571fc4649a",
   "metadata": {},
   "source": [
    "## Compilation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8d73b78a-a6a4-4a7f-a0d6-72b6c0c3f833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f jacobi_seq\n",
      "gcc -O3 -Wall -march=native -funroll-loops -ffast-math -std=c99 -fopenmp -o jacobi_seq jacobi.c laplace2d.c -lm\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd CPU\n",
    "make clean \n",
    "make\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd62542-88ff-412e-a777-1a3b3eb61d7f",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7411ab7c-f1d9-451f-a5ec-24e3b69d2b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacobi solver stopped after 1000 iterations\n",
      "Reached max_iter = 1000 (error = 2.419263e-04 > tol = 1.000000e-06)\n",
      "Elapsed time   : 15.464313 seconds\n",
      "Values at corners: [0,0]=1.00  [0,4095]=1.00\n",
      "Value at center [2048,2048] = 0.000000\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "./CPU/jacobi_seq 4096 4096 1000 1e-6 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412d80f5-0b3b-4ad8-8786-3e9275540c79",
   "metadata": {},
   "source": [
    "# Partie 2 : Version CPU + GPU "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672acc20-64eb-4817-beb2-695c4d586616",
   "metadata": {},
   "source": [
    "\n",
    "## Fonctionnement global\n",
    "\n",
    "- **Initialisation** : la grille 2D est initialisée côté host (CPU) puis copiée sur le device (GPU).\n",
    "- **Itérations de Jacobi** : à chaque itération, un kernel CUDA met à jour tous les points intérieurs de la grille sur le GPU.\n",
    "- **Application des conditions périodiques** : un kernel CUDA copie les bonnes lignes pour gérer la périodicité en Y.\n",
    "- **Critère d’arrêt** : on utilise Thrust pour calculer, sur le GPU, l’erreur maximale entre deux itérations consécutives, sans transfert sur le CPU.\n",
    "- **Affichage et analyse** : le programme copie le résultat sur le CPU pour affichage et donne le temps de calcul.\n",
    "\n",
    "---\n",
    "\n",
    "## Explications code\n",
    "\n",
    "### 1. Initialisation sur le CPU et transfert sur le GPU\n",
    "\n",
    "```c\n",
    "void initialize_host(double* a, int nx, int ny, double left, double right) {\n",
    "    for (int iy = 0; iy < ny; iy++) {\n",
    "        int base = iy * nx;\n",
    "        a[base] = left;\n",
    "        a[base + nx - 1] = right;\n",
    "        for (int ix = 1; ix < nx-1; ix++) {\n",
    "            a[base + ix] = 0.0;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "- Cette fonction initialise la grille sur le CPU : bords gauche/droit à 1, reste à 0.\n",
    "- Ensuite, on copie ces données sur la mémoire GPU avec `cudaMemcpy`.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Étape de Jacobi : kernel CUDA pour la mise à jour\n",
    "\n",
    "```c\n",
    "__global__ void jacobi_step(double* a_new, const double* a, int nx, int ny) {\n",
    "    int ix = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int iy = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    if (ix > 0 && ix < nx-1 && iy > 0 && iy < ny-1) {\n",
    "        int idx = iy * nx + ix;\n",
    "        a_new[idx] = 0.25 * (\n",
    "            a[idx - nx] +   // voisin du haut\n",
    "            a[idx + nx] +   // voisin du bas\n",
    "            a[idx - 1 ] +   // voisin de gauche\n",
    "            a[idx + 1 ]     // voisin de droite\n",
    "        );\n",
    "    }\n",
    "}\n",
    "```\n",
    "- Ce kernel exécute la mise à jour Jacobi sur **tous les points intérieurs** de la grille en parallèle sur le GPU.\n",
    "- Chaque thread traite un point `(ix, iy)`.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Application des conditions périodiques en Y sur GPU\n",
    "\n",
    "```c\n",
    "__global__ void apply_periodic_bc(double* a, int nx, int ny) {\n",
    "    int ix = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (ix < nx) {\n",
    "        int top    = 0 * nx + ix;\n",
    "        int bottom = (ny-1) * nx + ix;\n",
    "        int first  = 1 * nx + ix;\n",
    "        int last   = (ny-2) * nx + ix;\n",
    "        a[top]    = a[last];\n",
    "        a[bottom] = a[first];\n",
    "    }\n",
    "}\n",
    "```\n",
    "- Ce kernel copie la première ligne intérieure sur la ligne du bas, et la dernière ligne intérieure sur la ligne du haut pour respecter la périodicité.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Calcul de l’erreur de convergence avec Thrust\n",
    "\n",
    "```c\n",
    "struct max_abs_diff {\n",
    "    __host__ __device__\n",
    "    double operator()(const thrust::tuple<double,double>& t) const {\n",
    "        return fabs(thrust::get<0>(t) - thrust::get<1>(t));\n",
    "    }\n",
    "};\n",
    "...\n",
    "thrust::device_ptr<double> ptr_new(d_a_new);\n",
    "thrust::device_ptr<double> ptr_old(d_a);\n",
    "error = thrust::transform_reduce(\n",
    "    thrust::make_zip_iterator(thrust::make_tuple(ptr_new, ptr_old)),\n",
    "    thrust::make_zip_iterator(thrust::make_tuple(ptr_new + N, ptr_old + N)),\n",
    "    max_abs_diff(),\n",
    "    0.0,\n",
    "    thrust::maximum<double>());\n",
    "```\n",
    "Thrust est une bibliothèque C++ intégrée à CUDA qui fournit des primitives algorithmiques (comme les réductions, tris, scans) optimisées pour le GPU. Ici, j'utilise `thrust::transform_reduce` pour parcourir les tableaux sur le GPU et calculer en parallèle l’erreur maximale entre deux itérations, sans repasser les données sur le CPU.\n",
    "- transform : applique le foncteur max_abs_diff à chaque paire (a_new, a_old) → calcule la différence absolue pour chaque point.\n",
    "- reduce : trouve le maximum de toutes ces différences (critère de convergence Jacobi)Tout ça SANS aller-retour entre GPU et CPU !\n",
    "      \n",
    "- Le **foncteur** `max_abs_diff` calcule la différence absolue entre chaque case des deux grilles (état courant et précédent).\n",
    "- `thrust::transform_reduce` applique ce foncteur à tous les éléments du tableau, puis prend le maximum : on obtient ainsi l’erreur max **directement sur le GPU**, sans rapatrier les données.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Organisation de la boucle principale\n",
    "\n",
    "Dans `main`, la boucle effectue :\n",
    "- Une étape Jacobi sur le GPU (`jacobi_step`)\n",
    "- L’application des conditions périodiques (`apply_periodic_bc`)\n",
    "- Le calcul de l’erreur de convergence avec Thrust\n",
    "- L’échange des pointeurs device (pour éviter des copies de tableaux)\n",
    "- L’incrémentation du compteur d’itérations\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Gestion des temps\n",
    "\n",
    "- on utilise `cudaEvent` pour mesurer le temps de calcul CUDA uniquement, et `clock()` pour mesurer le temps global (CPU + GPU, initialisation comprise).\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Récupération et affichage des résultats\n",
    "\n",
    "- À la fin, le résultat final est copié du GPU vers le CPU pour affichage.\n",
    "- Affichage du nombre d’itérations, erreur finale, temps CUDA, et temps total du programme.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b4be28-3f6d-49e0-ba53-34bc0deec2a0",
   "metadata": {},
   "source": [
    "## Compilation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6434f52-319d-4e3d-a257-5dfb4358e844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc -O3 -arch=sm_60 -std=c++11        -o jacobi_cuda jacobi.cu -lm                             \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd CPU-GPU\n",
    "make"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f571c66-da19-4ecb-b4be-f79f9775e3ab",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5248b517-2afd-45e4-ab7c-bd146d5c7a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solveur Jacobi CUDA convergé en 1000 itérations (erreur = 2.419e-04)\n",
      "Temps passé (calcul CUDA) : 0.239045 s\n",
      "Temps total du programme (tout inclus) : 2.320342 s\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "./CPU-GPU/jacobi_cuda 4096 4096 1000 1e-6 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4559e5f-3c1c-4c2b-bfc7-09eae2c6aedb",
   "metadata": {},
   "source": [
    "# Partie 3 : MPI + GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af60e48c-8e5a-420d-8ee3-8d8d6102ba7c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 1. Introduction de MPI\n",
    "\n",
    "- Ajout de `#include <mpi.h>`.\n",
    "- Initialisation de MPI en début de programme (`MPI_Init`) et terminaison en fin (`MPI_Finalize`).\n",
    "- Chaque processus MPI correspond à un sous-domaine de la grille et, typiquement, à un GPU distinct (sélectionné automatiquement avec `cudaSetDevice(rank % num_gpus)`).\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Découpage de la grille\n",
    "\n",
    "- La grille globale est **découpée horizontalement** en sous-domaines : chaque processus MPI gère un bloc de lignes consécutives (appelées localement `local_ny`), plus deux lignes \"halo\" (fantômes) pour les échanges avec les voisins.\n",
    "- Calcul de la répartition : gestion du cas où le nombre de lignes n’est pas un multiple exact du nombre de processus.\n",
    "- Pour chaque processus :\n",
    "    - `local_ny` : nombre de lignes propres à traiter\n",
    "    - `ghost_ny = local_ny + 2` : nombre total de lignes allouées localement (incluant halos haut et bas)\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Initialisation et mémoire\n",
    "\n",
    "- Chaque processus alloue **sa portion locale** sur le GPU (`cudaMalloc`).\n",
    "- Initialisation des bords effectuée sur cette portion locale (kernel `init_boundaries`).\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Échanges des halos (communication MPI)\n",
    "\n",
    "- **À chaque itération, chaque processus échange ses lignes de halo** avec ses voisins direct (haut et bas) via `MPI_Sendrecv` :\n",
    "    - Envoie sa première ligne intérieure à son voisin du haut et reçoit le halo du haut.\n",
    "    - Envoie sa dernière ligne intérieure à son voisin du bas et reçoit le halo du bas.\n",
    "- Ces échanges se font directement entre les buffers device (`d_a + IDX(...)`)\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Adaptation des kernels\n",
    "\n",
    "- Les kernels CUDA (`jacobi_kernel` et `init_boundaries`) opèrent sur la portion locale avec gestion des halos.\n",
    "- Les indices sont adaptés pour ne jamais toucher les bords globaux (gérés séparément si besoin).\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Réduction de l’erreur et convergence globale\n",
    "\n",
    "- Chaque processus calcule **l’erreur maximale locale** via Thrust.\n",
    "- La convergence globale est obtenue en combinant toutes les erreurs locales par une réduction MPI :\n",
    "    - `MPI_Allreduce` avec l’opération `MPI_MAX` pour obtenir l’erreur maximale sur tous les processus.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Synchronisation et chronométrage\n",
    "\n",
    "- Synchronisation avec `MPI_Barrier` avant et après la boucle principale pour bien mesurer le temps de calcul pur.\n",
    "- Deux chronos affichés : temps scientifique (calcul seul) et temps total du programme.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Rassemblement et affichage des résultats\n",
    "\n",
    "- À la fin, chaque processus envoie son sous-domaine au processus 0 via `MPI_Send` / `MPI_Recv` pour permettre un affichage complet.\n",
    "- Reconstruction de la grille globale sur le processus 0, avec gestion de la périodicité sur les bords pour l’affichage.\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Libération des ressources\n",
    "\n",
    "- Libération de la mémoire GPU pour chaque processus.\n",
    "- Nettoyage MPI via `MPI_Finalize`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28376d3-241c-43b1-ac81-64c993bb6281",
   "metadata": {},
   "source": [
    "## Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34e00f77-d58b-493d-8904-fe87c7ccec7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir -p nsight\n",
      "# Adapte la ligne ci-dessous selon les arguments à donner à ton programme\n",
      "nsys profile -o nsight/jacobi_cuda_profile \\\n",
      "\tmpirun -np 4 ./jacobi_cuda 4096 4096 10000 1e-6 0\n",
      "Solveur Jacobi MPI+multiGPU convergé en 10000 itérations (erreur = 2.420e-05)\n",
      "Temps calcul MPI+multiGPU (hors init): 10.474947 s\n",
      "Temps total du programme (allocs, MPI, init, calcul, etc.): 12.065351 s\n",
      "Collecting data...\n",
      "Generating '/tmp/nsys-report-35d9.qdstrm'\n",
      "[1/1] [========================100%] jacobi_cuda_profile.nsys-rep\n",
      "Generated:\n",
      "    /gpfs/home/scortinhal/CHPS0904/MultiGPU/MPI-GPU/nsight/jacobi_cuda_profile.nsys-rep\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd MPI-GPU\n",
    "make nsight\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "857c76fc-5659-4962-a0fa-1b8f4c3e17a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solveur Jacobi MPI+multiGPU convergé en 1000 itérations (erreur = 2.422e-04)\n",
      "Temps calcul MPI+multiGPU (hors init): 1.188106 s\n",
      "Temps total du programme (allocs, MPI, init, calcul, etc.): 1.944380 s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd MPI-GPU\n",
    "\n",
    "mpirun -np 4 ./jacobi_cuda 4096 4096 1000 1e-6 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4280d903-02c4-4cbf-93e5-0bf575c172bc",
   "metadata": {},
   "source": [
    "![MPI-GPU](MPI-GPU/nsight/MPI-GPU.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3760f8-4fa9-4239-9a0e-db4ce6f389c1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93b5e5d9-e106-444f-9e9d-b792fd9d445c",
   "metadata": {},
   "source": [
    "# Partie 4 : MPI + Overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772c3301-4f6c-4226-bf97-0727543532bc",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 1. Ajout de CUDA streams multiples\n",
    "\n",
    "Pour permettre d’exécuter en parallèle transferts mémoire et calculs sur différentes parties de la grille, plusieurs streams CUDA sont déclarés et créés :\n",
    "```c\n",
    "cudaStream_t stream_halo_top, stream_halo_bot, stream_interior, stream_memcpy;\n",
    "cudaStreamCreate(&stream_halo_top);\n",
    "cudaStreamCreate(&stream_halo_bot);\n",
    "cudaStreamCreate(&stream_interior);\n",
    "cudaStreamCreate(&stream_memcpy);\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Buffers de halos côté host\n",
    "\n",
    "Des tampons explicites pour l’envoi/réception des halos sont alloués :\n",
    "```c\n",
    "double *h_halo_top_send = (double*)malloc(nx * sizeof(double));\n",
    "double *h_halo_top_recv = (double*)malloc(nx * sizeof(double));\n",
    "double *h_halo_bot_send = (double*)malloc(nx * sizeof(double));\n",
    "double *h_halo_bot_recv = (double*)malloc(nx * sizeof(double));\n",
    "```\n",
    "Ces buffers servent pour les transferts mémoire asynchrones et les échanges MPI non bloquants.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Boucle principale : pipeline overlap calcul/transferts/comm\n",
    "\n",
    "### a. Copies device->host asynchrones des halos\n",
    "```c\n",
    "cudaMemcpyAsync(h_halo_top_send, d_a + IDX(1,0,nx), nx*sizeof(double), cudaMemcpyDeviceToHost, stream_memcpy);\n",
    "cudaMemcpyAsync(h_halo_bot_send, d_a + IDX(local_ny,0,nx), nx*sizeof(double), cudaMemcpyDeviceToHost, stream_memcpy);\n",
    "cudaStreamSynchronize(stream_memcpy);\n",
    "```\n",
    "On lance la copie sur un stream dédié et on synchronise juste ce stream, sans bloquer le calcul sur le reste de la grille.\n",
    "\n",
    "### b. Communications MPI non bloquantes pour les halos\n",
    "```c\n",
    "MPI_Request reqs[4];\n",
    "MPI_Irecv(h_halo_top_recv, nx, MPI_DOUBLE, prev, 1, MPI_COMM_WORLD, &reqs[0]);\n",
    "MPI_Irecv(h_halo_bot_recv, nx, MPI_DOUBLE, next, 0, MPI_COMM_WORLD, &reqs[1]);\n",
    "MPI_Isend(h_halo_top_send, nx, MPI_DOUBLE, prev, 0, MPI_COMM_WORLD, &reqs[2]);\n",
    "MPI_Isend(h_halo_bot_send, nx, MPI_DOUBLE, next, 1, MPI_COMM_WORLD, &reqs[3]);\n",
    "```\n",
    "Cela permet de continuer les calculs sans attendre la fin des communications.\n",
    "\n",
    "### c. Calcul du cœur de la grille en overlap pendant le MPI\n",
    "```c\n",
    "int iy_start_interior = 2;\n",
    "int iy_end_interior = local_ny;\n",
    "if (iy_end_interior > iy_start_interior) {\n",
    "    dim3 grid_interior((nx+BLOCK-1)/BLOCK, (iy_end_interior-iy_start_interior+BLOCK-1)/BLOCK);\n",
    "    jacobi_kernel_slice<<<grid_interior, block, 0, stream_interior>>>(d_a, d_a_new, nx, iy_start_interior, iy_end_interior);\n",
    "}\n",
    "```\n",
    "On lance immédiatement le calcul Jacobi sur la partie intérieur (lignes intérieures sans dépendance aux halos).\n",
    "\n",
    "### d. Synchronisation minimale, gestion des halos reçus\n",
    "```c\n",
    "MPI_Wait(&reqs[0], MPI_STATUS_IGNORE); // halo du haut\n",
    "MPI_Wait(&reqs[1], MPI_STATUS_IGNORE); // halo du bas\n",
    "\n",
    "cudaMemcpyAsync(d_a + IDX(0,0,nx), h_halo_top_recv, nx*sizeof(double), cudaMemcpyHostToDevice, stream_memcpy);\n",
    "cudaMemcpyAsync(d_a + IDX(ghost_ny-1,0,nx), h_halo_bot_recv, nx*sizeof(double), cudaMemcpyHostToDevice, stream_memcpy);\n",
    "cudaStreamSynchronize(stream_memcpy);\n",
    "```\n",
    "On ne synchronise que ce qui est nécessaire, afin de lancer le calcul sur les bandes extrêmes dès que possible.\n",
    "\n",
    "### e. Calcul des bandes extrêmes (haut et bas)\n",
    "```c\n",
    "dim3 grid_band((nx+BLOCK-1)/BLOCK, 1);\n",
    "jacobi_kernel_slice<<<grid_band, block, 0, stream_halo_top>>>(d_a, d_a_new, nx, 1, 2);\n",
    "jacobi_kernel_slice<<<grid_band, block, 0, stream_halo_bot>>>(d_a, d_a_new, nx, local_ny, local_ny+1);\n",
    "```\n",
    "On traite les lignes qui dépendent des halos dans leurs propres streams, après leur arrivée.\n",
    "\n",
    "### f. Attentes sélectives des streams CUDA\n",
    "```c\n",
    "cudaStreamSynchronize(stream_interior);\n",
    "cudaStreamSynchronize(stream_halo_top);\n",
    "cudaStreamSynchronize(stream_halo_bot);\n",
    "```\n",
    "On attend la fin des calculs avant la réduction.\n",
    "\n",
    "### g. Attente de la fin des communications MPI\n",
    "```c\n",
    "MPI_Waitall(4, reqs, MPI_STATUSES_IGNORE);\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Nettoyage spécifique\n",
    "\n",
    "À la fin du programme, il faut maintenant libérer explicitement les nouveaux objets alloués :\n",
    "```c\n",
    "cudaStreamDestroy(stream_halo_top);\n",
    "cudaStreamDestroy(stream_halo_bot);\n",
    "cudaStreamDestroy(stream_interior);\n",
    "cudaStreamDestroy(stream_memcpy);\n",
    "\n",
    "free(h_halo_top_send); free(h_halo_top_recv);\n",
    "free(h_halo_bot_send); free(h_halo_bot_recv);\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Résumé des apports principaux\n",
    "\n",
    "- **Overlap calcul/communication** : Superposition des calculs et des échanges MPI grâce aux copies asynchrones, aux streams CUDA multiples et aux MPI non bloquants.\n",
    "- **Découpage fin** : Séparation explicite des calculs sur l’intérieur et sur les bandes extrêmes de la grille.\n",
    "- **Gestion des dépendances** : Synchronisations minimales et localisées pour maximiser l’occupation GPU.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Remarque sur les kernels\n",
    "\n",
    "Un nouveau kernel `jacobi_kernel_slice` est introduit :\n",
    "```c\n",
    "__global__\n",
    "void jacobi_kernel_slice(const double *a, double *a_new, int nx, int iy_start, int iy_end) { ... }\n",
    "```\n",
    "Il permet de ne traiter qu’une **bande verticale** de la grille, paramétrable par indices, indispensable pour l’overlap.\n",
    "---\n",
    "\n",
    "## Compilation et execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c9438358-adbd-4f00-a3d1-bffbaeb305d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make: Nothing to be done for 'all'.\n",
      "Solveur Jacobi MPI+multiGPU (full overlap memcpyasync) convergé en 1000 itérations (error = 2.422e-04)\n",
      "Temps calcul MPI+multiGPU (hors alloc/init): 0.937806 s\n",
      "Temps total du programme (tout inclus): 1.822555 s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd MPI-GPU-overlap\n",
    "make\n",
    "mpirun -np 4 ./jacobi_cuda 4096 4096 1000 1e-6 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ecc13c-f926-4dbe-9b3e-b79a668fc751",
   "metadata": {},
   "source": [
    "# Partie 5 : NCCL\n",
    "\n",
    "## NCCL ?\n",
    "\n",
    "- **NCCL** (NVIDIA Collective Communications Library) permet de faire des transferts **directement GPU-to-GPU** (device-to-device) même sur plusieurs machines équipées de NVLink ou d’Infiniband.\n",
    "- Cela évite les copies host/device et la synchronisation CPU, ce qui réduit considérablement la latence d’échange entre GPU et augmente l’overlap.\n",
    "- **NCCL est asynchrone** et s’intègre naturellement avec les streams CUDA, donc le calcul, les transferts et la communication peuvent être superposés sans blocage.\n",
    "- Il est utilisé ici pour échanger les halos (lignes fantômes) entre sous-domaines de la grille Jacobi, mais peut également servir à la réduction globale (AllReduce, etc.).\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Initialisation de NCCL dans MPI\n",
    "\n",
    "Après avoir choisi le GPU local :\n",
    "```c\n",
    "int num_gpus = 0;\n",
    "cudaGetDeviceCount(&num_gpus);\n",
    "cudaSetDevice(rank % num_gpus);\n",
    "```\n",
    "**NCCL doit être initialisé et chaque rang reçoit le même identifiant :**\n",
    "```c\n",
    "ncclUniqueId id;\n",
    "if (rank == 0) ncclGetUniqueId(&id);\n",
    "MPI_Bcast(&id, sizeof(id), MPI_BYTE, 0, MPI_COMM_WORLD);\n",
    "ncclComm_t nccl_comm;\n",
    "ncclCommInitRank(&nccl_comm, size, id, rank);\n",
    "```\n",
    "- `ncclGetUniqueId(&id);`  \n",
    "  (rendu par le rank 0) crée un identifiant de communicateur unique pour NCCL.\n",
    "- `MPI_Bcast(...)`  \n",
    "  Le communique à tous les processus via MPI.\n",
    "- `ncclCommInitRank(...)`  \n",
    "  Crée un contexte NCCL sur chaque rang avec ce communicateur, le nombre total de rangs, et le rang courant.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Utilisation de NCCL pour l’échange des halos\n",
    "\n",
    "Au lieu des traditionnels `MPI_Sendrecv`, on utilise :\n",
    "```c\n",
    "cudaStream_t stream;\n",
    "cudaStreamCreate(&stream);\n",
    "\n",
    "while (error > tol && iter < max_iter) {\n",
    "    // Echange des halos avec NCCL (overlap possible)\n",
    "    ncclGroupStart();\n",
    "    ncclRecv(d_a + IDX(0,0,nx), nx, ncclDouble, prev, nccl_comm, stream);\n",
    "    ncclSend(d_a + IDX(1,0,nx), nx, ncclDouble, prev, nccl_comm, stream);\n",
    "    ncclRecv(d_a + IDX(ghost_ny-1,0,nx), nx, ncclDouble, next, nccl_comm, stream);\n",
    "    ncclSend(d_a + IDX(local_ny,0,nx), nx, ncclDouble, next, nccl_comm, stream);\n",
    "    ncclGroupEnd();\n",
    "    cudaStreamSynchronize(stream);\n",
    "\n",
    "    // ...\n",
    "}\n",
    "```\n",
    "**Explication de chaque commande NCCL :**\n",
    "- `ncclGroupStart(); ... ncclGroupEnd();`  \n",
    "  Grouper plusieurs opérations NCCL permet d’optimiser la communication et de lancer tous les échanges de façon atomique.\n",
    "  ncclGroupStart() indique le début d'un groupe d'opérations de communication NCCL.\n",
    "  ncclGroupEnd() marque la fin du groupe.\n",
    "- `ncclRecv(...)`  \n",
    "  Reçoit une ligne halo depuis le GPU voisin, directement en mémoire device, sur le stream CUDA donné.\n",
    "- `ncclSend(...)`  \n",
    "  Envoie la ligne halo correspondante à ce voisin, toujours device-to-device.\n",
    "- Tous les échanges sont lancés en overlap sur le même stream.\n",
    "- `cudaStreamSynchronize(stream);`  \n",
    "  S’assure que tous les transferts sont finis avant de passer au calcul.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Boucle principale : pipeline calcul/communication\n",
    "\n",
    "- Après les échanges NCCL, on lance le calcul Jacobi comme avant :\n",
    "```c\n",
    "jacobi_kernel<<<grid,block,0,stream>>>(d_a, d_a_new, nx, ghost_ny);\n",
    "cudaStreamSynchronize(stream);\n",
    "```\n",
    "- On fait le swap des pointeurs, puis on utilise Thrust sur le GPU pour l’erreur locale, puis une `MPI_Allreduce` pour la convergence globale.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Nettoyage spécifique\n",
    "\n",
    "En fin de programme, il faut maintenant :\n",
    "```c\n",
    "cudaStreamDestroy(stream);\n",
    "ncclCommDestroy(nccl_comm);\n",
    "```\n",
    "pour libérer le communicateur NCCL et le stream CUDA.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Comparaison avec la version MPI+CUDA classique\n",
    "\n",
    "- **Communication device-to-device** au lieu de device→host→MPI→host→device.\n",
    "- **Overlapping naturel** grâce à l’intégration CUDA streams/NCCL.\n",
    "- **Suppression des tampons host pour les halos** : tout est fait sur le device.\n",
    "- Initialisation NCCL requiert une étape supplémentaire (diffusion d’un identifiant entre ranks).\n",
    "- Tout le reste du code (calcul, Thrust, swap, affichage) reste quasi identique.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddff1e20-d7f2-46eb-ad60-8437da34ecda",
   "metadata": {},
   "source": [
    "## Compilation et execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4388d1e5-0243-428b-832f-cd06a8b61460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make: Nothing to be done for 'all'.\n",
      "Solveur Jacobi NCCL convergé en 1000 itérations (error = 2.422e-04)\n",
      "1. Temps total du programme (MPI_Init → fin)                : 6.308566 s\n",
      "2. Temps après init NCCL (juste avant alloc/init CUDA)      : 1.565614 s\n",
      "3. Temps NCCL (calcul pur boucle Jacobi)                    : 1.562996 s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd NCCL\n",
    "\n",
    "eval $(spack load --sh nccl)\n",
    "export LD_LIBRARY_PATH=$(spack location -i nccl)/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "make\n",
    "mpirun -np 4 ./jacobi_nccl 4096 4096 1000 1e-6 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5bd7b3-2412-4543-af79-8133db1e5c65",
   "metadata": {},
   "source": [
    "# Partie 6 : NCCL - Overlap\n",
    "\n",
    "\n",
    "## 1. Initialisation du communicateur NCCL\n",
    "\n",
    "On initialise NCCL comme dans la version simple :\n",
    "```c\n",
    "ncclUniqueId id;\n",
    "if (rank == 0) ncclGetUniqueId(&id);\n",
    "MPI_Bcast(&id, sizeof(id), MPI_BYTE, 0, MPI_COMM_WORLD);\n",
    "ncclComm_t nccl_comm;\n",
    "ncclCommInitRank(&nccl_comm, size, id, rank);\n",
    "```\n",
    "- Permet à chaque rang de communiquer directement en device-to-device.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **Streams CUDA multiples** pour overlap\n",
    "\n",
    "Au lieu d'un seul stream pour tout, on crée :\n",
    "```c\n",
    "cudaStream_t stream_halo_top, stream_halo_bot, stream_interior;\n",
    "cudaStreamCreate(&stream_halo_top);\n",
    "cudaStreamCreate(&stream_halo_bot);\n",
    "cudaStreamCreate(&stream_interior);\n",
    "```\n",
    "- **Un stream par halo** (haut/bas) pour la communication.\n",
    "- **Un stream pour l'intérieur** pour le calcul bulk.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. **Échange des halos device-to-device avec NCCL, en overlap**\n",
    "\n",
    "On utilise **NCCL group** pour regrouper toutes les comms et on attribue **chaque halo à son stream** :\n",
    "```c\n",
    "ncclGroupStart();\n",
    "ncclRecv(d_a + IDX(0,0,nx), nx, ncclDouble, prev, nccl_comm, stream_halo_top); // halo haut\n",
    "ncclSend(d_a + IDX(1,0,nx), nx, ncclDouble, prev, nccl_comm, stream_halo_top);\n",
    "ncclRecv(d_a + IDX(ghost_ny-1,0,nx), nx, ncclDouble, next, nccl_comm, stream_halo_bot); // halo bas\n",
    "ncclSend(d_a + IDX(local_ny,0,nx), nx, ncclDouble, next, nccl_comm, stream_halo_bot);\n",
    "ncclGroupEnd();\n",
    "```\n",
    "- **`ncclRecv`** : reçoit la ligne fantôme (halo) du voisin (top ou bottom), directement dans le device buffer, sur le stream dédié.\n",
    "- **`ncclSend`** : envoie la ligne fantôme calculée au voisin, device-to-device, sur le même stream.\n",
    "- L’usage d’un stream par direction permet d’overlapper communication et calcul.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. **Calcul en overlap sur plusieurs streams**\n",
    "\n",
    "Après avoir lancé les communications, on peut immédiatement lancer le calcul sur les bandes et l'intérieur :\n",
    "```c\n",
    "// Calcul extrémités (dépendants des halos) chacun sur leur stream\n",
    "dim3 grid_band((nx+BLOCK-1)/BLOCK, 1);\n",
    "jacobi_kernel_slice<<<grid_band, block, 0, stream_halo_top>>>(d_a, d_a_new, nx, 1, 2);\n",
    "jacobi_kernel_slice<<<grid_band, block, 0, stream_halo_bot>>>(d_a, d_a_new, nx, local_ny, local_ny+1);\n",
    "\n",
    "// Calcul bulk (indépendant des halos) sur un troisième stream\n",
    "int iy_start_interior = 2;\n",
    "int iy_end_interior = local_ny;\n",
    "if (iy_end_interior > iy_start_interior) {\n",
    "    dim3 grid_interior((nx+BLOCK-1)/BLOCK, (iy_end_interior-iy_start_interior+BLOCK-1)/BLOCK);\n",
    "    jacobi_kernel_slice<<<grid_interior, block, 0, stream_interior>>>(d_a, d_a_new, nx, iy_start_interior, iy_end_interior);\n",
    "}\n",
    "```\n",
    "- Permet d’exécuter les calculs sur les régions qui n’attendent pas la fin des communications pendant que les halos arrivent.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. **Synchronisation fine**\n",
    "\n",
    "Pour garantir la cohérence, chaque stream est synchronisé individuellement avant la réduction :\n",
    "```c\n",
    "cudaStreamSynchronize(stream_halo_top);\n",
    "cudaStreamSynchronize(stream_halo_bot);\n",
    "cudaStreamSynchronize(stream_interior);\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. **Nettoyage**\n",
    "\n",
    "Libération explicite des nouveaux streams :\n",
    "```c\n",
    "cudaStreamDestroy(stream_halo_top);\n",
    "cudaStreamDestroy(stream_halo_bot);\n",
    "cudaStreamDestroy(stream_interior);\n",
    "ncclCommDestroy(nccl_comm);\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7. **Résumé de l’intérêt**\n",
    "\n",
    "- **Overlap optimal** communication/calcul : réduit le temps mort lié à la latence réseau.\n",
    "- **Tout reste sur le device** : plus aucun transfert inutile device<->host pour les halos.\n",
    "- **Kernels spécialisés** et lancement sur plusieurs streams permettent d’exploiter pleinement le GPU même si la comm est en attente.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. **Points-clés sur NCCL**\n",
    "\n",
    "- `ncclRecv/Send` : transferts directs entre GPUs (device-to-device) sur le stream spécifié.\n",
    "- `ncclGroupStart/End` : regroupe toutes les ops de comms pour maximiser le pipelining.\n",
    "- **Aucune barrière explicite MPI dans la boucle**, seul MPI_Allreduce pour la convergence.\n",
    "- **Tout le découpage reste identique** (partition des lignes entre rangs).\n",
    "\n",
    "---\n",
    "\n",
    "## Compilation et execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5c553686-9caa-4982-bcf9-ac4a1e2182d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make: Nothing to be done for 'all'.\n",
      "NCCL Comm Init: 2.270 s\n",
      "Solveur Jacobi NCCL + overlap convergé en 1000 itérations (error = 2.422e-04)\n",
      "1. Temps total du programme (MPI_Init → fin)                 : 5.631847 s\n",
      "2. Temps après init NCCL (juste avant alloc/init CUDA)       : 0.912701 s\n",
      "3. Temps calcul NCCL + overlap (boucle Jacobi uniquement)    : 0.909242 s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd NCCL-overlap\n",
    "\n",
    "eval $(spack load --sh nccl)\n",
    "export LD_LIBRARY_PATH=$(spack location -i nccl)/lib:$LD_LIBRARY_PATH\n",
    "export NCCL_IB_HCA=mlx5_0         # si tu as Infiniband\n",
    "export NCCL_IB_DISABLE=0\n",
    "export NCCL_P2P_DISABLE=1\n",
    "make\n",
    "mpirun -np 4 ./jacobi_nccl 4096 4096 1000 1e-6 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9acb6f-b796-4105-bc78-c0983cdd32ad",
   "metadata": {},
   "source": [
    "![MPI-GPU](NCCL/nsight/NCCL-nsight.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51849b41-74e4-401a-a2d8-64867697436b",
   "metadata": {},
   "source": [
    "# Partie 7 : NCCL - graph\n",
    "\n",
    "## Graph ? \n",
    "Les CUDA Graphs sont une fonctionnalité de CUDA qui permet de capturer (enregistrer) toute une séquence d’opérations (kernels, transferts mémoire, communications, etc.) dans un graphe orienté acyclique (DAG), puis de la lancer d’un seul coup sur le GPU.\n",
    "Objectifs et avantages principaux\n",
    "\n",
    "- Réduire l’overhead CPU :\n",
    "    Quand tu lances des centaines/milliers de kernels dans une boucle, chaque appel CPU→GPU a un coût (latence, passage de paramètres…).\n",
    "    Avec un graph, tu encapsules toute la séquence : le CPU ne fait qu’un seul appel par itération.\n",
    "\n",
    "- Pipeline matériel :\n",
    "    Le GPU peut analyser le graphe et planifier/exécuter les tâches de façon plus efficace, notamment en pipeline ou en parallélisant ce qui peut l’être.\n",
    "\n",
    "- Optimisation automatique :\n",
    "    CUDA peut mieux optimiser les transferts, le scheduling, et profiter des dépendances entre opérations.\n",
    "\n",
    "- Mutualisation avec communication :\n",
    "    Si tu inclues aussi des transferts NCCL/MPI dans le graph, tu maximises l’overlap calcul/comm sans avoir à tout gérer manuellement côté CPU.\n",
    "\n",
    "## 1. **Gestion de l’affinité GPU / MPI (multi-nœud)**\n",
    "\n",
    "Chaque rang MPI choisit son GPU en se basant sur `local_rank` pour éviter les conflits sur un nœud multi-GPU :\n",
    "\n",
    "```c\n",
    "int local_rank = 0;\n",
    "{\n",
    "    MPI_Comm local_comm;\n",
    "    MPI_Comm_split_type(MPI_COMM_WORLD, MPI_COMM_TYPE_SHARED, rank, MPI_INFO_NULL, &local_comm);\n",
    "    MPI_Comm_rank(local_comm, &local_rank);\n",
    "    MPI_Comm_free(&local_comm);\n",
    "}\n",
    "cudaSetDevice(local_rank);\n",
    "```\n",
    "- **But :** pas de collisions entre plusieurs processus MPI du même nœud.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **Un seul stream CUDA (pour graph)**\n",
    "\n",
    "On utilise ici un seul stream `s` :\n",
    "```c\n",
    "cudaStream_t s;\n",
    "cudaStreamCreate(&s);\n",
    "```\n",
    "- **But :** tout sera capturé dans un graph, donc plus besoin de gérer plusieurs streams manuellement.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. **Création et capture des CUDA Graphs**\n",
    "\n",
    "On capture deux graphs, pour la parité (ping-pong des buffers). Tout le contenu d’une itération est capturé :\n",
    "\n",
    "```c\n",
    "cudaGraph_t       graph[2];\n",
    "cudaGraphExec_t   graphExec[2];\n",
    "\n",
    "for (int g=0; g<2; g++) {\n",
    "    if (g==1) swap_ptrs(&d_a, &d_a_new);\n",
    "    cudaStreamBeginCapture(s, cudaStreamCaptureModeGlobal);\n",
    "\n",
    "    // Calcul bandes haut/bas (kernel sur s)\n",
    "    dim3 grid_band((nx+BLOCK-1)/BLOCK, 1);\n",
    "    jacobi_kernel_slice<<<grid_band, block, 0, s>>>(d_a, d_a_new, nx, 1, 2);\n",
    "    jacobi_kernel_slice<<<grid_band, block, 0, s>>>(d_a, d_a_new, nx, local_ny, local_ny+1);\n",
    "\n",
    "    // Calcul bulk intérieur (kernel sur s)\n",
    "    if (iy_end_interior > iy_start_interior) {\n",
    "        dim3 grid_interior((nx+BLOCK-1)/BLOCK, (iy_end_interior-iy_start_interior+BLOCK-1)/BLOCK);\n",
    "        jacobi_kernel_slice<<<grid_interior, block, 0, s>>>(d_a, d_a_new, nx, iy_start_interior, iy_end_interior);\n",
    "    }\n",
    "\n",
    "    // NCCL halos dans le même stream/capture\n",
    "    ncclGroupStart();\n",
    "    ncclRecv(d_a_new + IDX(0,0,nx), nx, ncclDouble, prev, nccl_comm, s);\n",
    "    ncclSend(d_a_new + IDX(1,0,nx), nx, ncclDouble, prev, nccl_comm, s);\n",
    "    ncclRecv(d_a_new + IDX(ghost_ny-1,0,nx), nx, ncclDouble, next, nccl_comm, s);\n",
    "    ncclSend(d_a_new + IDX(local_ny,0,nx), nx, ncclDouble, next, nccl_comm, s);\n",
    "    ncclGroupEnd();\n",
    "\n",
    "    cudaStreamEndCapture(s, &graph[g]);\n",
    "    cudaGraphInstantiate(&graphExec[g], graph[g], NULL, NULL, 0);\n",
    "    if (g==1) swap_ptrs(&d_a, &d_a_new);\n",
    "}\n",
    "```\n",
    "- **`cudaStreamBeginCapture` / `cudaStreamEndCapture`** : tout ce qui est entre est enregistré dans un CUDA Graph.\n",
    "- **`cudaGraphInstantiate`** : le graph capturé devient un objet exécutable.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. **Lancement de la boucle Jacobi par graph**\n",
    "\n",
    "Chaque itération lance simplement un des deux graphs, puis synchronise le stream :\n",
    "```c\n",
    "int idx = iter & 1;\n",
    "cudaGraphLaunch(graphExec[idx], s);\n",
    "cudaStreamSynchronize(s);\n",
    "```\n",
    "- **But :** coût d’appel quasi-nul, pipelines hardware, moins d’overhead CPU.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. **Réduction de l’erreur toutes les N itérations**\n",
    "\n",
    "Pour ne pas sortir du graph à chaque boucle, la norme de l’erreur (convergence) est calculée moins fréquemment :\n",
    "```c\n",
    "if (iter % 10 == 0) {\n",
    "    error = thrust::transform_reduce(\n",
    "        thrust::make_zip_iterator(thrust::make_tuple(ptr_new, ptr_old)),\n",
    "        thrust::make_zip_iterator(thrust::make_tuple(ptr_new + local_N, ptr_old + local_N)),\n",
    "        max_abs_diff(),\n",
    "        0.0,\n",
    "        thrust::maximum<double>());\n",
    "    MPI_Allreduce(MPI_IN_PLACE, &error, 1, MPI_DOUBLE, MPI_MAX, MPI_COMM_WORLD);\n",
    "}\n",
    "```\n",
    "- **But :** réduire l’overhead du critère d’arrêt, profiter du GPU au max.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. **Nettoyage des graphs et objets CUDA**\n",
    "\n",
    "On libère explicitement :\n",
    "```c\n",
    "cudaGraphExecDestroy(graphExec[0]);\n",
    "cudaGraphExecDestroy(graphExec[1]);\n",
    "cudaStreamDestroy(s);\n",
    "ncclCommDestroy(nccl_comm);\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7. **Intérêt**\n",
    "\n",
    "- **Utilisation maximale du hardware** (pipelining, batch de kernels, latence minimale).\n",
    "- **Scalabilité** sur gros clusters GPU/multi-nœuds (communiquer et calculer en masse).\n",
    "- **Très peu d’overhead CPU** (le host n’intervient quasiment plus dans la boucle).\n",
    "\n",
    "---\n",
    "\n",
    "## 8. **Sur NCCL (dans ce contexte)**\n",
    "\n",
    "- **NCCL dans un CUDA Graph** : permet de pipeline les transferts et calculs sur le device dans le même DAG hardware.\n",
    "- **Tout reste sur le device**, sans synchronisation coûteuse côté host.\n",
    "\n",
    "---\n",
    "\n",
    "## Compilation et execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35845d0f-801b-4e8f-a94b-8d330fe19d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc -O3 -arch=sm_90 -std=c++11 -ccbin=mpicxx -use_fast_math -I/home/scortinhal/.spack/userspace-installed/linux-rhel9-neoverse_v2/linux-rhel9-neoverse_v2/gcc-11.4.1/nccl-2.22.3-1-6mbbenokgc3egfcu6fgqxjwi7ea5oqc6/include -o jacobi_nccl jacobi.cu -L/home/scortinhal/.spack/userspace-installed/linux-rhel9-neoverse_v2/linux-rhel9-neoverse_v2/gcc-11.4.1/nccl-2.22.3-1-6mbbenokgc3egfcu6fgqxjwi7ea5oqc6/lib -lm -lmpi -lnccl -lcudart\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Compilation terminated.\n",
      "make: *** [Makefile:20: jacobi_nccl] Error 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process is interrupted.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd NCCL-graph\n",
    "\n",
    "eval $(spack load --sh nccl)\n",
    "export LD_LIBRARY_PATH=$(spack location -i nccl)/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "mpirun -np 4 ./jacobi_nccl 4096 4096 1000 1e-6 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554f7281-d79c-4257-84ce-3567d78a22a1",
   "metadata": {},
   "source": [
    "# Partie 8 : NVSHMEM\n",
    "\n",
    "NVSHMEM est une bibliothèque de communication destinée au calcul parallèle multi-GPU.\n",
    "Son but est de faciliter et d’accélérer les échanges de données entre GPUs (sur le même nœud ou sur des nœuds différents) via des opérations de type mémoire partagée distribuée (Partitioned Global Address Space, PGAS).\n",
    "Rôle principal\n",
    "\n",
    "Partage de mémoire entre GPUs : chaque GPU (ou processus) a accès à un « heap symétrique » : une région de mémoire device accessible, en lecture/écriture, par tous les autres GPUs via des pointeurs d’adresse globale.\n",
    "    \n",
    "## 1. Initialisation de NVSHMEM\n",
    "\n",
    "**Initialisation MPI et attribution GPU par processus local** :\n",
    "```c\n",
    "int local_rank;\n",
    "MPI_Comm local_comm;\n",
    "MPI_Comm_split_type(MPI_COMM_WORLD, MPI_COMM_TYPE_SHARED, rank, MPI_INFO_NULL, &local_comm);\n",
    "MPI_Comm_rank(local_comm, &local_rank);\n",
    "cudaSetDevice(local_rank);\n",
    "cudaFree(0);\n",
    "MPI_Comm_free(&local_comm);\n",
    "```\n",
    "- Permet d’avoir un GPU par processus MPI sur le nœud.\n",
    "\n",
    "**Initialisation NVSHMEM avec MPI** :\n",
    "```c\n",
    "MPI_Comm mpi_comm = MPI_COMM_WORLD;\n",
    "nvshmemx_init_attr_t attr;\n",
    "attr.mpi_comm = &mpi_comm;\n",
    "nvshmemx_init_attr(NVSHMEMX_INIT_WITH_MPI_COMM, &attr);\n",
    "```\n",
    "- NVSHMEM est initialisé à partir du communicateur MPI. Prend la main sur les échanges device-to-device.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Allocation de la mémoire sur le heap symétrique\n",
    "\n",
    "```c\n",
    "double *d_a     = (double*)nvshmem_malloc(bytes);\n",
    "double *d_a_new = (double*)nvshmem_malloc(bytes);\n",
    "```\n",
    "- Permet à chaque processus de lire/écrire dans le segment mémoire de tous les autres PEs.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Communication des halos par NVSHMEM\n",
    "\n",
    "Dans la boucle, on remplace MPI/NCCL par deux `nvshmem_double_put` (one-sided PUT) :\n",
    "\n",
    "```c\n",
    "// Envoi de la dernière ligne réelle dans le halo bas du voisin du bas\n",
    "nvshmem_double_put(\n",
    "    d_a + IDX(ghost_nys[next]-1, 0, nx),     // Destination : halo bas du voisin next\n",
    "    d_a + IDX(local_ny, 0, nx),              // Source      : ma dernière ligne réelle\n",
    "    nx, next);\n",
    "// Envoi de la première ligne réelle dans le halo haut du voisin du haut\n",
    "nvshmem_double_put(\n",
    "    d_a + IDX(0, 0, nx),                     // Destination : halo haut du voisin prev\n",
    "    d_a + IDX(1, 0, nx),                     // Source      : ma première ligne réelle\n",
    "    nx, prev);\n",
    "```\n",
    "- L’adresse d’arrivée est calculée selon le découpage, grâce aux tableaux `ghost_nys`.\n",
    "- Ces échanges se font **sans synchronisation immédiate** du récepteur.\n",
    "\n",
    "**Synchronisation mémoire (fence) et barrière globale :**\n",
    "```c\n",
    "nvshmem_fence();\n",
    "nvshmem_barrier_all();\n",
    "```\n",
    "- `nvshmem_fence()` : assure la visibilité de toutes les écritures PUT précédentes.\n",
    "- `nvshmem_barrier_all()` : attend que tout le monde ait terminé les halos (prérequis à la cohérence du calcul).\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Boucle Jacobi, calcul CUDA\n",
    "\n",
    "Rien ne change côté kernel, à part l’utilisation du heap symétrique.\n",
    "\n",
    "```c\n",
    "jacobi_kernel<<<grid, block, 0, stream>>>(d_a, d_a_new, nx, ghost_ny);\n",
    "cudaStreamSynchronize(stream);\n",
    "```\n",
    "- L’intégralité du calcul se fait sur la mémoire allouée NVSHMEM.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Réduction pour la convergence (MPI_Allreduce)\n",
    "\n",
    "Le calcul de l’erreur max reste sur chaque GPU (via thrust), puis est globalisé :\n",
    "```c\n",
    "thrust::device_ptr<double> ptr_new(d_a_new);\n",
    "thrust::device_ptr<double> ptr_old(d_a);\n",
    "error = thrust::transform_reduce(\n",
    "    thrust::make_zip_iterator(thrust::make_tuple(ptr_new, ptr_old)),\n",
    "    thrust::make_zip_iterator(thrust::make_tuple(ptr_new + ghost_ny*nx, ptr_old + ghost_ny*nx)),\n",
    "    max_abs_diff(),\n",
    "    0.0,\n",
    "    thrust::maximum<double>());\n",
    "MPI_Allreduce(MPI_IN_PLACE, &error, 1, MPI_DOUBLE, MPI_MAX, MPI_COMM_WORLD);\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Nettoyage NVSHMEM\n",
    "\n",
    "À la fin, on libère la mémoire et on arrête NVSHMEM :\n",
    "```c\n",
    "if(d_a) nvshmem_free(d_a);\n",
    "if(d_a_new) nvshmem_free(d_a_new);\n",
    "cudaStreamDestroy(stream);\n",
    "nvshmem_finalize();\n",
    "MPI_Finalize();\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Points clés sur NVSHMEM\n",
    "\n",
    "- **PUT/GET** one-sided : Pas besoin que le voisin soit \"prêt\" (contrairement à MPI_Send/Recv).\n",
    "- **Heap symétrique** : Obligatoire pour l'adressage direct de la mémoire entre processus.\n",
    "- **Fence/barrière** : Pour garantir que les échanges sont terminés avant de lancer le calcul.\n",
    "- **Remplace NCCL/MPI** pour les halos, mais la réduction d’erreur utilise toujours MPI_Allreduce (car NVSHMEM n'a pas d'opérations collectives avancées pour la réduction).\n",
    "  \n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "78e010da-4b63-4d81-9782-7f7856db762e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make: Nothing to be done for 'all'.\n",
      "[NVSHMEM] Temps d'init avant NVSHMEM : 2.425963s\n",
      "[NVSHMEM] Temps d'init NVSHMEM : 4.042471s\n",
      "[NVSHMEM] Converged in 1000 iterations | Error: 2.42e-04\n",
      "Temps de calcul Jacobi (seulement boucle)         : 0.133587s\n",
      "Temps setup+calcul+affichage (après init SHMEM)   : 0.133753s\n",
      "Temps total du programme (tout compris)           : 6.263870s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd NVSHMEM\n",
    "\n",
    "# Charge NVSHMEM et NCCL\n",
    "export NVSHMEM_HOME=/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/nvshmem\n",
    "export LD_LIBRARY_PATH=$NVSHMEM_HOME/lib:$LD_LIBRARY_PATH\n",
    "export CPATH=$NVSHMEM_HOME/include:$CPATH\n",
    "export PATH=$NVSHMEM_HOME/bin:$PATH\n",
    "\n",
    "eval $(spack load --sh nccl)\n",
    "export LD_LIBRARY_PATH=$(spack location -i nccl)/lib:$LD_LIBRARY_PATH\n",
    "export NVSHMEM_ENABLE_ALL_DEVICE_INLINING=1\n",
    "\n",
    "\n",
    "make \n",
    "mpirun -np 4 ./jacobi_nvshmem 4096 4096 1000 1e-6 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa7c996-4c15-4a89-9241-d7555763ce83",
   "metadata": {},
   "source": [
    "# Partie 9 : NVSHMEM-LTO\n",
    "\n",
    "- Ajout du support de la compilation **LTO CUDA** (Link-Time Optimization) dans le Makefile :\n",
    "    - Ajout du flag `-gencode=arch=compute_90,code=lto_90` à la variable NVCCFLAGS pour activer la génération du code intermédiaire LTO pour l’architecture sm_90.\n",
    "    - Pas de modification du code source, uniquement une optimisation de la génération du binaire final via la chaîne de compilation NVCC pour de meilleures performances potentielles à l’exécution.\n",
    "    - Optimiser les appels de fonctions entre différents fichiers sources CUDA (inlining, suppression de code mort, etc.)\n",
    "    - Réorganiser ou fusionner des kernels s’il le juge pertinent.\n",
    "    - Générer un code plus performant, parfois plus rapide à l’exécution que le même code compilé sans LTO.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f68c86fb-c880-411f-bdf0-81a5bd37c927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f jacobi_nvshmem *.o *.qdrep *.sqlite\n",
      "nvcc -O3 -arch=sm_90  -std=c++17 --expt-relaxed-constexpr -rdc=true -gencode=arch=compute_90,code=sm_90 -gencode=arch=compute_90,code=lto_90 -Xcompiler \"-Wall -Wextra\" -ccbin=mpicxx -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/nvshmem/include -I/apps/2025/spack_install/linux-rhel9-neoverse_v2/linux-rhel9-neoverse_v2/gcc-11.4.1/cuda-12.6.2-3mzltpzgs4dekx5xvbqzz2no3j3tkcxq/include -o jacobi_nvshmem jacobi.cu -L/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/nvshmem/lib -lnvshmem -lnvshmem_host -L/apps/2025/spack_install/linux-rhel9-neoverse_v2/linux-rhel9-neoverse_v2/gcc-11.4.1/cuda-12.6.2-3mzltpzgs4dekx5xvbqzz2no3j3tkcxq/lib64 -lcudart -lmpi\n",
      "[NVSHMEM] Temps d'init avant NVSHMEM : 2.440521s\n",
      "[NVSHMEM] Temps d'init NVSHMEM : 4.019447s\n",
      "[NVSHMEM] Converged in 1000 iterations | Error: 2.42e-04\n",
      "Temps de calcul Jacobi (seulement boucle)         : 0.135151s\n",
      "Temps setup+calcul+affichage (après init SHMEM)   : 0.135318s\n",
      "Temps total du programme (tout compris)           : 6.128369s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd NVSHMEM-LTO\n",
    "\n",
    "\n",
    "export NVSHMEM_HOME=/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/nvshmem\n",
    "export CUDA_HOME=/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.3\n",
    "export LD_LIBRARY_PATH=$NVSHMEM_HOME/lib:$CUDA_HOME/lib64:$LD_LIBRARY_PATH\n",
    "export CPATH=$NVSHMEM_HOME/include:$CUDA_HOME/include:$CPATH\n",
    "export PATH=$NVSHMEM_HOME/bin:$CUDA_HOME/bin:$PATH\n",
    "\n",
    "# Charge NCCL via Spack si tu utilises aussi NCCL ailleurs\n",
    "eval $(spack load --sh nccl)\n",
    "export LD_LIBRARY_PATH=$(spack location -i nccl)/lib:$LD_LIBRARY_PATH\n",
    "export NVSHMEM_ENABLE_ALL_DEVICE_INLINING=1\n",
    "\n",
    "make clean\n",
    "make\n",
    "#  lance avec mpirun (remplace nvshmrun si dispo) :\n",
    "mpirun -np 4 ./jacobi_nvshmem 4096 4096 1000 1e-6 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361ada1e-aea3-4cb6-ac4f-2b930c159cad",
   "metadata": {},
   "source": [
    "# Partie 10 : NVSHMEM-neighborhood_sync-lto\n",
    "\n",
    "\n",
    "- Utilisation de **NVSHMEM** pour les échanges de halos entre GPUs (Remote Memory Access/RMA) :\n",
    "    - Chaque processus écrit directement dans la mémoire du halo (bord) de ses voisins grâce à `nvshmem_double_put`.\n",
    "    - Synchronisation mémoire assurée avec `nvshmem_fence()` puis barrière NVSHMEM pour garantir la cohérence avant le calcul.\n",
    "- Gestion avancée de la **synchronisation neighborhood** sur device (entre GPU voisins) :\n",
    "    - Ajout d’un kernel `syncneighborhood_kernel` pour notifier les voisins haut et bas que les halos sont prêts (utilisation de `nvshmemx_signal_op` et `nvshmem_uint64_wait_until_all` pour attendre les notifications des deux côtés).\n",
    "- Initialisation :\n",
    "    - Attribution automatique des GPU locaux par rang MPI intra-nœud.\n",
    "    - Calcul dynamique des tailles de sous-domaines (ghost_ny) pour chaque processus, prise en compte des décompositions déséquilibrées.\n",
    "    - Allocation de la mémoire sur le heap symétrique NVSHMEM (`nvshmem_malloc`) pour les tableaux et les variables de synchronisation, avec configuration dynamique de la variable d'environnement `NVSHMEM_SYMMETRIC_SIZE` pour garantir un espace mémoire suffisant.\n",
    "    - Initialisation de NVSHMEM avec un communicateur MPI personnalisé via `nvshmemx_init_attr`.\n",
    "- Calcul Jacobi toujours exécuté sur GPU via kernel CUDA, synchronisation des streams CUDA.\n",
    "- Calcul de l’erreur max avec **Thrust** sur GPU (utilisation de la bibliothèque Thrust pour la réduction GPU des différences a_new/a_old), puis MPI_Allreduce pour la convergence globale.\n",
    "- Mesure détaillée des temps : temps avant et après initialisation NVSHMEM, temps de calcul pur, temps total global, etc.\n",
    "- Rassemblement final et affichage de la grille sur le rang 0 avec gestion correcte des tailles locales, comme dans les versions précédentes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d7093a9d-f001-490b-bf43-06a84951ae4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f jacobi_nvshmem *.o *.qdrep *.sqlite\n",
      "nvcc -O3 -arch=sm_90  -std=c++17 --expt-relaxed-constexpr -rdc=true -gencode=arch=compute_90,code=sm_90 -gencode=arch=compute_90,code=lto_90 -Xcompiler \"-Wall -Wextra\" -ccbin=mpicxx -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/nvshmem/include -I/apps/2025/spack_install/linux-rhel9-neoverse_v2/linux-rhel9-neoverse_v2/gcc-11.4.1/cuda-12.6.2-3mzltpzgs4dekx5xvbqzz2no3j3tkcxq/include -o jacobi_nvshmem jacobi.cu -L/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/nvshmem/lib -lnvshmem -lnvshmem_host -L/apps/2025/spack_install/linux-rhel9-neoverse_v2/linux-rhel9-neoverse_v2/gcc-11.4.1/cuda-12.6.2-3mzltpzgs4dekx5xvbqzz2no3j3tkcxq/lib64 -lcudart -lmpi\n",
      "[NVSHMEM-neighSync] Converged in 1000 iterations | Error: 2.42e-04\n",
      "Temps de calcul Jacobi (seulement boucle)         : 0.129805s\n",
      "Temps setup+calcul+affichage (après init SHMEM)   : 0.130036s\n",
      "Temps d'init avant NVSHMEM                        : 2.443952s\n",
      "Temps d'init NVSHMEM                              : 3.984407s\n",
      "Temps total du programme (tout compris)           : 6.139337s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd NVSHMEM-neighborhood_sync-lto\n",
    "\n",
    "# Charge les bonnes librairies NVSHMEM et CUDA\n",
    "export NVSHMEM_HOME=/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/nvshmem\n",
    "export CUDA_HOME=/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.3\n",
    "export LD_LIBRARY_PATH=$NVSHMEM_HOME/lib:$CUDA_HOME/lib64:$LD_LIBRARY_PATH\n",
    "export CPATH=$NVSHMEM_HOME/include:$CUDA_HOME/include:$CPATH\n",
    "export PATH=$NVSHMEM_HOME/bin:$CUDA_HOME/bin:$PATH\n",
    "\n",
    "# Charge NCCL via Spack si nécessaire\n",
    "eval $(spack load --sh nccl)\n",
    "export LD_LIBRARY_PATH=$(spack location -i nccl)/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "# Active l'inlining maximal NVSHMEM\n",
    "export NVSHMEM_ENABLE_ALL_DEVICE_INLINING=1\n",
    "\n",
    "make clean && make\n",
    "\n",
    "mpirun -np 4 ./jacobi_nvshmem 4096 4096 1000 1e-6 0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91144fa-f2d8-4398-a778-f3dfbe61a129",
   "metadata": {},
   "source": [
    "# Partie 11 : nvshmem-norm_overlap-neighborhood_sync+lto \n",
    "\n",
    "- le calcul sur la grille locale est vraiment overlap avec l'échange des halos grâce à un découpage fin des kernels et à la synchronisation asynchrone device-side.\n",
    "\n",
    "- On découpe la mise à jour Jacobi en :\n",
    "    - `jacobi_inner_kernel` (zone intérieure, ne dépend pas du halo) qui est lancé pendant que les halos sont envoyés aux voisins.\n",
    "    - Une fois l'échange de halos terminé (vérifié via le kernel `syncneighborhood_kernel` qui utilise des signaux device NVSHMEMX),  \n",
    "      on lance `jacobi_border_kernel` sur les deux bandes frontalières haut/bas qui ont besoin du halo à jour.\n",
    "  Ce découpage permet de **masquer le coût de communication** et d'accélérer la convergence pour des grosses grilles.\n",
    "\n",
    "- **Synchronisation neighborhood**: au lieu d'une barrière globale (`nvshmem_barrier_all`), chaque processus attend juste un signal de ses deux voisins directs (haut/bas) pour la cohérence des halos : cela limite l'attente et améliore l'overlap comm/calcul.\n",
    "\n",
    "- Le reste de la structure générale (allocation heap symétrique, gestion des tailles fantômes variables, calcul erreur avec thrust, réduction MPI_Allreduce, réassemblage final) reste identique à la version précédente.\n",
    "\n",
    "- L'objectif est de **maximiser le recouvrement entre calcul local et communication** des halos, ce qui est particulièrement efficace sur des architectures GPU modernes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d8280652-6caf-4dbb-9593-70015d37395f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f jacobi_nvshmem *.o *.qdrep *.sqlite\n",
      "nvcc -O3 -arch=sm_90  -std=c++17 --expt-relaxed-constexpr -rdc=true -gencode=arch=compute_90,code=sm_90 -gencode=arch=compute_90,code=lto_90 -Xcompiler \"-Wall -Wextra\" -ccbin=mpicxx -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/nvshmem/include -I/apps/2025/spack_install/linux-rhel9-neoverse_v2/linux-rhel9-neoverse_v2/gcc-11.4.1/cuda-12.6.2-3mzltpzgs4dekx5xvbqzz2no3j3tkcxq/include -o jacobi_nvshmem jacobi.cu -L/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/nvshmem/lib -lnvshmem -lnvshmem_host -L/apps/2025/spack_install/linux-rhel9-neoverse_v2/linux-rhel9-neoverse_v2/gcc-11.4.1/cuda-12.6.2-3mzltpzgs4dekx5xvbqzz2no3j3tkcxq/lib64 -lcudart -lmpi\n",
      "[NVSHMEM-norm_overlap-neighSync+LTO] Converged in 1000 iterations | Error: 2.42e-04\n",
      "Temps de calcul Jacobi (seulement boucle)         : 0.114876s\n",
      "Temps setup+calcul+affichage (après init SHMEM)   : 0.115086s\n",
      "Temps d'init avant NVSHMEM                        : 2.432568s\n",
      "Temps d'init NVSHMEM                              : 4.024340s\n",
      "Temps total du programme (tout compris)           : 6.002402s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd NVSHMEM-norm_overlap-neighborhood_sync+lto\n",
    "\n",
    "# Charge les bonnes librairies NVSHMEM et CUDA\n",
    "export NVSHMEM_HOME=/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/nvshmem\n",
    "export CUDA_HOME=/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.3\n",
    "export LD_LIBRARY_PATH=$NVSHMEM_HOME/lib:$CUDA_HOME/lib64:$LD_LIBRARY_PATH\n",
    "export CPATH=$NVSHMEM_HOME/include:$CUDA_HOME/include:$CPATH\n",
    "export PATH=$NVSHMEM_HOME/bin:$CUDA_HOME/bin:$PATH\n",
    "\n",
    "# Charge NCCL via Spack si nécessair\n",
    "\n",
    "eval $(spack load --sh nccl)\n",
    "export LD_LIBRARY_PATH=$(spack location -i nccl)/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "# Active l'inlining maximal NVSHMEM\n",
    "export NVSHMEM_ENABLE_ALL_DEVICE_INLINING=1\n",
    "\n",
    "make clean && make\n",
    "\n",
    "mpirun -np 4 ./jacobi_nvshmem 4096 4096 1000 1e-6 0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bef36e4-97e0-49c0-b629-5bce390fbeb2",
   "metadata": {},
   "source": [
    "# Partie 12 : nvshmem-norm_overlap-neighborhood_sync+lto[1]  \n",
    "\n",
    "\n",
    "- **Ici, c'est une version \"baseline\" (de base, séquentielle côté device)** :\n",
    "    - On utilise **un seul kernel Jacobi** (`jacobi_kernel_full`) qui fait tout le calcul d'un coup sur tout le sous-domaine local, sans découpage ni overlap calcul/communication.\n",
    "    - **Pas de découpage** en \"zone intérieure\" et \"zone de bord\" : tout est fait d’un bloc, donc la communication des halos ne peut pas être masquée par le calcul local.\n",
    "    - **Pas de synchronisation neighborhood device** (pas de kernel de synchronisation avec les voisins via NVSHMEMX), ni de stratégie fine pour attendre juste les voisins.\n",
    "    - On se contente d’un schéma classique : \n",
    "        - On fait tout le calcul local,\n",
    "        - Puis l’erreur est calculée (avec thrust),\n",
    "        - Puis un `MPI_Allreduce` pour la convergence globale,\n",
    "        - Et on passe à l’itération suivante.\n",
    "\n",
    "- **En résumé :**  \n",
    "  - Version \"simple\" qui sert de référence pour les perfs : pas d’overlap calcul/comm, pas d’optimisation, pas de synchronisation fine, **juste un Jacobi standard sur tout le domaine local**.\n",
    "  - Cela permet de comparer l’apport réel des optimisations dans les autres versions (découpage kernels + overlap, synchronisation neighborhood, etc).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f28e42c0-e360-4fa8-a2af-65a4b823fa2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f jacobi_nvshmem *.o *.qdrep *.sqlite\n",
      "nvcc -O3 -arch=sm_90  -std=c++17 --expt-relaxed-constexpr -rdc=true -gencode=arch=compute_90,code=sm_90 -gencode=arch=compute_90,code=lto_90 -Xcompiler \"-Wall -Wextra\" -ccbin=mpicxx -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/nvshmem/include -I/apps/2025/spack_install/linux-rhel9-neoverse_v2/linux-rhel9-neoverse_v2/gcc-11.4.1/cuda-12.6.2-3mzltpzgs4dekx5xvbqzz2no3j3tkcxq/include -o jacobi_nvshmem jacobi.cu -L/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/nvshmem/lib -lnvshmem -lnvshmem_host -L/apps/2025/spack_install/linux-rhel9-neoverse_v2/linux-rhel9-neoverse_v2/gcc-11.4.1/cuda-12.6.2-3mzltpzgs4dekx5xvbqzz2no3j3tkcxq/lib64 -lcudart -lmpi\n",
      "[NVSHMEM-baseline-single-rank] Converged in 1000 iterations | Error: 2.42e-04\n",
      "Temps de calcul Jacobi (seulement boucle)         : 0.092085s\n",
      "Temps setup+calcul+affichage (après init SHMEM)   : 0.092252s\n",
      "Temps d'init avant NVSHMEM                        : 2.448728s\n",
      "Temps d'init NVSHMEM                              : 3.991321s\n",
      "Temps total du programme (tout compris)           : 5.868052s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd NVSHMEM-norm_overlap-neighborhood_sync+lto[1]\n",
    "\n",
    "# Charge les bonnes librairies NVSHMEM et CUDA\n",
    "export NVSHMEM_HOME=/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/nvshmem\n",
    "export CUDA_HOME=/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.3\n",
    "export LD_LIBRARY_PATH=$NVSHMEM_HOME/lib:$CUDA_HOME/lib64:$LD_LIBRARY_PATH\n",
    "export CPATH=$NVSHMEM_HOME/include:$CUDA_HOME/include:$CPATH\n",
    "export PATH=$NVSHMEM_HOME/bin:$CUDA_HOME/bin:$PATH\n",
    "\n",
    "# Charge NCCL via Spack si nécessair\n",
    "\n",
    "eval $(spack load --sh nccl)\n",
    "export LD_LIBRARY_PATH=$(spack location -i nccl)/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "# Active l'inlining maximal NVSHMEM\n",
    "export NVSHMEM_ENABLE_ALL_DEVICE_INLINING=1\n",
    "\n",
    "make clean && make\n",
    "\n",
    "mpirun -np 4 ./jacobi_nvshmem 4096 4096 1000 1e-6 0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf1811b-4221-45e4-9c36-0541f17072f6",
   "metadata": {},
   "source": [
    "![MPI-GPU](Graph.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07545e39-451d-4514-b8fb-cde0e493f14f",
   "metadata": {},
   "source": [
    "![MPI-GPU](Result.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc2307e-09f6-4c27-bba5-35a10e54afb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
