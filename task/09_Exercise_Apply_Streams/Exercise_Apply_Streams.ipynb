{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#65AE11;\">Exercise: Apply Copy/Compute Overlap</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section you will perform copy/compute overlap in the cipher application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#65AE11;\">Objectives</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the time you complete this section you will:\n",
    "\n",
    "* Be able to perform copy/compute overlap using CUDA Streams in a CUDA C++ application\n",
    "* Observe copy/compute overlap in the Nsight Systems timeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#65AE11;\">Exercise Instructions</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the techniques from the previous sections to perform copy/compute overlap in [streams.cu](streams_cipher/streams.cu).\n",
    "\n",
    "Use the terminal to run `make streams` to compile the program, and then `./streams` to run it. You will see the timing outputs and check for correctness. See the [Makefile](streams_cipher/Makefile) for details.\n",
    "\n",
    "After a successful refactor, adjust the number of streams (and therefore the size of memory chunks) and rerun to try to find the optimal number of streams.\n",
    "\n",
    "**As a goal try to get the total amount of time (including memory transfers) on the GPU below 100ms, or even below 75ms.**\n",
    "\n",
    "Use the terminal to run `make profile` to generate a report file that will be named `streams-report.qdrep`, and which you can open in Nsight Systems. See the [Makefile](streams_cipher/Makefile) for details.\n",
    "\n",
    "The following screenshot, shows a profiler view of almost all host-to-device memory transfer (green) and device-to-host memory transfer (violet) overlapping with GPU compute (blue):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![streams solution](images/streams_solution.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#65AE11;\">Exercise Hints</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like, expand the following hints to guide your work:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* All your work should be within the `main` function\n",
    "* Define a number of streams\n",
    "* Create the number of streams you defined and store them in an array\n",
    "* As you work, edit the use of the timer instances, including their message strings, to reflect changes you make to the application\n",
    "* Using the number of entries and the number of streams, define a chunk size for each stream's work. Remember to use the round-up division helper function `sdiv` for the reasons discussed in the previous section\n",
    "* For each stream you have created:\n",
    "  * Create indices for it to correctly access its chunk of data from within the global data\n",
    "  * Asynchronously copy its chunk of data to the device\n",
    "  * Perform the `decryptGPU` computations for its chunk of data\n",
    "  * Asynchronously copy its chunk of data back to the host\n",
    "  * Synchronize each stream before continuing on to check results on the CPU\n",
    "* `make clean` will delete all binaries and report files\n",
    "* You can edit the [*Makefile*](streams_cipher/Makefile) as you wish, for example, to change the name of generated binaries or report files. You can of course also enter the commands found in the *Makefile* directly into the terminal\n",
    "* If you have time, play around with different numbers of streams, aiming to reduce the total time the application spends on the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ma version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gpfs/home/scortinhal/CHPS0904/task/09_Exercise_Apply_Streams\n",
      "rm -f streams streams_solution *.qdrep *.sqlite\n",
      "nvcc -arch=sm_70 -O3 -Xcompiler=\"-march=native -fopenmp\" streams.cu -o streams\n",
      "nsys profile --stats=true --force-overwrite=true -o streams-report ./streams\n",
      "concurrentKernels: 1, asyncEngineCount: 3\n",
      "TIMING: 33.4562 ms (total time on GPU)\n",
      "STATUS: test passed\n",
      "TIMING: 15613 ms (Temps total)\n",
      "Collecting data...\n",
      "Generating '/tmp/nsys-report-118e.qdstrm'\n",
      "[1/8] [========================100%] streams-report.nsys-rep\n",
      "[2/8] [========================100%] streams-report.sqlite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SKIPPED: /gpfs/home/scortinhal/CHPS0904/task/09_Exercise_Apply_Streams/kernel_streams_cipher/streams-report.sqlite does not contain NV Tools Extension (NVTX) data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)     Med (ns)    Min (ns)  Max (ns)   StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  -----------  -----------  --------  ---------  -----------  ----------------------\n",
      "     57.0      15892758464        168   94599752.8  100115424.0      1280  158675584   23370593.1  poll                  \n",
      "     41.9      11680803008         63  185409571.6    4501504.0     29344  780538272  314659151.0  futex                 \n",
      "      1.1        305753888        893     342389.6      11520.0      1024   81645408    3058807.4  ioctl                 \n",
      "      0.0          6764032         17     397884.2     353344.0    333056     834176     124566.7  pthread_create        \n",
      "      0.0          5888544         38     154961.7      10144.0      2304    5464736     884652.7  mmap                  \n",
      "      0.0           760064          3     253354.7      14080.0      4320     741664     422916.4  fflush                \n",
      "      0.0           757600         81       9353.1       6016.0      2848     120576      13765.8  fopen                 \n",
      "      0.0           574400         48      11966.7      10752.0      5984      26528       3727.1  mmap64                \n",
      "      0.0           394176         10      39417.6      31264.0     14656     133920      34496.5  sem_timedwait         \n",
      "      0.0           372288         77       4834.9       4608.0      1024      10976       1845.5  open64                \n",
      "      0.0           177504         37       4797.4       1568.0      1056      90720      14719.4  fclose                \n",
      "      0.0            83296          1      83296.0      83296.0     83296      83296          0.0  pthread_cond_wait     \n",
      "      0.0            81504          4      20376.0      14672.0     11072      41088      14090.1  fread                 \n",
      "      0.0            68128          5      13625.6       4768.0      2752      44768      17741.0  fgets                 \n",
      "      0.0            63264          9       7029.3       5760.0      1504      14464       4114.3  open                  \n",
      "      0.0            51712          6       8618.7       1248.0      1024      42880      16827.4  fcntl                 \n",
      "      0.0            33536         11       3048.7       3392.0      1568       4608       1111.7  write                 \n",
      "      0.0            32352          4       8088.0       5968.0      4384      16032       5350.3  fopen64               \n",
      "      0.0            29376         10       2937.6       2256.0      1056       6848       2080.1  read                  \n",
      "      0.0            28576          6       4762.7       4976.0      1408       7424       2355.5  munmap                \n",
      "      0.0            18912          3       6304.0       6432.0      3040       9440       3201.9  pipe2                 \n",
      "      0.0            15296          2       7648.0       7648.0      4736      10560       4118.2  socket                \n",
      "      0.0            12800          1      12800.0      12800.0     12800      12800          0.0  connect               \n",
      "      0.0             7040          3       2346.7       2304.0      1216       3520       1152.6  fwrite                \n",
      "      0.0             5280          4       1320.0       1264.0      1216       1536        150.9  pthread_mutex_trylock \n",
      "      0.0             4736          1       4736.0       4736.0      4736       4736          0.0  bind                  \n",
      "      0.0             3232          1       3232.0       3232.0      3232       3232          0.0  pthread_cond_broadcast\n",
      "      0.0             1440          1       1440.0       1440.0      1440       1440          0.0  listen                \n",
      "      0.0             1152          1       1152.0       1152.0      1152       1152          0.0  dup                   \n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)  StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  --------  -----------  ----------------------\n",
      "     44.7         31030848         32    969714.0      2192.0      1632   7063648    1937751.7  cudaStreamSynchronize \n",
      "     33.8         23473568          1  23473568.0  23473568.0  23473568  23473568          0.0  cudaHostAlloc         \n",
      "     14.5         10040128          1  10040128.0  10040128.0  10040128  10040128          0.0  cudaFreeHost          \n",
      "      2.0          1398176          1   1398176.0   1398176.0   1398176   1398176          0.0  cudaMalloc            \n",
      "      1.7          1175360         32     36730.0      4832.0      3136    476096      86046.2  cudaStreamCreate      \n",
      "      1.3           885824         32     27682.0      4832.0      3808    697568     122273.3  cudaLaunchKernel      \n",
      "      1.2           838464          1    838464.0    838464.0    838464    838464          0.0  cudaFree              \n",
      "      0.4           294784         64      4606.0      3184.0      1632     51744       6518.4  cudaMemcpyAsync       \n",
      "      0.3           207616         32      6488.0      5664.0      4352     28128       4113.7  cudaStreamDestroy     \n",
      "      0.1            74368          4     18592.0     13216.0      8768     39168      14274.0  cudaEventRecord       \n",
      "      0.0            14304          2      7152.0      7152.0      5792      8512       1923.3  cudaEventSynchronize  \n",
      "      0.0            12608          4      3152.0      1424.0       416      9344       4200.4  cudaEventCreate       \n",
      "      0.0             3296          4       824.0       816.0       224      1440        533.8  cudaEventDestroy      \n",
      "      0.0             2016          1      2016.0      2016.0      2016      2016          0.0  cuModuleGetLoadingMode\n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances   Avg (ns)    Med (ns)   Min (ns)  Max (ns)  StdDev (ns)                             Name                           \n",
      " --------  ---------------  ---------  ----------  ----------  --------  --------  -----------  ----------------------------------------------------------\n",
      "    100.0        396226576         32  12382080.5  14275343.5   5667200  16056959    3392594.0  decrypt_gpu(unsigned long *, unsigned long, unsigned long)\n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)           Operation          \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ----------------------------\n",
      "     63.6          3235360     32  101105.0  101072.0    100448    101952        375.0  [CUDA memcpy Device-to-Host]\n",
      "     36.4          1850592     32   57831.0   45584.0     43360    110144      23203.5  [CUDA memcpy Host-to-Device]\n",
      "\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)           Operation          \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ----------------------------\n",
      "    536.871     32    16.777    16.777    16.777    16.777        0.000  [CUDA memcpy Device-to-Host]\n",
      "    536.871     32    16.777    16.777    16.777    16.777        0.000  [CUDA memcpy Host-to-Device]\n",
      "\n",
      "Generated:\n",
      "    /gpfs/home/scortinhal/CHPS0904/task/09_Exercise_Apply_Streams/kernel_streams_cipher/streams-report.nsys-rep\n",
      "    /gpfs/home/scortinhal/CHPS0904/task/09_Exercise_Apply_Streams/kernel_streams_cipher/streams-report.sqlite\n",
      "concurrentKernels: 1, asyncEngineCount: 3\n",
      "TIMING: 33.2243 ms (total time on GPU)\n",
      "STATUS: test passed\n",
      "TIMING: 14877.3 ms (Temps total)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Affiche le répertoire courant pour vérifier le point de départ\n",
    "pwd\n",
    "\n",
    "cd kernel_streams_cipher\n",
    "\n",
    "\n",
    "module load cuda/12.6\n",
    "\n",
    "make clean\n",
    "make profile\n",
    "\n",
    "./streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#65AE11;\">Exercise Solution</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you complete your work, or if you get stuck, refer to [the solution](streams_cipher/streams_solution.cu). If you wish, you can compile the solution with `make streams_solution`, and/or generate a report file for viewing in Nsight Systems with `make profile_solution`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#65AE11;\">Next</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have demonstrated the ability to perform copy/compute overlap, we will shift our attention for the next few sections to utilizing multiple GPUs on the same node before, at the end of the course, combining the use of multiple GPUs with copy/compute overlap.\n",
    "\n",
    "Please continue to the next section: [*Multiple GPUs*](../10_Multiple_GPUs/Multiple_GPUs.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
