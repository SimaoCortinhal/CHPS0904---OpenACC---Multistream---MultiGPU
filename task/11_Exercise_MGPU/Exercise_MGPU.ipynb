{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#65AE11;\">Exercise: Use Multiple GPUs</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section you will refactor the baseline cipher application to utilize multiple GPUs.\n",
    "\n",
    "*Please note, you will be working with the baseline cipher application that **does not use multiple non-default streams**. For the sake of learning you will be focusing on multiple GPU usage in this section, before combining multiple GPUs with multiple non-default streams in the next section.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#65AE11;\">Objectives</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the time you complete this section you will:\n",
    "\n",
    "* Be able to utilize multiple GPUs in a CUDA C++ application\n",
    "* Observe multiple GPU usage in the Nsight Systems timeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#65AE11;\">Exercise Instructions</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the techniques from the previous section to utilize multiple GPUs in [mgpu.cu](mgpu_cipher/mgpu.cu).\n",
    "\n",
    "Use the terminal to run `make mgpu` to compile the program, and then `./mgpu` to run it. You will see the timing outputs and check for correctness. See the [Makefile](mgpu_cipher/Makefile) for details.\n",
    "\n",
    "**As a goal try to get the amount of time spent decrypting on the GPUs (not including memory transfers) below 20ms.**\n",
    "\n",
    "Use the terminal to run `make profile` to generate a report file that will be named `mgpu-report.qdrep`, and which you can open in Nsight Systems. See the [Makefile](mgpu_cipher/Makefile) for details.\n",
    "\n",
    "The following screenshot shows the application utilizing multiple GPUs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![multiple gpus](images/multiple_gpus.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#65AE11;\">Exercise Hints</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like, expand the following hints to guide your work:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* All your work should be within the `main` function\n",
    "* Store the number of GPUs available in a variable for later use\n",
    "* Using the number of entries and the number of GPUs, define a chunk size for each stream's work. Remember to use the round-up division helper function `sdiv` for the reasons discussed in a previous section\n",
    "* Create an array that contains pointers for the memory that will be allocated on each GPU\n",
    "* Allocate a chunk's worth of data for each GPU\n",
    "* Copy the correct chunk of data to each GPU\n",
    "* For each GPU, decrypt its chunk of data\n",
    "* Copy each GPU's chunk of data back to the host\n",
    "* You may wish to edit the use of the timer instances, including their message strings, to reflect changes you make to the application\n",
    "* `make clean` will delete all binaries and report files\n",
    "* You can edit the [*Makefile*](mgpu_cipher/Makefile) as you wish, for example, to change the name of generated binaries or report files. You can of course also enter the commands found in the *Makefile* directly into the terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ma version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gpfs/home/scortinhal/CHPS0904/task/11_Exercise_MGPU\n",
      "nvcc -arch=sm_70 -O3 -Xcompiler=\"-march=native -fopenmp\" mgpu.cu -o mgpu\n",
      "nsys profile --stats=true --force-overwrite=true -o mgpu-report ./mgpu\n",
      "Nombre de GPUs disponibles : 2\n",
      "TIMING: 22.822 ms (Temps de d√©cryptage kernel sur GPUs)\n",
      "STATUS: test passed\n",
      "TIMING: 15909.4 ms (Temps total)\n",
      "Collecting data...\n",
      "Generating '/tmp/nsys-report-dc92.qdstrm'\n",
      "[1/8] [========================100%] mgpu-report.nsys-rep\n",
      "[2/8] [========================100%] mgpu-report.sqlite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SKIPPED: /gpfs/home/scortinhal/CHPS0904/task/11_Exercise_MGPU/kernel_streams_cipher/mgpu-report.sqlite does not contain NV Tools Extension (NVTX) data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)     Med (ns)    Min (ns)  Max (ns)   StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  -----------  -----------  --------  ---------  -----------  ----------------------\n",
      "     50.0      16477826688        184   89553405.9  100114208.0      1120  157531232   29777802.7  poll                  \n",
      "     48.3      15914324096         63  252608319.0    5261568.0     32320  774758496  302117346.2  futex                 \n",
      "      1.2        407368800       1408     289324.4      13600.0      1024   82185696    2487670.6  ioctl                 \n",
      "      0.5        151834400        129    1177010.9       5312.0      1024  151123296   13305175.3  open64                \n",
      "      0.0          6954368         18     386353.8     347168.0    340256     775712     105646.2  pthread_create        \n",
      "      0.0          6064736         67      90518.4      10784.0      2304    5256704     640745.3  mmap                  \n",
      "      0.0          1689728         20      84486.4      40288.0     16544     302880      85774.1  sem_timedwait         \n",
      "      0.0          1160576         92      12615.0      11232.0      5728      34880       4747.8  mmap64                \n",
      "      0.0           784672         82       9569.2       6352.0      2240      86048      11579.3  fopen                 \n",
      "      0.0           269120         47       5726.0       1568.0      1024     156192      22588.6  fclose                \n",
      "      0.0            91200         21       4342.9       4000.0      1568       8896       1928.5  write                 \n",
      "      0.0            89824          1      89824.0      89824.0     89824      89824          0.0  pthread_cond_wait     \n",
      "      0.0            80032          4      20008.0      16576.0      8448      38432      13313.9  fread                 \n",
      "      0.0            64160          5      12832.0       3712.0      2752      44128      17724.6  fgets                 \n",
      "      0.0            55648          8       6956.0       6240.0      1408      16160       5634.6  munmap                \n",
      "      0.0            54560          6       9093.3       1344.0      1152      44064      17215.1  fcntl                 \n",
      "      0.0            52800          9       5866.7       5536.0      1376      12736       3649.4  open                  \n",
      "      0.0            42752          4      10688.0      10288.0      8352      13824       2331.8  fflush                \n",
      "      0.0            29376          4       7344.0       5520.0      4768      13568       4164.6  fopen64               \n",
      "      0.0            28000         13       2153.8       1600.0      1024       6720       1648.8  read                  \n",
      "      0.0            20416          3       6805.3       7744.0      2112      10560       4301.5  pipe2                 \n",
      "      0.0            18816          2       9408.0       9408.0      4832      13984       6471.4  socket                \n",
      "      0.0            17760          6       2960.0       2016.0      1504       7936       2474.7  fwrite                \n",
      "      0.0            10976          1      10976.0      10976.0     10976      10976          0.0  connect               \n",
      "      0.0             3712          1       3712.0       3712.0      3712       3712          0.0  bind                  \n",
      "      0.0             2752          1       2752.0       2752.0      2752       2752          0.0  pthread_cond_broadcast\n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)  StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  --------  -----------  ----------------------\n",
      "     38.3         22845696          1  22845696.0  22845696.0  22845696  22845696          0.0  cudaHostAlloc         \n",
      "     32.7         19486560          2   9743280.0   9743280.0   3291136  16195424    9124709.6  cudaStreamSynchronize \n",
      "     18.1         10798144          1  10798144.0  10798144.0  10798144  10798144          0.0  cudaFreeHost          \n",
      "      5.4          3238016          2   1619008.0   1619008.0   1340672   1897344     393626.5  cudaLaunchKernel      \n",
      "      2.8          1684512          2    842256.0    842256.0    709568    974944     187649.2  cudaMalloc            \n",
      "      2.2          1340544          2    670272.0    670272.0    514816    825728     219848.0  cudaFree              \n",
      "      0.1            60320          4     15080.0     15248.0      5664     24160       8143.7  cudaEventRecord       \n",
      "      0.1            58176          2     29088.0     29088.0     26464     31712       3710.9  cudaStreamDestroy     \n",
      "      0.1            55264          4     13816.0      8128.0      6112     32896      12794.0  cudaMemcpyAsync       \n",
      "      0.1            49408          2     24704.0     24704.0     15680     33728      12761.9  cudaStreamCreate      \n",
      "      0.0            15264          4      3816.0      3040.0       608      8576       3875.9  cudaEventCreate       \n",
      "      0.0            12064          2      6032.0      6032.0      4896      7168       1606.5  cudaEventSynchronize  \n",
      "      0.0             5792          4      1448.0      1184.0       576      2848       1052.1  cudaEventDestroy      \n",
      "      0.0             2048          1      2048.0      2048.0      2048      2048          0.0  cuModuleGetLoadingMode\n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances   Avg (ns)    Med (ns)   Min (ns)  Max (ns)  StdDev (ns)                             Name                           \n",
      " --------  ---------------  ---------  ----------  ----------  --------  --------  -----------  ----------------------------------------------------------\n",
      "    100.0         32978208          2  16489104.0  16489104.0  16452128  16526080      52292.0  decrypt_gpu(unsigned long *, unsigned long, unsigned long)\n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)   Med (ns)   Min (ns)  Max (ns)  StdDev (ns)           Operation          \n",
      " --------  ---------------  -----  ---------  ---------  --------  --------  -----------  ----------------------------\n",
      "     50.9          4623552      2  2311776.0  2311776.0   1579264   3044288    1035928.4  [CUDA memcpy Device-to-Host]\n",
      "     49.1          4456800      2  2228400.0  2228400.0   1257568   3199232    1372963.8  [CUDA memcpy Host-to-Device]\n",
      "\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)           Operation          \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ----------------------------\n",
      "    536.871      2   268.435   268.435   268.435   268.435        0.000  [CUDA memcpy Device-to-Host]\n",
      "    536.871      2   268.435   268.435   268.435   268.435        0.000  [CUDA memcpy Host-to-Device]\n",
      "\n",
      "Generated:\n",
      "    /gpfs/home/scortinhal/CHPS0904/task/11_Exercise_MGPU/kernel_streams_cipher/mgpu-report.nsys-rep\n",
      "    /gpfs/home/scortinhal/CHPS0904/task/11_Exercise_MGPU/kernel_streams_cipher/mgpu-report.sqlite\n",
      "Nombre de GPUs disponibles : 2\n",
      "TIMING: 22.8362 ms (Temps de d√©cryptage kernel sur GPUs)\n",
      "STATUS: test passed\n",
      "TIMING: 14935.6 ms (Temps total)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Affiche le r√©pertoire courant pour v√©rifier le point de d√©part\n",
    "pwd\n",
    "\n",
    "cd kernel_streams_cipher\n",
    "\n",
    "module load cuda/12.6\n",
    "\n",
    "make profile\n",
    "\n",
    "./mgpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#65AE11;\">Exercise Solution</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you complete your work, or if you get stuck, refer to [the solution](mgpu_cipher/mgpu_solution.cu). If you wish, you can compile the solution with `make mgpu_solution`, and/or generate a report file for viewing in Nsight Systems with `make profile_solution`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#65AE11;\">Check for Understanding</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please answer the following to confirm you've learned the main objectives of this section. You can display the answers for each question by clicking on the \"...\" cells below the questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In the visual profiler, we can see that overlapping kernel execution. Why is this so?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "We are using multiple GPUs to execute chunks of the work required by our application, all of which can perform work at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In the visual profiler image of the solution code, above, we can see that there is no overlap of memory transfers. Why is this so?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "The solution code is using neither non-default streams, nor, `cudaMemcpyAsync` for memory copies. They are, therefore, blocking operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#65AE11;\">Next</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now know how to perform copy/compute overlap, and, how to perform work on multiple GPUs. In the next section you will learn about streams on multiple GPUs, and how to perform copy/compute overlap on multiple GPUs.\n",
    "\n",
    "Please continue to the next section: [*MGPU Streams*](../12_MGPU_Streams/MGPU_Streams.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
