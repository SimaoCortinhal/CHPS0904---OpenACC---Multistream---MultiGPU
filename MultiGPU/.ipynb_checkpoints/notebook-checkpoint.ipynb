{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aed246f7-aafd-4528-8b3a-b6e10fb70307",
   "metadata": {},
   "source": [
    "# **Multi gpu - CHPS0904 - Multi GPU Programming Models for HPC and AI**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635c214e-6069-4c50-b8bf-24d454430b37",
   "metadata": {},
   "source": [
    "# Partie 1 : Version CPU \n",
    "\n",
    "- Jacobi séquentiel sur CPU (pas de parallélisme, pas de MPI, pas de GPU)\n",
    "- Lecture des paramètres depuis la ligne de commande : nx, ny, max_iter, tol, affichage\n",
    "- Allocation dynamique des deux tableaux (a, a_new) pour stocker la grille\n",
    "- Initialisation des tableaux avec conditions aux bords : gauche/droite à 1, intérieur à 0\n",
    "- Boucle d'itération Jacobi utilisant une fonction dédiée (jacobi_step)\n",
    "- Application d'une condition périodique sur les bords haut/bas (apply_periodic_bc)\n",
    "- Calcul du maximum des différences (norme d’erreur) entre a et a_new pour tester la convergence\n",
    "- Échange des pointeurs a et a_new pour alterner à chaque itération sans recopie de données\n",
    "- Affichage optionnel de la grille à la fin, plus affichage des valeurs aux coins et au centre\n",
    "- Impression du nombre d’itérations, de l’erreur finale, du temps d’exécution total (avec omp_get_wtime, mais sans parallélisme)\n",
    "- Découpage en fonctions indépendantes pour chaque étape (initialisation, étape Jacobi, conditions bords, affichage, etc.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb254c3f-4144-4ebe-abd0-53571fc4649a",
   "metadata": {},
   "source": [
    "## Compilation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8d73b78a-a6a4-4a7f-a0d6-72b6c0c3f833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f jacobi_seq\n",
      "gcc -O3 -Wall -march=native -funroll-loops -ffast-math -std=c99 -fopenmp -o jacobi_seq jacobi.c laplace2d.c -lm\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd CPU\n",
    "make clean \n",
    "make\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd62542-88ff-412e-a777-1a3b3eb61d7f",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7411ab7c-f1d9-451f-a5ec-24e3b69d2b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 1.000000 \n",
      "1.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 1.000000 \n",
      "1.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 1.000000 \n",
      "1.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 1.000000 \n",
      "1.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 1.000000 \n",
      "1.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 1.000000 \n",
      "1.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 1.000000 \n",
      "1.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 1.000000 \n",
      "1.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 1.000000 \n",
      "1.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 1.000000 \n",
      "1.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 1.000000 \n",
      "1.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 1.000000 \n",
      "1.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 1.000000 \n",
      "1.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 1.000000 \n",
      "1.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 1.000000 \n",
      "1.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 1.000000 \n",
      "1.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 1.000000 \n",
      "1.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 1.000000 \n",
      "1.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 1.000000 \n",
      "1.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 1.000000 \n",
      "\n",
      "1.000000 0.663624 0.383310 0.189247 0.078354 0.026604 0.007197 0.001490 0.000221 0.000022 0.000022 0.000221 0.001490 0.007197 0.026604 0.078354 0.189247 0.383310 0.663624 1.000000 \n",
      "1.000000 0.663624 0.383310 0.189247 0.078354 0.026604 0.007197 0.001490 0.000221 0.000022 0.000022 0.000221 0.001490 0.007197 0.026604 0.078354 0.189247 0.383310 0.663624 1.000000 \n",
      "1.000000 0.663624 0.383310 0.189247 0.078354 0.026604 0.007197 0.001490 0.000221 0.000022 0.000022 0.000221 0.001490 0.007197 0.026604 0.078354 0.189247 0.383310 0.663624 1.000000 \n",
      "1.000000 0.663624 0.383310 0.189247 0.078354 0.026604 0.007197 0.001490 0.000221 0.000022 0.000022 0.000221 0.001490 0.007197 0.026604 0.078354 0.189247 0.383310 0.663624 1.000000 \n",
      "1.000000 0.663624 0.383310 0.189247 0.078354 0.026604 0.007197 0.001490 0.000221 0.000022 0.000022 0.000221 0.001490 0.007197 0.026604 0.078354 0.189247 0.383310 0.663624 1.000000 \n",
      "1.000000 0.663624 0.383310 0.189247 0.078354 0.026604 0.007197 0.001490 0.000221 0.000022 0.000022 0.000221 0.001490 0.007197 0.026604 0.078354 0.189247 0.383310 0.663624 1.000000 \n",
      "1.000000 0.663624 0.383310 0.189247 0.078354 0.026604 0.007197 0.001490 0.000221 0.000022 0.000022 0.000221 0.001490 0.007197 0.026604 0.078354 0.189247 0.383310 0.663624 1.000000 \n",
      "1.000000 0.663624 0.383310 0.189247 0.078354 0.026604 0.007197 0.001490 0.000221 0.000022 0.000022 0.000221 0.001490 0.007197 0.026604 0.078354 0.189247 0.383310 0.663624 1.000000 \n",
      "1.000000 0.663624 0.383310 0.189247 0.078354 0.026604 0.007197 0.001490 0.000221 0.000022 0.000022 0.000221 0.001490 0.007197 0.026604 0.078354 0.189247 0.383310 0.663624 1.000000 \n",
      "1.000000 0.663624 0.383310 0.189247 0.078354 0.026604 0.007197 0.001490 0.000221 0.000022 0.000022 0.000221 0.001490 0.007197 0.026604 0.078354 0.189247 0.383310 0.663624 1.000000 \n",
      "1.000000 0.663624 0.383310 0.189247 0.078354 0.026604 0.007197 0.001490 0.000221 0.000022 0.000022 0.000221 0.001490 0.007197 0.026604 0.078354 0.189247 0.383310 0.663624 1.000000 \n",
      "1.000000 0.663624 0.383310 0.189247 0.078354 0.026604 0.007197 0.001490 0.000221 0.000022 0.000022 0.000221 0.001490 0.007197 0.026604 0.078354 0.189247 0.383310 0.663624 1.000000 \n",
      "1.000000 0.663624 0.383310 0.189247 0.078354 0.026604 0.007197 0.001490 0.000221 0.000022 0.000022 0.000221 0.001490 0.007197 0.026604 0.078354 0.189247 0.383310 0.663624 1.000000 \n",
      "1.000000 0.663624 0.383310 0.189247 0.078354 0.026604 0.007197 0.001490 0.000221 0.000022 0.000022 0.000221 0.001490 0.007197 0.026604 0.078354 0.189247 0.383310 0.663624 1.000000 \n",
      "1.000000 0.663624 0.383310 0.189247 0.078354 0.026604 0.007197 0.001490 0.000221 0.000022 0.000022 0.000221 0.001490 0.007197 0.026604 0.078354 0.189247 0.383310 0.663624 1.000000 \n",
      "1.000000 0.663624 0.383310 0.189247 0.078354 0.026604 0.007197 0.001490 0.000221 0.000022 0.000022 0.000221 0.001490 0.007197 0.026604 0.078354 0.189247 0.383310 0.663624 1.000000 \n",
      "1.000000 0.663624 0.383310 0.189247 0.078354 0.026604 0.007197 0.001490 0.000221 0.000022 0.000022 0.000221 0.001490 0.007197 0.026604 0.078354 0.189247 0.383310 0.663624 1.000000 \n",
      "1.000000 0.663624 0.383310 0.189247 0.078354 0.026604 0.007197 0.001490 0.000221 0.000022 0.000022 0.000221 0.001490 0.007197 0.026604 0.078354 0.189247 0.383310 0.663624 1.000000 \n",
      "1.000000 0.663624 0.383310 0.189247 0.078354 0.026604 0.007197 0.001490 0.000221 0.000022 0.000022 0.000221 0.001490 0.007197 0.026604 0.078354 0.189247 0.383310 0.663624 1.000000 \n",
      "1.000000 0.663624 0.383310 0.189247 0.078354 0.026604 0.007197 0.001490 0.000221 0.000022 0.000022 0.000221 0.001490 0.007197 0.026604 0.078354 0.189247 0.383310 0.663624 1.000000 \n",
      "\n",
      "Jacobi solver stopped after 10 iterations\n",
      "Reached max_iter = 10 (error = 2.402687e-02 > tol = 1.000000e-06)\n",
      "Elapsed time   : 0.000004 seconds\n",
      "Values at corners: [0,0]=1.00  [0,19]=1.00\n",
      "Value at center [10,10] = 0.000022\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "./CPU/jacobi_seq 4096 4096 1000 1e-6 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412d80f5-0b3b-4ad8-8786-3e9275540c79",
   "metadata": {},
   "source": [
    "# Partie 2 : Version CPU + GPU "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672acc20-64eb-4817-beb2-695c4d586616",
   "metadata": {},
   "source": [
    "- Ajout du GPU avec CUDA : calculs réalisés sur le device, et non plus sur le CPU.\n",
    "- Utilisation de kernels CUDA pour :\n",
    "    - Effectuer Jacobi sur le tableau (jacobi_step).\n",
    "    - Appliquer les conditions aux limites périodiques en Y (apply_periodic_bc).\n",
    "- Allocation mémoire séparée sur le CPU (host) et le GPU (device).\n",
    "- Copie initiale des données du host vers le device avant le calcul.\n",
    "- Boucle principale :\n",
    "    - Lancement du kernel Jacobi sur la grille (d_a_new ← d_a).\n",
    "    - Lancement du kernel d’application des bords périodiques en Y.\n",
    "    - Calcul de l’erreur de convergence directement sur le GPU avec **Thrust** :  \n",
    "      Thrust est une bibliothèque C++ intégrée à CUDA qui fournit des primitives algorithmiques (comme les réductions, tris, scans) optimisées pour le GPU. Ici, j'utilise `thrust::transform_reduce` pour parcourir les tableaux sur le GPU et calculer en parallèle l’erreur maximale entre deux itérations, sans repasser les données sur le CPU.\n",
    "    - Inversion des pointeurs device (std::swap) pour éviter une recopie à chaque itération.\n",
    "- Mesure du temps CUDA (calcul pur) via cudaEvent_t, en plus du temps global CPU.\n",
    "- Récupération des résultats finaux du device vers le host pour affichage.\n",
    "- Affichage du temps CUDA, du temps global, du nombre d’itérations, de l’erreur finale, et de l’état final de la grille si demandé.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b4be28-3f6d-49e0-ba53-34bc0deec2a0",
   "metadata": {},
   "source": [
    "## Compilation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6434f52-319d-4e3d-a257-5dfb4358e844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc -O3 -arch=sm_60 -std=c++11        -o jacobi_cuda jacobi.cu -lm                             \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd CPU-GPU\n",
    "make"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f571c66-da19-4ecb-b4be-f79f9775e3ab",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5248b517-2afd-45e4-ab7c-bd146d5c7a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solveur Jacobi CUDA convergé en 1000 itérations (erreur = 2.419e-04)\n",
      "Temps passé (calcul CUDA) : 0.238161 s\n",
      "Temps total du programme (tout inclus) : 2.551201 s\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "./CPU-GPU/jacobi_cuda 4096 4096 1000 1e-6 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4559e5f-3c1c-4c2b-bfc7-09eae2c6aedb",
   "metadata": {},
   "source": [
    "# Partie 3 : MPI + GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af60e48c-8e5a-420d-8ee3-8d8d6102ba7c",
   "metadata": {},
   "source": [
    "- Ajout de la **parallelisation distribuée** avec **MPI** : le domaine 2D est découpé entre plusieurs processus MPI, chacun pouvant s’exécuter sur un GPU distinct.\n",
    "- Allocation mémoire device par sous-domaine local (chaque rang alloue uniquement sa bande locale + halos).\n",
    "- Chaque processus traite une bande de la grille ; gestion fine des indices pour découpage non uniforme si nécessaire.\n",
    "- Ajout d’une communication explicite entre processus MPI à chaque itération pour échanger les **halos** (les halos sont les bandes de points supplémentaires ajoutées au bord de chaque sous-domaine, servant à stocker temporairement les valeurs des points voisins d’un autre sous-domaine ; ils permettent à chaque processus de calculer ses points de frontière sans accès direct à toute la grille globale) via `MPI_Sendrecv`.\n",
    "- Les échanges de halos utilisent les pointeurs device CUDA : le code est compatible MPI CUDA-aware, c’est-à-dire que les transferts MPI sont réalisés directement depuis/vers la mémoire GPU.\n",
    "- Synchronisation inter-rangs avant/après le calcul (MPI_Barrier) pour bien délimiter les phases de calcul scientifique dans les mesures de temps.\n",
    "- Réduction MPI (`MPI_Allreduce`) pour calculer l’erreur globale de convergence sur l’ensemble des sous-domaines (chaque GPU calcule l’erreur locale avec Thrust, puis réduction MPI).\n",
    "- Regroupement et affichage des résultats sur le rang 0 via des communications MPI pour rassembler toute la grille.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28376d3-241c-43b1-ac81-64c993bb6281",
   "metadata": {},
   "source": [
    "## Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34e00f77-d58b-493d-8904-fe87c7ccec7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir -p nsight\n",
      "# Adapte la ligne ci-dessous selon les arguments à donner à ton programme\n",
      "nsys profile -o nsight/jacobi_cuda_profile \\\n",
      "\tmpirun -np 4 ./jacobi_cuda 4096 4096 10000 1e-6 0\n",
      "Solveur Jacobi MPI+multiGPU convergé en 10000 itérations (erreur = 2.420e-05)\n",
      "Temps calcul MPI+multiGPU (hors init): 10.474947 s\n",
      "Temps total du programme (allocs, MPI, init, calcul, etc.): 12.065351 s\n",
      "Collecting data...\n",
      "Generating '/tmp/nsys-report-35d9.qdstrm'\n",
      "[1/1] [========================100%] jacobi_cuda_profile.nsys-rep\n",
      "Generated:\n",
      "    /gpfs/home/scortinhal/CHPS0904/MultiGPU/MPI-GPU/nsight/jacobi_cuda_profile.nsys-rep\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd MPI-GPU\n",
    "make nsight\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "857c76fc-5659-4962-a0fa-1b8f4c3e17a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solveur Jacobi MPI+multiGPU convergé en 1000 itérations (erreur = 2.422e-04)\n",
      "Temps calcul MPI+multiGPU (hors init): 1.312076 s\n",
      "Temps total du programme (allocs, MPI, init, calcul, etc.): 2.197987 s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd MPI-GPU\n",
    "\n",
    "mpirun -np 4 ./jacobi_cuda 4096 4096 1000 1e-6 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3760f8-4fa9-4239-9a0e-db4ce6f389c1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93b5e5d9-e106-444f-9e9d-b792fd9d445c",
   "metadata": {},
   "source": [
    "# Partie 4 : MPI + Overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772c3301-4f6c-4226-bf97-0727543532bc",
   "metadata": {},
   "source": [
    "- Modif pour  **asynchrone et overlap** entre les communications MPI (transferts des halos) et les calculs sur le GPU.\n",
    "- Utilisation de **plusieurs streams CUDA** :\n",
    "  - Un stream pour chaque bande (top, bottom, intérieur) afin de lancer simultanément calculs et transferts.\n",
    "  - Un stream dédié aux copies mémoire host/device (cudaMemcpyAsync) pour les halos.\n",
    "- Implémentation du **schéma overlap** :\n",
    "  - Les halos (zones tampon en haut et en bas du sous-domaine, servant à stocker les valeurs échangées avec les voisins MPI et nécessaires pour calculer les points en bordure) sont d’abord transférés du GPU vers le CPU de façon asynchrone.\n",
    "  - Pendant que les échanges MPI asynchrones (MPI_Isend/Irecv) des halos sont en cours, le calcul sur la bande intérieure (qui ne dépend pas des halos) est lancé.\n",
    "  - Dès que les halos sont reçus, ils sont recopiés du CPU vers le GPU (toujours en asynchrone) ; on attend la fin de ce transfert avant de calculer les bandes du haut et du bas, dépendantes des halos.\n",
    "- Synchronisation sur les streams CUDA pour garantir la cohérence des calculs (on synchronise seulement ce qui est nécessaire, pas tout le GPU à chaque étape).\n",
    "- Attente asynchrone des communications MPI pour maximiser l’overlap (on lance la réduction Thrust/Allreduce seulement quand tout est fini).\n",
    "- La convergence reste mesurée avec **Thrust** (comme avant : `transform_reduce` calcule directement sur le GPU le maximum des différences entre deux itérations, sans repasser par le CPU).\n",
    "- Ce schéma permet d’**overlap** le calcul et la communication\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df5e3100-2c30-4051-a06e-22f1a7c1d9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir -p nsight\n",
      "# Adapte la ligne ci-dessous selon les arguments à donner à ton programme\n",
      "nsys profile -o nsight/jacobi_cuda_profile \\\n",
      "\tmpirun -np 4 ./jacobi_cuda 4096 4096 10000 1e-6 0\n",
      "Solveur Jacobi MPI+multiGPU (full overlap memcpyasync) convergé en 10000 itérations (error = 2.420e-05)\n",
      "Temps calcul MPI+multiGPU (hors alloc/init): 10.920290 s\n",
      "Temps total du programme (tout inclus): 12.466443 s\n",
      "Collecting data...\n",
      "Generating '/tmp/nsys-report-b766.qdstrm'\n",
      "[1/1] [========================100%] jacobi_cuda_profile.nsys-rep\n",
      "Generated:\n",
      "    /gpfs/home/scortinhal/CHPS0904/MultiGPU/MPI-GPU-overlap/nsight/jacobi_cuda_profile.nsys-rep\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd MPI-GPU-overlap\n",
    "make nsight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9438358-adbd-4f00-a3d1-bffbaeb305d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solveur Jacobi MPI+multiGPU (full overlap memcpyasync) convergé en 1000 itérations (error = 2.422e-04)\n",
      "Temps calcul MPI+multiGPU (hors alloc/init): 1.110611 s\n",
      "Temps total du programme (tout inclus): 1.979046 s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd MPI-GPU-overlap\n",
    "mpirun -np 4 ./jacobi_cuda 4096 4096 1000 1e-6 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ecc13c-f926-4dbe-9b3e-b79a668fc751",
   "metadata": {},
   "source": [
    "# Partie 5 : NCCL\n",
    "\n",
    "\n",
    "- Utilisation de **NCCL** (NVIDIA Collective Communication Library) pour les communications entre GPUs, à la place de MPI pur ou CUDA-aware MPI.\n",
    "    - Les échanges de halos (zones tampon haut/bas entre sous-domaines voisins) se font ici directement entre GPUs via `ncclSend` et `ncclRecv`, de façon totalement device-to-device, sans passer par la RAM du CPU.\n",
    "    - Le schéma d’échange est `ncclGroupStart/ncclGroupEnd` pour lancer plusieurs envois/réceptions en une seule opération collective, ce qui améliore le débit.\n",
    "- Initialisation de NCCL : un communicateur NCCL est créé pour permettre les échanges collectifs entre tous les GPUs.\n",
    "- Synchronisation sur un **stream CUDA** après les échanges de halos NCCL, pour garantir que les données sont prêtes avant le calcul.\n",
    "- Le calcul local du Jacobi reste inchangé : il est fait sur le GPU via un kernel CUDA.\n",
    "- La mesure de la convergence utilise toujours **Thrust** pour faire le calcul de la norme maximale sur GPU.\n",
    "- La réduction finale de l’erreur (convergence globale) se fait par **MPI_Allreduce** : NCCL n’est utilisé que pour les échanges de halos, la convergence reste synchronisée au niveau MPI.\n",
    "- Les chronos sont placés pour mesurer :\n",
    "    - Le temps global (du début à la fin du programme).\n",
    "    - Le temps après initialisation de NCCL.\n",
    "    - Le temps pur de la boucle Jacobi.\n",
    "- L'affichage/réassemblage final des résultats reste identique aux versions précédentes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddff1e20-d7f2-46eb-ad60-8437da34ecda",
   "metadata": {},
   "source": [
    "## Compilation et execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4388d1e5-0243-428b-832f-cd06a8b61460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make: Nothing to be done for 'all'.\n",
      "Solveur Jacobi NCCL convergé en 1000 itérations (error = 2.422e-04)\n",
      "1. Temps total du programme (MPI_Init → fin)                : 6.495399 s\n",
      "2. Temps après init NCCL (juste avant alloc/init CUDA)      : 1.747198 s\n",
      "3. Temps NCCL (calcul pur boucle Jacobi)                    : 1.744446 s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd NCCL\n",
    "\n",
    "eval $(spack load --sh nccl)\n",
    "export LD_LIBRARY_PATH=$(spack location -i nccl)/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "make\n",
    "mpirun -np 4 ./jacobi_nccl 4096 4096 1000 1e-6 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5bd7b3-2412-4543-af79-8133db1e5c65",
   "metadata": {},
   "source": [
    "# Partie 6 : NCCL - Overlap\n",
    "\n",
    "- Utilisation de **NCCL** pour l’échange des halos entre GPUs, comme dans la version précédente, mais :\n",
    "    - On applique un **overlap** entre communications et calcul :\n",
    "        - Les échanges de halos (lignes du haut et du bas de chaque sous-domaine, nécessaires pour la continuité de la solution) se font via `ncclSend`/`ncclRecv` sur deux streams CUDA distincts (`stream_halo_top`, `stream_halo_bot`).\n",
    "        - Pendant que les transferts NCCL s’exécutent, on lance immédiatement le calcul Jacobi :\n",
    "            - Sur les bandes extrêmes du sous-domaine (`iy = 1` pour le haut, `iy = local_ny` pour le bas) sur les mêmes streams que les communications associées.\n",
    "            - Sur l’intérieur du domaine (lignes qui ne dépendent pas des halos) sur un troisième stream CUDA (`stream_interior`).\n",
    "    - On effectue une synchronisation (`cudaStreamSynchronize`) sur chaque stream avant de poursuivre, pour garantir que calculs et transferts sont bien terminés.\n",
    "- Ce **recouvrement comm/calcul** (overlap) permet de gagner du temps si le coût des transferts et du calcul sont similaires ou si l’un des deux prend plus de temps que l’autre.\n",
    "- Le découpage en bandes (slice) pour les kernels Jacobi permet ce parallélisme : on peut commencer à travailler sur ce qui est prêt sans attendre toute la communication.\n",
    "- Calcul de la convergence toujours via Thrust sur GPU, puis synchronisation globale via `MPI_Allreduce`.\n",
    "- Affichage et réassemblage final identiques aux versions précédentes.\n",
    "- Multiples chronomètres pour séparer le temps total, le temps d’allocation/init CUDA/NCCL, et le temps pur de la boucle Jacobi avec overlap.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5c553686-9caa-4982-bcf9-ac4a1e2182d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make: Nothing to be done for 'all'.\n",
      "NCCL Comm Init: 1.053 s\n",
      "Solveur Jacobi NCCL + overlap convergé en 1000 itérations (error = 2.422e-04)\n",
      "1. Temps total du programme (MPI_Init → fin)                 : 2.094690 s\n",
      "2. Temps après init NCCL (juste avant alloc/init CUDA)       : 0.380852 s\n",
      "3. Temps calcul NCCL + overlap (boucle Jacobi uniquement)    : 0.379018 s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd NCCL-overlap\n",
    "\n",
    "eval $(spack load --sh nccl)\n",
    "export LD_LIBRARY_PATH=$(spack location -i nccl)/lib:$LD_LIBRARY_PATH\n",
    "export NCCL_IB_HCA=mlx5_0         # si tu as Infiniband\n",
    "export NCCL_IB_DISABLE=0\n",
    "export NCCL_P2P_DISABLE=1\n",
    "make\n",
    "mpirun -np 1 ./jacobi_nccl 4096 4096 1000 1e-6 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51849b41-74e4-401a-a2d8-64867697436b",
   "metadata": {},
   "source": [
    "# Partie 7 : NCCL - graph\n",
    "\n",
    "- Ajout de l'utilisation des **CUDA Graphs** pour la capture et l'exécution de la boucle Jacobi :\n",
    "    - On capture la séquence \"comm/halo + calcul Jacobi (bande intérieures et extrêmes) + swap\" en graph CUDA, avec deux variantes :\n",
    "        - un graph sans calcul d’erreur,\n",
    "        - un graph qui inclut le calcul d’erreur locale toutes les 10 itérations (ou à une fréquence réglable).\n",
    "    - Cela permet d’**accélérer les itérations** (moins d’overhead de lancement de kernels, moins de passages CPU/GPU, ordonnancement optimisé par CUDA).\n",
    "- L'attribution du GPU à chaque processus MPI se fait proprement via le \"local_rank\" obtenu à partir du communicator partagé MPI (pour que deux processus MPI du même nœud ne prennent pas le même GPU).\n",
    "- Toujours un découpage en sous-domaines horizontaux équilibré même si ny-2 n'est pas multiple du nombre de processus.\n",
    "- Toujours overlap entre communications (NCCL) et calcul (multi-streams CUDA), mais cette logique est incluse à l’intérieur du CUDA Graph.\n",
    "- Nettoyage mémoire : destruction des graphs et exécuteurs (`cudaGraphExecDestroy`) à la fin.\n",
    "- Pour le reste : gestion des halos (lignes fantômes haut/bas pour l’échange entre voisins), calcul local Jacobi en slice, calcul de l’erreur via Thrust puis réduction MPI pour la convergence globale, impression et réassemblage final.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35845d0f-801b-4e8f-a94b-8d330fe19d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc -O3 -arch=sm_60 -std=c++11 -ccbin=mpicxx -use_fast_math -I/home/scortinhal/.spack/userspace-installed/linux-rhel9-neoverse_v2/linux-rhel9-neoverse_v2/gcc-11.4.1/nccl-2.22.3-1-6mbbenokgc3egfcu6fgqxjwi7ea5oqc6/include -o jacobi_nccl jacobi.cu -L/home/scortinhal/.spack/userspace-installed/linux-rhel9-neoverse_v2/linux-rhel9-neoverse_v2/gcc-11.4.1/nccl-2.22.3-1-6mbbenokgc3egfcu6fgqxjwi7ea5oqc6/lib -lm -lmpi -lnccl -lcudart\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "jacobi.cu(141): warning #177-D: variable \"d_old\" was declared but never referenced\n",
      "      double *d_error, *d_old, *d_new;\n",
      "                        ^\n",
      "\n",
      "Remark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
      "\n",
      "jacobi.cu(141): warning #177-D: variable \"d_new\" was declared but never referenced\n",
      "      double *d_error, *d_old, *d_new;\n",
      "                                ^\n",
      "\n",
      "terminate called after throwing an instance of 'thrust::THRUST_200500_600_NS::system::system_error'\n",
      "  what():  terminate called after throwing an instance of 'thrust::THRUST_200500_600_NS::system::system_error'\n",
      "  what():  after reduction step 1: cudaErrorInvalidDevice: invalid device ordinal\n",
      "terminate called after throwing an instance of 'thrust::THRUST_200500_600_NS::system::system_error'\n",
      "  what():  after reduction step 1: cudaErrorInvalidDevice: invalid device ordinal\n",
      "[romeo-a045:3965139] *** Process received signal ***\n",
      "[romeo-a045:3965139] Signal: Aborted (6)\n",
      "[romeo-a045:3965139] Signal code:  (-6)\n",
      "terminate called after throwing an instance of 'thrust::THRUST_200500_600_NS::system::system_error'\n",
      "  what():  after reduction step 1: cudaErrorInvalidDevice: invalid device ordinal\n",
      "[romeo-a045:3965140] *** Process received signal ***\n",
      "[romeo-a045:3965140] Signal: Aborted (6)\n",
      "[romeo-a045:3965140] Signal code:  (-6)\n",
      "after reduction step 1: cudaErrorInvalidDevice: invalid device ordinal\n",
      "[romeo-a045:3965137] *** Process received signal ***\n",
      "[romeo-a045:3965137] Signal: Aborted (6)\n",
      "[romeo-a045:3965137] Signal code:  (-6)\n",
      "[romeo-a045:3965138] *** Process received signal ***\n",
      "[romeo-a045:3965138] Signal: Aborted (6)\n",
      "[romeo-a045:3965138] Signal code:  (-6)\n",
      "[romeo-a045:3965139] [ 0] linux-vdso.so.1(__kernel_rt_sigreturn+0x0)[0x40002c8b07f0]\n",
      "[romeo-a045:3965139] [ 1] [romeo-a045:3965140] [ 0] linux-vdso.so.1(__kernel_rt_sigreturn+0x0)[0x4000202c07f0]\n",
      "[romeo-a045:3965140] [ 1] [romeo-a045:3965137] [ 0] linux-vdso.so.1(__kernel_rt_sigreturn+0x0)[0x40001d5807f0]\n",
      "[romeo-a045:3965137] [ 1] [romeo-a045:3965138] [ 0] linux-vdso.so.1(__kernel_rt_sigreturn+0x0)[0x40002b5707f0]\n",
      "[romeo-a045:3965138] [ 1] /lib64/libc.so.6(+0x822e8)[0x400030a422e8]\n",
      "[romeo-a045:3965138] [ 2] /lib64/libc.so.6(+0x822e8)[0x400022a522e8]\n",
      "[romeo-a045:3965137] [ 2] /lib64/libc.so.6(+0x822e8)[0x400031d822e8]\n",
      "[romeo-a045:3965139] [ 2] /lib64/libc.so.6(+0x822e8)[0x4000257922e8]\n",
      "[romeo-a045:3965140] [ 2] /lib64/libc.so.6(raise+0x1c)[0x4000309fa73c]\n",
      "[romeo-a045:3965138] [ 3] /lib64/libc.so.6(raise+0x1c)[0x400022a0a73c]\n",
      "[romeo-a045:3965137] [ 3] /lib64/libc.so.6(raise+0x1c)[0x400031d3a73c]\n",
      "[romeo-a045:3965139] [ 3] /lib64/libc.so.6(raise+0x1c)[0x40002574a73c]\n",
      "[romeo-a045:3965140] [ 3] /lib64/libc.so.6(abort+0xe8)[0x400031d27034]\n",
      "[romeo-a045:3965139] [ 4] /lib64/libc.so.6(abort+0xe8)[0x4000309e7034]\n",
      "[romeo-a045:3965138] [ 4] /lib64/libc.so.6(abort+0xe8)[0x4000229f7034]\n",
      "[romeo-a045:3965137] [ 4] /lib64/libc.so.6(abort+0xe8)[0x400025737034]\n",
      "[romeo-a045:3965140] [ 4] /lib64/libstdc++.so.6(_ZN9__gnu_cxx27__verbose_terminate_handlerEv+0x1a0)[0x400030734c60]\n",
      "[romeo-a045:3965138] [ 5] /lib64/libstdc++.so.6(_ZN9__gnu_cxx27__verbose_terminate_handlerEv+0x1a0)[0x400025484c60]\n",
      "[romeo-a045:3965140] [ 5] /lib64/libstdc++.so.6(_ZN9__gnu_cxx27__verbose_terminate_handlerEv+0x1a0)[0x400022744c60]\n",
      "[romeo-a045:3965137] [ 5] /lib64/libstdc++.so.6(_ZN9__gnu_cxx27__verbose_terminate_handlerEv+0x1a0)[0x400031a74c60]\n",
      "[romeo-a045:3965139] [ 5] /lib64/libstdc++.so.6(+0xa25bc)[0x4000307325bc]\n",
      "[romeo-a045:3965138] [ 6] /lib64/libstdc++.so.6(+0xa25bc)[0x4000227425bc]\n",
      "[romeo-a045:3965137] [ 6] /lib64/libstdc++.so.6(+0xa25bc)[0x400031a725bc]\n",
      "[romeo-a045:3965139] [ 6] /lib64/libstdc++.so.6(+0xa25bc)[0x4000254825bc]\n",
      "[romeo-a045:3965140] [ 6] /lib64/libstdc++.so.6(+0xa2620)[0x400030732620]\n",
      "[romeo-a045:3965138] [ 7] /lib64/libstdc++.so.6(+0xa2620)[0x400022742620]\n",
      "[romeo-a045:3965137] [ 7] /lib64/libstdc++.so.6(+0xa2620)[0x400025482620]\n",
      "[romeo-a045:3965140] [ 7] /lib64/libstdc++.so.6(+0xa2620)[0x400031a72620]\n",
      "[romeo-a045:3965139] [ 7] /lib64/libstdc++.so.6(+0xa2904)[0x400022742904]\n",
      "[romeo-a045:3965137] [ 8] ./jacobi_nccl[0x403650]\n",
      "[romeo-a045:3965137] [ 9] /lib64/libstdc++.so.6(+0xa2904)[0x400030732904]\n",
      "[romeo-a045:3965138] [ 8] ./jacobi_nccl[0x403650]\n",
      "[romeo-a045:3965138] [ 9] /lib64/libstdc++.so.6(+0xa2904)[0x400025482904]\n",
      "[romeo-a045:3965140] [ 8] ./jacobi_nccl[0x403650]\n",
      "[romeo-a045:3965140] [ 9] /lib64/libstdc++.so.6(+0xa2904)[0x400031a72904]\n",
      "[romeo-a045:3965139] [ 8] ./jacobi_nccl[0x403650]\n",
      "[romeo-a045:3965139] [ 9] /lib64/libc.so.6(+0x27300)[0x4000229f7300]\n",
      "[romeo-a045:3965137] [10] /lib64/libc.so.6(__libc_start_main+0x98)[0x4000229f73d8]\n",
      "[romeo-a045:3965137] [11] ./jacobi_nccl[0x403d30]\n",
      "/lib64/libc.so.6(+0x27300)[0x4000309e7300]\n",
      "[romeo-a045:3965138] [10] /lib64/libc.so.6(__libc_start_main+0x98)[0x4000309e73d8]\n",
      "[romeo-a045:3965138] [11] ./jacobi_nccl[0x403d30]\n",
      "[romeo-a045:3965138] *** End of error message ***\n",
      "[romeo-a045:3965137] *** End of error message ***\n",
      "/lib64/libc.so.6(+0x27300)[0x400025737300]\n",
      "[romeo-a045:3965140] [10] /lib64/libc.so.6(+0x27300)[0x400031d27300]\n",
      "[romeo-a045:3965139] [10] /lib64/libc.so.6(__libc_start_main+0x98)[0x4000257373d8]\n",
      "[romeo-a045:3965140] [11] ./jacobi_nccl[0x403d30]\n",
      "[romeo-a045:3965140] *** End of error message ***\n",
      "/lib64/libc.so.6(__libc_start_main+0x98)[0x400031d273d8]\n",
      "[romeo-a045:3965139] [11] ./jacobi_nccl[0x403d30]\n",
      "[romeo-a045:3965139] *** End of error message ***\n",
      "--------------------------------------------------------------------------\n",
      "Primary job  terminated normally, but 1 process returned\n",
      "a non-zero exit code. Per user-direction, the job has been aborted.\n",
      "--------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------\n",
      "mpirun noticed that process rank 0 with PID 0 on node romeo-a045 exited on signal 6 (Aborted).\n",
      "--------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'cd NCCL-graph\\n\\neval $(spack load --sh nccl)\\nexport LD_LIBRARY_PATH=$(spack location -i nccl)/lib:$LD_LIBRARY_PATH\\n\\nmake\\nmpirun -np 4 ./jacobi_nccl 20 20 1000 1e-6 1\\n'' returned non-zero exit status 134.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbash\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcd NCCL-graph\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43meval $(spack load --sh nccl)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mexport LD_LIBRARY_PATH=$(spack location -i nccl)/lib:$LD_LIBRARY_PATH\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmake\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmpirun -np 4 ./jacobi_nccl 20 20 1000 1e-6 1\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/IPython/core/interactiveshell.py:2517\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2515\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2516\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2517\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2519\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2520\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2521\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/IPython/core/magics/script.py:154\u001b[0m, in \u001b[0;36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m     line \u001b[38;5;241m=\u001b[39m script\n\u001b[0;32m--> 154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshebang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/IPython/core/magics/script.py:314\u001b[0m, in \u001b[0;36mScriptMagics.shebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mraise_error \u001b[38;5;129;01mand\u001b[39;00m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;66;03m# If we get here and p.returncode is still None, we must have\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;66;03m# killed it but not yet seen its return code. We don't wait for it,\u001b[39;00m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;66;03m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001b[39;00m\n\u001b[1;32m    313\u001b[0m     rc \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m9\u001b[39m\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(rc, cell)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'cd NCCL-graph\\n\\neval $(spack load --sh nccl)\\nexport LD_LIBRARY_PATH=$(spack location -i nccl)/lib:$LD_LIBRARY_PATH\\n\\nmake\\nmpirun -np 4 ./jacobi_nccl 20 20 1000 1e-6 1\\n'' returned non-zero exit status 134."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd NCCL-graph\n",
    "\n",
    "eval $(spack load --sh nccl)\n",
    "export LD_LIBRARY_PATH=$(spack location -i nccl)/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "make\n",
    "mpirun -np 4 ./jacobi_nccl 20 20 1000 1e-6 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554f7281-d79c-4257-84ce-3567d78a22a1",
   "metadata": {},
   "source": [
    "# Partie 8 : NVSHMEM\n",
    "\n",
    "- Introduction de **NVSHMEM** comme méthode de communication pour les échanges de halos entre GPUs, en remplacement de NCCL ou MPI direct :\n",
    "    - NVSHMEM permet le \"Remote Memory Access\" (RMA) : chaque GPU peut écrire directement dans la mémoire d’un voisin (put), sans impliquer le CPU, ce qui réduit la latence et le coût des échanges de halos.\n",
    "    - Utilisation de `nvshmem_double_put` pour écrire directement la première/dernière ligne réelle dans le halo du voisin.\n",
    "    - Utilisation de `nvshmem_fence()` pour garantir la visibilité des échanges mémoire, et `nvshmem_barrier_all()` pour synchroniser tous les processus avant de lancer le calcul.\n",
    "- Initialisation avancée :\n",
    "    - Calcul et mise à disposition de la taille réelle des buffers (ghost_ny) sur chaque PE pour gérer les halos même si la décomposition est déséquilibrée (i.e., tous les processus n'ont pas le même nombre de lignes).\n",
    "    - Allocation des tableaux via `nvshmem_malloc` (heap symétrique, identique sur tous les PE).\n",
    "    - Calcul et configuration dynamique de la taille du heap symétrique avec la variable d’environnement `NVSHMEM_SYMMETRIC_SIZE`.\n",
    "    - Initialisation de NVSHMEM avec le communicateur MPI pour garantir le mapping entre processus MPI et PE NVSHMEM.\n",
    "- Toujours le calcul Jacobi en kernel CUDA sur le sous-domaine local, synchronisation CUDA stream comme dans les autres versions.\n",
    "- Utilisation de **Thrust** pour la réduction GPU de l’erreur max entre a_new et a (calcul de convergence), puis MPI_Allreduce pour la convergence globale.\n",
    "- Affichage des chronos \n",
    "- Rassemblement final et affichage de la grille comme dans les autres versions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c8509a4-88ea-43bb-ac9e-ae0c81f3bf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "export NVSHMEM_HOME=/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/nvshmem\n",
    "export CPATH=$NVSHMEM_HOME/include:$CPATH\n",
    "export LD_LIBRARY_PATH=$NVSHMEM_HOME/lib:$LD_LIBRARY_PATH\n",
    "export PATH=$NVSHMEM_HOME/bin:$PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ec800b66-9455-4cea-8cbd-2cb99d6c2501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f jacobi_nvshmem *.o *.qdrep *.sqlite\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd NVSHMEM\n",
    "make clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78e010da-4b63-4d81-9782-7f7856db762e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make: Nothing to be done for 'all'.\n",
      "[NVSHMEM] Temps d'init avant NVSHMEM : 2.511654s\n",
      "[NVSHMEM] Temps d'init NVSHMEM : 4.144677s\n",
      "[NVSHMEM] Converged in 1000 iterations | Error: 9.34e-06\n",
      "Temps de calcul Jacobi (seulement boucle)         : 0.078557s\n",
      "Temps setup+calcul+affichage (après init SHMEM)   : 0.078708s\n",
      "Temps total du programme (tout compris)           : 6.955773s\n",
      "État final :\n",
      "1.000000 0.999777 0.999560 0.999355 0.999167 0.999003 0.998865 0.998758 0.998686 0.998649 0.998649 0.998686 0.998758 0.998865 0.999003 0.999167 0.999355 0.999560 0.999777 1.000000 \n",
      "1.000000 0.999777 0.999560 0.999355 0.999167 0.999003 0.998865 0.998758 0.998686 0.998649 0.998649 0.998686 0.998758 0.998865 0.999003 0.999167 0.999355 0.999560 0.999777 1.000000 \n",
      "1.000000 0.999777 0.999560 0.999355 0.999167 0.999003 0.998865 0.998758 0.998686 0.998649 0.998649 0.998686 0.998758 0.998865 0.999003 0.999167 0.999355 0.999560 0.999777 1.000000 \n",
      "1.000000 0.999777 0.999560 0.999355 0.999167 0.999003 0.998865 0.998758 0.998686 0.998649 0.998649 0.998686 0.998758 0.998865 0.999003 0.999167 0.999355 0.999560 0.999777 1.000000 \n",
      "1.000000 0.999777 0.999560 0.999355 0.999167 0.999003 0.998865 0.998758 0.998686 0.998649 0.998649 0.998686 0.998758 0.998865 0.999003 0.999167 0.999355 0.999560 0.999777 1.000000 \n",
      "1.000000 0.999777 0.999560 0.999355 0.999167 0.999003 0.998865 0.998758 0.998686 0.998649 0.998649 0.998686 0.998758 0.998865 0.999003 0.999167 0.999355 0.999560 0.999777 1.000000 \n",
      "1.000000 0.999777 0.999560 0.999355 0.999167 0.999003 0.998865 0.998758 0.998686 0.998649 0.998649 0.998686 0.998758 0.998865 0.999003 0.999167 0.999355 0.999560 0.999777 1.000000 \n",
      "1.000000 0.999777 0.999560 0.999355 0.999167 0.999003 0.998865 0.998758 0.998686 0.998649 0.998649 0.998686 0.998758 0.998865 0.999003 0.999167 0.999355 0.999560 0.999777 1.000000 \n",
      "1.000000 0.999777 0.999560 0.999355 0.999167 0.999003 0.998865 0.998758 0.998686 0.998649 0.998649 0.998686 0.998758 0.998865 0.999003 0.999167 0.999355 0.999560 0.999777 1.000000 \n",
      "1.000000 0.999777 0.999560 0.999355 0.999167 0.999003 0.998865 0.998758 0.998686 0.998649 0.998649 0.998686 0.998758 0.998865 0.999003 0.999167 0.999355 0.999560 0.999777 1.000000 \n",
      "1.000000 0.999777 0.999560 0.999355 0.999167 0.999003 0.998865 0.998758 0.998686 0.998649 0.998649 0.998686 0.998758 0.998865 0.999003 0.999167 0.999355 0.999560 0.999777 1.000000 \n",
      "1.000000 0.999777 0.999560 0.999355 0.999167 0.999003 0.998865 0.998758 0.998686 0.998649 0.998649 0.998686 0.998758 0.998865 0.999003 0.999167 0.999355 0.999560 0.999777 1.000000 \n",
      "1.000000 0.999777 0.999560 0.999355 0.999167 0.999003 0.998865 0.998758 0.998686 0.998649 0.998649 0.998686 0.998758 0.998865 0.999003 0.999167 0.999355 0.999560 0.999777 1.000000 \n",
      "1.000000 0.999777 0.999560 0.999355 0.999167 0.999003 0.998865 0.998758 0.998686 0.998649 0.998649 0.998686 0.998758 0.998865 0.999003 0.999167 0.999355 0.999560 0.999777 1.000000 \n",
      "1.000000 0.999777 0.999560 0.999355 0.999167 0.999003 0.998865 0.998758 0.998686 0.998649 0.998649 0.998686 0.998758 0.998865 0.999003 0.999167 0.999355 0.999560 0.999777 1.000000 \n",
      "1.000000 0.999777 0.999560 0.999355 0.999167 0.999003 0.998865 0.998758 0.998686 0.998649 0.998649 0.998686 0.998758 0.998865 0.999003 0.999167 0.999355 0.999560 0.999777 1.000000 \n",
      "1.000000 0.999777 0.999560 0.999355 0.999167 0.999003 0.998865 0.998758 0.998686 0.998649 0.998649 0.998686 0.998758 0.998865 0.999003 0.999167 0.999355 0.999560 0.999777 1.000000 \n",
      "1.000000 0.999777 0.999560 0.999355 0.999167 0.999003 0.998865 0.998758 0.998686 0.998649 0.998649 0.998686 0.998758 0.998865 0.999003 0.999167 0.999355 0.999560 0.999777 1.000000 \n",
      "1.000000 0.999777 0.999560 0.999355 0.999167 0.999003 0.998865 0.998758 0.998686 0.998649 0.998649 0.998686 0.998758 0.998865 0.999003 0.999167 0.999355 0.999560 0.999777 1.000000 \n",
      "1.000000 0.999777 0.999560 0.999355 0.999167 0.999003 0.998865 0.998758 0.998686 0.998649 0.998649 0.998686 0.998758 0.998865 0.999003 0.999167 0.999355 0.999560 0.999777 1.000000 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd NVSHMEM\n",
    "\n",
    "# Charge NVSHMEM et NCCL\n",
    "export NVSHMEM_HOME=/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/nvshmem\n",
    "export LD_LIBRARY_PATH=$NVSHMEM_HOME/lib:$LD_LIBRARY_PATH\n",
    "export CPATH=$NVSHMEM_HOME/include:$CPATH\n",
    "export PATH=$NVSHMEM_HOME/bin:$PATH\n",
    "\n",
    "eval $(spack load --sh nccl)\n",
    "export LD_LIBRARY_PATH=$(spack location -i nccl)/lib:$LD_LIBRARY_PATH\n",
    "export NVSHMEM_ENABLE_ALL_DEVICE_INLINING=1\n",
    "\n",
    "\n",
    "make \n",
    "mpirun -np 4 ./jacobi_nvshmem 20 20 1000 1e-6 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa7c996-4c15-4a89-9241-d7555763ce83",
   "metadata": {},
   "source": [
    "# Partie 9 : NVSHMEM-LTO\n",
    "\n",
    "- Ajout du support de la compilation **LTO CUDA** (Link-Time Optimization) dans le Makefile :\n",
    "    - Ajout du flag `-gencode=arch=compute_90,code=lto_90` à la variable NVCCFLAGS pour activer la génération du code intermédiaire LTO pour l’architecture sm_90.\n",
    "    - Pas de modification du code source, uniquement une optimisation de la génération du binaire final via la chaîne de compilation NVCC pour de meilleures performances potentielles à l’exécution.\n",
    "    - Optimiser les appels de fonctions entre différents fichiers sources CUDA (inlining, suppression de code mort, etc.)\n",
    "    - Réorganiser ou fusionner des kernels s’il le juge pertinent.\n",
    "    - Générer un code plus performant, parfois plus rapide à l’exécution que le même code compilé sans LTO.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f68c86fb-c880-411f-bdf0-81a5bd37c927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f jacobi_nvshmem *.o *.qdrep *.sqlite\n",
      "nvcc -O3 -arch=sm_90  -std=c++17 --expt-relaxed-constexpr -rdc=true -gencode=arch=compute_90,code=sm_90 -gencode=arch=compute_90,code=lto_90 -Xcompiler \"-Wall -Wextra\" -ccbin=mpicxx -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/nvshmem/include -I/apps/2025/spack_install/linux-rhel9-neoverse_v2/linux-rhel9-neoverse_v2/gcc-11.4.1/cuda-12.6.2-3mzltpzgs4dekx5xvbqzz2no3j3tkcxq/include -o jacobi_nvshmem jacobi.cu -L/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/nvshmem/lib -lnvshmem -lnvshmem_host -L/apps/2025/spack_install/linux-rhel9-neoverse_v2/linux-rhel9-neoverse_v2/gcc-11.4.1/cuda-12.6.2-3mzltpzgs4dekx5xvbqzz2no3j3tkcxq/lib64 -lcudart -lmpi\n",
      "[NVSHMEM] Temps d'init avant NVSHMEM : 2.514711s\n",
      "[NVSHMEM] Temps d'init NVSHMEM : 4.495068s\n",
      "[NVSHMEM] Converged in 1000 iterations | Error: 2.42e-04\n",
      "Temps de calcul Jacobi (seulement boucle)         : 2.497435s\n",
      "Temps setup+calcul+affichage (après init SHMEM)   : 2.738734s\n",
      "Temps total du programme (tout compris)           : 11.659013s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd NVSHMEM-LTO\n",
    "\n",
    "\n",
    "export NVSHMEM_HOME=/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/nvshmem\n",
    "export CUDA_HOME=/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.3\n",
    "export LD_LIBRARY_PATH=$NVSHMEM_HOME/lib:$CUDA_HOME/lib64:$LD_LIBRARY_PATH\n",
    "export CPATH=$NVSHMEM_HOME/include:$CUDA_HOME/include:$CPATH\n",
    "export PATH=$NVSHMEM_HOME/bin:$CUDA_HOME/bin:$PATH\n",
    "\n",
    "# Charge NCCL via Spack si tu utilises aussi NCCL ailleurs\n",
    "eval $(spack load --sh nccl)\n",
    "export LD_LIBRARY_PATH=$(spack location -i nccl)/lib:$LD_LIBRARY_PATH\n",
    "export NVSHMEM_ENABLE_ALL_DEVICE_INLINING=1\n",
    "\n",
    "make clean\n",
    "make\n",
    "#  lance avec mpirun (remplace nvshmrun si dispo) :\n",
    "mpirun -np 4 ./jacobi_nvshmem 32000 32000 1000 1e-6 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361ada1e-aea3-4cb6-ac4f-2b930c159cad",
   "metadata": {},
   "source": [
    "# Partie 10 : NVSHMEM-neighborhood_sync-lto\n",
    "\n",
    "\n",
    "- Utilisation de **NVSHMEM** pour les échanges de halos entre GPUs (Remote Memory Access/RMA) :\n",
    "    - Chaque processus écrit directement dans la mémoire du halo (bord) de ses voisins grâce à `nvshmem_double_put`.\n",
    "    - Synchronisation mémoire assurée avec `nvshmem_fence()` puis barrière NVSHMEM pour garantir la cohérence avant le calcul.\n",
    "- Gestion avancée de la **synchronisation neighborhood** sur device (entre GPU voisins) :\n",
    "    - Ajout d’un kernel `syncneighborhood_kernel` pour notifier les voisins haut et bas que les halos sont prêts (utilisation de `nvshmemx_signal_op` et `nvshmem_uint64_wait_until_all` pour attendre les notifications des deux côtés).\n",
    "- Initialisation :\n",
    "    - Attribution automatique des GPU locaux par rang MPI intra-nœud.\n",
    "    - Calcul dynamique des tailles de sous-domaines (ghost_ny) pour chaque processus, prise en compte des décompositions déséquilibrées.\n",
    "    - Allocation de la mémoire sur le heap symétrique NVSHMEM (`nvshmem_malloc`) pour les tableaux et les variables de synchronisation, avec configuration dynamique de la variable d'environnement `NVSHMEM_SYMMETRIC_SIZE` pour garantir un espace mémoire suffisant.\n",
    "    - Initialisation de NVSHMEM avec un communicateur MPI personnalisé via `nvshmemx_init_attr`.\n",
    "- Calcul Jacobi toujours exécuté sur GPU via kernel CUDA, synchronisation des streams CUDA.\n",
    "- Calcul de l’erreur max avec **Thrust** sur GPU (utilisation de la bibliothèque Thrust pour la réduction GPU des différences a_new/a_old), puis MPI_Allreduce pour la convergence globale.\n",
    "- Mesure détaillée des temps : temps avant et après initialisation NVSHMEM, temps de calcul pur, temps total global, etc.\n",
    "- Rassemblement final et affichage de la grille sur le rang 0 avec gestion correcte des tailles locales, comme dans les versions précédentes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7093a9d-f001-490b-bf43-06a84951ae4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f jacobi_nvshmem *.o *.qdrep *.sqlite\n",
      "nvcc -O3 -arch=sm_90  -std=c++17 --expt-relaxed-constexpr -rdc=true -gencode=arch=compute_90,code=sm_90 -gencode=arch=compute_90,code=lto_90 -Xcompiler \"-Wall -Wextra\" -ccbin=mpicxx -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/nvshmem/include -I/apps/2025/spack_install/linux-rhel9-neoverse_v2/linux-rhel9-neoverse_v2/gcc-11.4.1/cuda-12.6.2-3mzltpzgs4dekx5xvbqzz2no3j3tkcxq/include -o jacobi_nvshmem jacobi.cu -L/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/nvshmem/lib -lnvshmem -lnvshmem_host -L/apps/2025/spack_install/linux-rhel9-neoverse_v2/linux-rhel9-neoverse_v2/gcc-11.4.1/cuda-12.6.2-3mzltpzgs4dekx5xvbqzz2no3j3tkcxq/lib64 -lcudart -lmpi\n",
      "[NVSHMEM-neighSync] Converged in 1000 iterations | Error: 2.42e-04\n",
      "Temps de calcul Jacobi (seulement boucle)         : 0.122526s\n",
      "Temps setup+calcul+affichage (après init SHMEM)   : 0.122720s\n",
      "Temps d'init avant NVSHMEM                        : 2.492420s\n",
      "Temps d'init NVSHMEM                              : 4.072520s\n",
      "Temps total du programme (tout compris)           : 6.850103s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd NVSHMEM-neighborhood_sync-lto\n",
    "\n",
    "# Charge les bonnes librairies NVSHMEM et CUDA\n",
    "export NVSHMEM_HOME=/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/nvshmem\n",
    "export CUDA_HOME=/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.3\n",
    "export LD_LIBRARY_PATH=$NVSHMEM_HOME/lib:$CUDA_HOME/lib64:$LD_LIBRARY_PATH\n",
    "export CPATH=$NVSHMEM_HOME/include:$CUDA_HOME/include:$CPATH\n",
    "export PATH=$NVSHMEM_HOME/bin:$CUDA_HOME/bin:$PATH\n",
    "\n",
    "# Charge NCCL via Spack si nécessaire\n",
    "eval $(spack load --sh nccl)\n",
    "export LD_LIBRARY_PATH=$(spack location -i nccl)/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "# Active l'inlining maximal NVSHMEM\n",
    "export NVSHMEM_ENABLE_ALL_DEVICE_INLINING=1\n",
    "\n",
    "make clean && make\n",
    "\n",
    "mpirun -np 4 ./jacobi_nvshmem 4096 4096 1000 1e-6 0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91144fa-f2d8-4398-a778-f3dfbe61a129",
   "metadata": {},
   "source": [
    "# Partie 11 : nvshmem-norm_overlap-neighborhood_sync+lto \n",
    "\n",
    "- le **calcul sur la grille locale est vraiment overlap avec l'échange des halos grâce à un découpage fin des kernels et à la synchronisation asynchrone device-side.\n",
    "\n",
    "- On découpe la mise à jour Jacobi en :\n",
    "    - `jacobi_inner_kernel` (zone intérieure, ne dépend pas du halo) qui est lancé pendant que les halos sont envoyés aux voisins.\n",
    "    - Une fois l'échange de halos terminé (vérifié via le kernel `syncneighborhood_kernel` qui utilise des signaux device NVSHMEMX),  \n",
    "      on lance `jacobi_border_kernel` sur les deux bandes frontalières haut/bas qui ont besoin du halo à jour.\n",
    "  Ce découpage permet de **masquer le coût de communication** et d'accélérer la convergence pour des grosses grilles.\n",
    "\n",
    "- **Synchronisation neighborhood**: au lieu d'une barrière globale (`nvshmem_barrier_all`), chaque processus attend juste un signal de ses deux voisins directs (haut/bas) pour la cohérence des halos : cela limite l'attente et améliore l'overlap comm/calcul.\n",
    "\n",
    "- Le reste de la structure générale (allocation heap symétrique, gestion des tailles fantômes variables, calcul erreur avec thrust, réduction MPI_Allreduce, réassemblage final) reste identique à la version précédente.\n",
    "\n",
    "- L'objectif est de **maximiser le recouvrement entre calcul local et communication** des halos, ce qui est particulièrement efficace sur des architectures GPU modernes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8280652-6caf-4dbb-9593-70015d37395f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f jacobi_nvshmem *.o *.qdrep *.sqlite\n",
      "nvcc -O3 -arch=sm_90  -std=c++17 --expt-relaxed-constexpr -rdc=true -gencode=arch=compute_90,code=sm_90 -gencode=arch=compute_90,code=lto_90 -Xcompiler \"-Wall -Wextra\" -ccbin=mpicxx -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/nvshmem/include -I/apps/2025/spack_install/linux-rhel9-neoverse_v2/linux-rhel9-neoverse_v2/gcc-11.4.1/cuda-12.6.2-3mzltpzgs4dekx5xvbqzz2no3j3tkcxq/include -o jacobi_nvshmem jacobi.cu -L/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/nvshmem/lib -lnvshmem -lnvshmem_host -L/apps/2025/spack_install/linux-rhel9-neoverse_v2/linux-rhel9-neoverse_v2/gcc-11.4.1/cuda-12.6.2-3mzltpzgs4dekx5xvbqzz2no3j3tkcxq/lib64 -lcudart -lmpi\n",
      "[NVSHMEM-norm_overlap-neighSync+LTO] Converged in 1000 iterations | Error: 2.42e-04\n",
      "Temps de calcul Jacobi (seulement boucle)         : 0.110126s\n",
      "Temps setup+calcul+affichage (après init SHMEM)   : 0.110322s\n",
      "Temps d'init avant NVSHMEM                        : 2.480157s\n",
      "Temps d'init NVSHMEM                              : 4.116901s\n",
      "Temps total du programme (tout compris)           : 7.095077s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd NVSHMEM-norm_overlap-neighborhood_sync+lto\n",
    "\n",
    "# Charge les bonnes librairies NVSHMEM et CUDA\n",
    "export NVSHMEM_HOME=/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/nvshmem\n",
    "export CUDA_HOME=/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.3\n",
    "export LD_LIBRARY_PATH=$NVSHMEM_HOME/lib:$CUDA_HOME/lib64:$LD_LIBRARY_PATH\n",
    "export CPATH=$NVSHMEM_HOME/include:$CUDA_HOME/include:$CPATH\n",
    "export PATH=$NVSHMEM_HOME/bin:$CUDA_HOME/bin:$PATH\n",
    "\n",
    "# Charge NCCL via Spack si nécessair\n",
    "\n",
    "eval $(spack load --sh nccl)\n",
    "export LD_LIBRARY_PATH=$(spack location -i nccl)/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "# Active l'inlining maximal NVSHMEM\n",
    "export NVSHMEM_ENABLE_ALL_DEVICE_INLINING=1\n",
    "\n",
    "make clean && make\n",
    "\n",
    "mpirun -np 4 ./jacobi_nvshmem 4096 4096 1000 1e-6 0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bef36e4-97e0-49c0-b629-5bce390fbeb2",
   "metadata": {},
   "source": [
    "# Partie 12 : nvshmem-norm_overlap-neighborhood_sync+lto[1]  \n",
    "\n",
    "\n",
    "- **Ici, c'est une version \"baseline\" (de base, séquentielle côté device)** :\n",
    "    - On utilise **un seul kernel Jacobi** (`jacobi_kernel_full`) qui fait tout le calcul d'un coup sur tout le sous-domaine local, sans découpage ni overlap calcul/communication.\n",
    "    - **Pas de découpage** en \"zone intérieure\" et \"zone de bord\" : tout est fait d’un bloc, donc la communication des halos ne peut pas être masquée par le calcul local.\n",
    "    - **Pas de synchronisation neighborhood device** (pas de kernel de synchronisation avec les voisins via NVSHMEMX), ni de stratégie fine pour attendre juste les voisins.\n",
    "    - On se contente d’un schéma classique : \n",
    "        - On fait tout le calcul local,\n",
    "        - Puis l’erreur est calculée (avec thrust),\n",
    "        - Puis un `MPI_Allreduce` pour la convergence globale,\n",
    "        - Et on passe à l’itération suivante.\n",
    "\n",
    "- **En résumé :**  \n",
    "  - Version \"simple\" qui sert de référence pour les perfs : pas d’overlap calcul/comm, pas d’optimisation, pas de synchronisation fine, **juste un Jacobi standard sur tout le domaine local**.\n",
    "  - Cela permet de comparer l’apport réel des optimisations dans les autres versions (découpage kernels + overlap, synchronisation neighborhood, etc).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f28e42c0-e360-4fa8-a2af-65a4b823fa2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f jacobi_nvshmem *.o *.qdrep *.sqlite\n",
      "nvcc -O3 -arch=sm_90  -std=c++17 --expt-relaxed-constexpr -rdc=true -gencode=arch=compute_90,code=sm_90 -gencode=arch=compute_90,code=lto_90 -Xcompiler \"-Wall -Wextra\" -ccbin=mpicxx -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/nvshmem/include -I/apps/2025/spack_install/linux-rhel9-neoverse_v2/linux-rhel9-neoverse_v2/gcc-11.4.1/cuda-12.6.2-3mzltpzgs4dekx5xvbqzz2no3j3tkcxq/include -o jacobi_nvshmem jacobi.cu -L/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/nvshmem/lib -lnvshmem -lnvshmem_host -L/apps/2025/spack_install/linux-rhel9-neoverse_v2/linux-rhel9-neoverse_v2/gcc-11.4.1/cuda-12.6.2-3mzltpzgs4dekx5xvbqzz2no3j3tkcxq/lib64 -lcudart -lmpi\n",
      "[NVSHMEM-baseline-single-rank] Converged in 1000 iterations | Error: 2.42e-04\n",
      "Temps de calcul Jacobi (seulement boucle)         : 0.086699s\n",
      "Temps setup+calcul+affichage (après init SHMEM)   : 0.086862s\n",
      "Temps d'init avant NVSHMEM                        : 2.420626s\n",
      "Temps d'init NVSHMEM                              : 3.956365s\n",
      "Temps total du programme (tout compris)           : 6.616686s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd NVSHMEM-norm_overlap-neighborhood_sync+lto[1]\n",
    "\n",
    "# Charge les bonnes librairies NVSHMEM et CUDA\n",
    "export NVSHMEM_HOME=/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/nvshmem\n",
    "export CUDA_HOME=/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.3\n",
    "export LD_LIBRARY_PATH=$NVSHMEM_HOME/lib:$CUDA_HOME/lib64:$LD_LIBRARY_PATH\n",
    "export CPATH=$NVSHMEM_HOME/include:$CUDA_HOME/include:$CPATH\n",
    "export PATH=$NVSHMEM_HOME/bin:$CUDA_HOME/bin:$PATH\n",
    "\n",
    "# Charge NCCL via Spack si nécessair\n",
    "\n",
    "eval $(spack load --sh nccl)\n",
    "export LD_LIBRARY_PATH=$(spack location -i nccl)/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "# Active l'inlining maximal NVSHMEM\n",
    "export NVSHMEM_ENABLE_ALL_DEVICE_INLINING=1\n",
    "\n",
    "make clean && make\n",
    "\n",
    "mpirun -np 4 ./jacobi_nvshmem 4096 4096 1000 1e-6 0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ab51b5-354a-4dbe-a2a3-052dbb4e5c62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
